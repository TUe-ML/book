{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85f9902",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "In data mining and machine learning, optimization plays a major role at the heart of almost every method. We could say that optimization is what actually defines the \"learning\" of machines. Whether we are training a simple linear regression model or a deep neural network, the goal remains the same: finding the best possible solution to a given problem within the constraints of the available data and model. This process of finding the \"best\" solution is what we refer to as optimization.\n",
    "\n",
    "At its core, optimization seeks to minimize (or sometimes maximize) an objective function, such as a loss function, which quantifies how well the model is performing. Some loss functions are easier to optimize than others. For example, if there are many local minima, or saddle points of the loss function, finding the global optimum is not achievable in practical scenarios (often also not really desirable). The optimization determines then what kind of local minimum we are going to get. Hence, the power of a machine learning model largely depends on the power of the optimization method in combination with the suitability of the loss.  \n",
    "\n",
    "\n",
    "## Structure of the Chapter\n",
    "\n",
    "In this chapter, we discuss the basics of the optimization techniques used in this course. It's good to read this chapter now and to get familiar with the existing methods. Later, you might want to revisit this chapter, when we  apply the presented optimization methods to machine learning. Often, the optimization methods still need some alterations to the general scheme, to make the optimization work well in practice. We will wor out these details in the corresponding chapers then.     \n",
    "\n",
    "Now, we discuss finding minima by the analytical solutions of the First and Secondary order necessary condition, as well as numeric optimization methods like coordinate and gradient descent. We discuss properties of optimization objectives, such as convexity and the existence of constraints, and how these properties influence the set of potential minimizers. \n",
    "\n",
    "\n",
    "## Recommended Literature:     \n",
    "If you want to read up further on this topic, the following material is recommended.       \n",
    "**Linear Algebra and Optimization for Machine Learning by Charu C. Aggarwal}**     \n",
    "Sections 4.1-4.3 build up nicely the aspects of optimization from the one-dimensional case (univariate optimizattion) to higher dimensions (multivariate optimization). Section 4.6 gives an overview over computing gradients subject to vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529761d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
