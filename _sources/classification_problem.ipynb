{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770aa9c0",
   "metadata": {},
   "source": [
    "## Classification Objective\n",
    "Classification is a supervised learning task where the goal is to predict a discrete class label for each input. A typical toy dataset in classification is the Iris dataset, where the task is to classify flower  species based on physical features of the plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4167037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name=\"target\")\n",
    "\n",
    "# Preview the dataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6863c",
   "metadata": {},
   "source": [
    "The Iris dataset contains 150 data points, having 4 features: sepal length, sepal width, petal length, and petal width. The target classes are:\n",
    "\n",
    "* $y=0$: setosa\n",
    "* $y=1$: versicolor\n",
    "* $y=2$: virginica\n",
    "\n",
    "All features in the Iris dataset are continuous numerical values, as they represent physical measurements of the flower in centimeters. However, classification datasets may also have discrete features such as male/female or color (green, red, blue).     \n",
    "\n",
    "### Classification Challenges\n",
    "Even though the Iris dataset is relatively “clean,” it still allows us to discuss core challenges in classification tasks.\n",
    "\n",
    "#### Class Imbalance\n",
    "In many real-world problems, one class dominates the others. Let's check the class distribution in Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b6f578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.333333\n",
       "1    0.333333\n",
       "2    0.333333\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbd7f9",
   "metadata": {},
   "source": [
    "Here, each class occurs exactly 50 times (1/3 of the data), making the Iris dataset a perfectly balanced classification dataset. Having one class dominate over others may result in classifiers that predict the majority class in most cases and that are also correct most of the time with that prediction. Hence, the evaluation must be performed carefull in those cases, and maybe the classifier needs to be adapted to this case as well.  \n",
    "#### Mixed Feature Types\n",
    "While the Iris dataset only contains continuous numerical features, many real-world classification problems involve discrete or categorical features, which introduce their own set of challenges.\n",
    "In many datasets, features can be discrete — meaning they take on a finite set of values, such as:\n",
    "\n",
    "* Categorical/Nominal: e.g., \"color\" = red, green, blue\"\n",
    "* Ordinal: e.g., \"education level\" = high school < college < graduate\"\n",
    "* Binary: e.g., \"has_credit_card\" = yes/no\"\n",
    "\n",
    "Categorical and binary data has no meaningful $\\leq$ operation and this requires special treatment. For example, simple statistics such as computing the mean and variance don't have a meaning for categorical data. Not all classifiers can handle all feature types, hence it's important to match the classifier to the feature types in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff969e6-e092-4549-a97c-e58f91aba87f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classifier definition\n",
    "\n",
    "`````{admonition} Task (Classification)\n",
    ":class: tip\n",
    "**Given** a dataset consisting of $n$ observations $\\vvec{x}_i$ and their corresponding labels $y_i$ indicating one of $c$ classes\n",
    "    $$\\mathcal{D}=\\left\\{(\\vvec{x}_{i},y_i)\\vert \\vvec{x}_{i}\\in\\mathbb{R}^{d},y_i\\in\\{1,\\ldots, c\\}, 1\\leq i \\leq n\\right\\}$$\n",
    "\n",
    "**Find** a classifier $f:\\mathbb{R}^ d\\rightarrow \\mathbb{R}^c$, that captures the relationship between observations and their class. The classifier predicts the label with the maximum value:  \n",
    "$$\\hat{y}_i = \\argmax_{l\\in\\{1,\\ldots,c\\}}f(\\vvec{x}_{i})_l.$$ \n",
    "The goal is to find a classifier that predicts the correct labels $\\hat{y}_i = y_i$.\n",
    "`````\n",
    "\n",
    "Classifiers, similar to regression models, are defined by their inference and their training. Inference describes how the model performs prediction of (unseen) data points. The training or learning describes how the model is generated, given the training data. \n",
    "### Evaluation\n",
    "We quickly define the most straightforward classification evaluation metrics: the $L_{01}$-loss and the accuracy. Both put into relation how many errors/correct predictions a classifier makes in a dataset. \n",
    "\n",
    "```{prf:definition} 0-1 Loss\n",
    "Given a classifier $\\hat{y}(\\vvec{x})$ returning the predicted label. We define the **0-1 loss** as\n",
    "$$L_{01}(y,\\hat{y}) = \\begin{cases}\n",
    "1, & \\text{if } y\\neq \\hat{y}\\\\\n",
    "0, & \\text{if } y=\\hat{y}\n",
    "\\end{cases}$$\n",
    "```\n",
    "The 0-1 loss indicates whether a classifier makes an error in its prediction. In contrast, the accuracy indicates how much a classifier gets right.\n",
    "```{prf:definition} Accuracy\n",
    "Given a classifier $\\hat{y}(\\vvec{x})$returning the predicted label. the accuracy of the classifier on dataset $\\mathcal{D}$ containing $n$ data points is given as\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{Acc}(\\hat{y},\\mathcal{D}) &= \\frac{\\text{Correct predictions}}{\\text{Total predictions}}\\\\\n",
    "&= \\frac1n \\lvert\\{(\\vvec{x}_i,y_i)\\in\\mathcal{D}\\mid \\hat{y}(\\vvec{x}_i)=y_i\\}\\rvert\\\\\n",
    "&= 1- \\frac1n\\sum_{i=1}^n L_{01}(\\hat{y}(\\vvec{x}_i),y_i).\n",
    "\\end{align*}\n",
    "```\n",
    "### Theoretically Optimal Classifiers\n",
    "Similarly to the regression data sampling process according to a true regression function plus some noise, we also have some assumptions about how our classification data is sampled and what the ground truth classifier is that we want to recover.\n",
    "````{prf:property} i.i.d. Class Distribution\n",
    "Under the i.i.d. class distribution assumption, we assume that the dataset samples are _identically_ distributed and _independently_ drawn from an _unknown_ probability distribution $ p^{\\ast}({\\bf x}, y) $, i.e. \n",
    "```{math}\n",
    ":label: iid_assumption\n",
    "{\\bf x}_{i}, y_{i} \\sim  p^{\\ast}({\\bf x}, y), \\forall i \\in \\lbrace 1, \\ldots, n \\rbrace.\n",
    "```\n",
    "````\n",
    "Likewise, we can define the expected prediction error for a classification problem.\n",
    "````{prf:definition} Classifier EPE\n",
    ":label: true_classifier_error\n",
    "Given a classifier $f_\\mathcal{D}:\\mathbb{R}^d\\rightarrow \\mathbb{R}^c$ that has been trained on dataset $\\mathcal{D}$. \n",
    "the Expected Prediction Error (EPE) of classifiers is the expected error\n",
    "```{math}\n",
    ":label: true_classification_error\n",
    "p( y \\neq \\argmax_{l} f(\\vvec{x})_l ) = \\mathbb{E}_{\\vvec{x},y,\\mathcal{D}} [L_{01} (y, \\argmax_l f_\\mathcal{D}(\\vvec{x}) )_l] ,\n",
    "```\n",
    "over three random variables:\n",
    "* $\\vvec{x}$ is the random variable of a feature vector in the test set.\n",
    "* $y$ is the random variable of the class of $\\vvec{x}$.\n",
    "* $\\mathcal{D}$ is the random variable of the training data.\n",
    "````\n",
    "\n",
    "\n",
    "\n",
    "```{note}\n",
    "The probability of misclassification $ p( y \\neq \\argmax_l f(\\vvec{x})_l) $ is also known as the risk $ R(f) $ of the classifier $ f $.\n",
    "```\n",
    "\n",
    "````{prf:definition} Bayes optimal classifier\n",
    ":label: bayes_optimal_classifier\n",
    "The **Bayes classifier** is the optimal classifier that minimizes the probability of misclassification \n",
    "```{math}\n",
    ":label: final_estimator\n",
    "y^\\ast(\\vvec{x}) = \\argmax_{1\\leq y\\leq c} p^\\ast (y\\mid \\vvec{x})\n",
    "```\n",
    "````\n",
    "The Bayes classifier has the lowest EPE possible.     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21e6a1-98f0-457c-877f-e39d1ebecc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
