
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Exercises &#8212; Data Mining and Machine Learning Jupyter Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optimization_exercises';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Regression" href="regression.html" />
    <link rel="prev" title="Matrix Derivatives" href="optimization_gradients.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Data Mining and Machine Learning Jupyter Book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Data Mining and Machine Learning Jupyter Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linalg.html">Linear Algebra</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="linalg_spaces.html">Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_normed_vs.html">Normed Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="optimization.html">Optimization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="optimization_problems.html">Optimization Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_convex.html">Convex Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_analytic.html">Analytic Solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_numerical.html">Numerical Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_gradients.html">Matrix Derivatives</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="regression_objective.html">Regression Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_functions.html">Regression Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_optimization.html">Minimizing the RSS</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_bias_var.html">The Bias-Variance Tradeoff</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_sparse.html">The Sparse Regression Task</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_ridge.html">Ridge Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_lasso.html">Lasso</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="classification.html">Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="classification_problem.html">Classification Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_knn.html">K-Nearest Neighbor</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_naive_bayes.html">Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_random_forests.html">Random Forests</a></li>

<li class="toctree-l2"><a class="reference internal" href="classification_svms.html">Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_kernel_svm.html">Kernel SVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="neuralnets.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_intro.html">From Linear Models to Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_mlps.html">MLPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_backprop.html">Backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_sgd.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_conv.html">Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_pooling.html">Pooling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="dim_reduction.html">Dimensionality Reduction Techniques</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_mf.html">Low Rank Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_pca.html">Principal Component Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="clustering.html">Clustering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="clustering_k_means.html">k-Means</a></li>



<li class="toctree-l2"><a class="reference internal" href="clustering_k_means_mf.html">k-Means is MF</a></li>


<li class="toctree-l2"><a class="reference internal" href="clustering_kernel_kmeans.html">Kernel k-means</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering_spectral.html">Spectral Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tue-ml/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/edit/main/optimization_exercises.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/issues/new?title=Issue%20on%20page%20%2Foptimization_exercises.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/optimization_exercises.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Exercises</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-functions">Convex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-optimization">Numerical Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-gradients">Computing the Gradients</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="exercises">
<h1>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h1>
<section id="convex-functions">
<h2>Convex Functions<a class="headerlink" href="#convex-functions" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p>Show that nonnegative weighted sums of convex functions are convex. That is, show
for all <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_k\geq 0\)</span> and convex functions <span class="math notranslate nohighlight">\(f_1,\ldots,f_k:\mathcal{X}\rightarrow \mathbb{R}\)</span>, that the function
<div class="math notranslate nohighlight">
\[f(\vvec{x}) = \lambda_1 f_1(\vvec{x})+\ldots + \lambda_k f_k(\vvec{x})\]</div>

is convex.</p>
<div class="toggle docutils container">
<p>Let
<div class="math notranslate nohighlight">
\[f(\vvec{x}) = \lambda_1 f_1(\vvec{x})+\ldots + \lambda_k f_k(\vvec{x})\]</div>

for <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_k\geq 0\)</span> and <span class="math notranslate nohighlight">\(f_1,\ldots,f_k:\mathcal{X}\rightarrow \mathbb{R}\)</span> be convex functions. Let <span class="math notranslate nohighlight">\(\alpha\in[0,1]\)</span> and <span class="math notranslate nohighlight">\(\vvec{x},\vvec{y}\in\mathcal{X}\)</span>. Then we have to show according to the definition of convex functions that
<div class="math notranslate nohighlight">
\[f(\alpha\vvec{x}+(1-\alpha)\vvec{y})\leq \alpha f(\vvec{x})+(1-\alpha)f(\vvec{y}).\]</div>

According to the definition of <span class="math notranslate nohighlight">\(f\)</span>, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    f(\alpha\vvec{x}+(1-\alpha)\vvec{y}) &amp;= \lambda_1 f_1(\alpha\vvec{x}+(1-\alpha)\vvec{y})+\ldots + \lambda_k f_k(\alpha\vvec{x}+(1-\alpha)\vvec{y})
\end{align*}\]</div>
<p>Since <span class="math notranslate nohighlight">\(f_i\)</span> is convex for <span class="math notranslate nohighlight">\(1\leq i\leq k\)</span>, for the functions <span class="math notranslate nohighlight">\(f_i\)</span> holds that
<div class="math notranslate nohighlight">
\[f_i(\alpha\vvec{x}+(1-\alpha)\vvec{y})\leq \alpha f_i(\vvec{x})+(1-\alpha)f_i(\vvec{y}).\]</div>

Now we multiply the inequality above with the nonnegative value <span class="math notranslate nohighlight">\(\lambda_i\)</span>. Note that here it becomes obvious why the coefficients have to be nonnegative. Multiplying an inequality with a negative value changes the direction of the inequality, that is a <span class="math notranslate nohighlight">\(\leq\)</span> becomes a <span class="math notranslate nohighlight">\(\geq\)</span> and vice versa. However, here we have only nonnegative values <span class="math notranslate nohighlight">\(\lambda_i\)</span> and thus, multiplying with the coefficient keeps the inequality intact:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a28275f5-8aed-4916-8d1e-b0c02084a16b">
<span class="eqno">(4)<a class="headerlink" href="#equation-a28275f5-8aed-4916-8d1e-b0c02084a16b" title="Permalink to this equation">#</a></span>\[\begin{align}
    \lambda_if_i(\alpha\vvec{x}+(1-\alpha)\vvec{y})&amp;\leq \lambda_i(\alpha f_i(\vvec{x})+(1-\alpha)f_i(\vvec{y}))\nonumber\\ 
    &amp;= \alpha \lambda_i f_i(\vvec{x})+(1-\alpha)\lambda_i f_i(\vvec{y}).\label{eq:ficonv}
\end{align}\]</div>
<p>We can now put these inequalities together to derive the convexity of the function <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\alpha\vvec{x}+(1-\alpha)\vvec{y}) &amp;= \lambda_1 f_1(\alpha\vvec{x}+(1-\alpha)\vvec{y})+\ldots + \lambda_k f_k(\alpha\vvec{x}+(1-\alpha)\vvec{y})\\
&amp;= \sum_{i=1}^k \lambda_i f_i(\alpha\vvec{x}+(1-\alpha)\vvec{y})\\
&amp;\leq 
\sum_{i=1}^k \alpha \lambda_i f_i(\vvec{x})+(1-\alpha)\lambda_i f_i(\vvec{y})&amp;\text{apply Eq.~\eqref{eq:ficonv}}\\
&amp;=
\alpha \sum_{i=1}^k \lambda_i f_i(\vvec{x})+(1-\alpha)\sum_{i=1}^k\lambda_i f_i(\vvec{y})\\
&amp;=\alpha f(\vvec{x}) + (1-\alpha)f(\vvec{y}).&amp;\text{apply definition of $f$}
\end{align*}\]</div>
<p>This concludes what we wanted to show.</p>
</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(g:\mathbb{R}^d\rightarrow \mathbb{R}^k\)</span>, <span class="math notranslate nohighlight">\(g(\vvec{x})=A\vvec{x}+\vvec{b}\)</span> is an affine map, and <span class="math notranslate nohighlight">\(f:\mathbb{R}^k\rightarrow \mathbb{R}\)</span> is a convex function, then the composition
<div class="math notranslate nohighlight">
\[f(g(\vvec{x}))=f(A\vvec{x}+\vvec{b})\]</div>

is a convex function.</p>
<div class="toggle docutils container">
<p>Let <span class="math notranslate nohighlight">\(g:\mathbb{R}^d\rightarrow \mathbb{R}^k\)</span>, <span class="math notranslate nohighlight">\(g(\vvec{x})=A\vvec{x}+\vvec{b}\)</span> be an affine map, and let <span class="math notranslate nohighlight">\(f:\mathbb{R}^k\rightarrow \mathbb{R}\)</span> be a convex function.
Then we have to show according to the definition of convex functions that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    f(g(\alpha\vvec{x}+(1-\alpha)\vvec{y}))&amp;\leq \alpha f(g(\vvec{x}))+(1-\alpha)f(g(\vvec{y}))\\
    \Leftrightarrow f(A(\alpha\vvec{x}+(1-\alpha)\vvec{y})+\vvec{b})&amp;\leq\alpha f(A\vvec{x}+\vvec{b})+(1-\alpha)f(A\vvec{y}+\vvec{b}).
\end{align*}\]</div>
<p>We might see already that the term on the right could be derived from the convexity of <span class="math notranslate nohighlight">\(f\)</span>. To do so, we first need to put the argument of <span class="math notranslate nohighlight">\(f\)</span> on the left into the form <span class="math notranslate nohighlight">\(\alpha (A\vvec{x}+\vvec{b})+(1-\alpha)(A\vvec{y}+\vvec{b}\)</span>. This, we can actually achieve like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A(\alpha\vvec{x}+(1-\alpha)\vvec{y})+\vvec{b} &amp;= \alpha A\vvec{x}+(1-\alpha)A\vvec{y}+\vvec{b} &amp; \text{(linearity)} \\
    &amp;= \alpha A\vvec{x}+(1-\alpha)A\vvec{y}+(\alpha + 1-\alpha)\vvec{b}\\
    &amp;=\alpha A\vvec{x}+(1-\alpha)A\vvec{y}+\alpha\vvec{b} + (1-\alpha)\vvec{b}\\
    &amp;=\alpha (A\vvec{x}+\vvec{b})+(1-\alpha)(A\vvec{y}+\vvec{b})
\end{align*}\]</div>
<p>As a result, we get with respect to <span class="math notranslate nohighlight">\(g\)</span> that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    g(\alpha\vvec{x}+(1-\alpha)\vvec{y}) &amp;=
    A(\alpha\vvec{x}+(1-\alpha)\vvec{y})+\vvec{b}\\
    &amp;=\alpha (A\vvec{x}+\vvec{b})+(1-\alpha)(A\vvec{y}+\vvec{b})\\
    &amp;= \alpha g(\vvec{x}) + (1-\alpha)g(\vvec{y}).
\end{align*}\]</div>
<p>If we apply now the function <span class="math notranslate nohighlight">\(f\)</span> to the equality above and use the convexity of <span class="math notranslate nohighlight">\(f\)</span>, then we can conclude what we wanted to show:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    f(g(\alpha\vvec{x}+(1-\alpha)\vvec{y}))&amp;= f\left(\alpha g(\vvec{x}) + (1-\alpha)g(\vvec{y})\right)\\
    &amp;\leq \alpha f\left( g(\vvec{x})\right) + (1-\alpha) f\left(g(\vvec{y})\right).
\end{align*}\]</div>
</div>
</li>
</ol>
</section>
<section id="numerical-optimization">
<h2>Numerical Optimization<a class="headerlink" href="#numerical-optimization" title="Link to this heading">#</a></h2>
<ol class="arabic" start="3">
<li><p>Compute three gradient descent steps for the following objective:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}\min (x-2)^2 + 1 \quad\text{ s.t. }x\in\mathbb{R}\end{align*} \]</div>
<p>Try the following combinations of initalizations and step sizes:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(x_0=4\)</span>, step size <span class="math notranslate nohighlight">\(\eta=\frac14\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_0=4\)</span>, step size <span class="math notranslate nohighlight">\(\eta=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_0=3\)</span>, step size <span class="math notranslate nohighlight">\(\eta=\frac54\)</span></p></li>
</ol>
<p>Mark the iterates <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span> and <span class="math notranslate nohighlight">\(x_3\)</span> in a plot of the objective function. What do you observe regarding the convergence of gradient descent methods? Does gradient descent always “descent” from an iterate?</p>
<div class="toggle docutils container">
<p>In order to conduct gradient descent, we need the derivative:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(x) &amp;= (x-2)^2 +1\\
f'(x)&amp;= 2(x-2)
\end{align*}\]</div>
<p>The gradient descent update rules are defined as
<div class="math notranslate nohighlight">
\[x_{t+1}=x_t-\eta f'(x_t).\]</div>

We conduct now two gradient descent steps for the stated scenarios:</p>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(x_0=4\)</span>, step size <span class="math notranslate nohighlight">\(\eta=\frac14\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 &amp;= x_0 -\eta f'(x_0) = 4 - \frac14 4 = 3\\
x_2 &amp; = x_1 -\eta f'(x_1)= 3 -\frac14 2 = 2.5\\
x_3 &amp; = x_2 -\eta f'(x_2)= 2.5 -\frac14 = 2.25
\end{align*}\]</div>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-ea98f3190fd05f4932466f5f2e738d6795c73259.png" alt="Figure made with TikZ" /></p>
</div><p>The iterates are slowly converging to the minimum <span class="math notranslate nohighlight">\(x^*=2\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(x_0=4\)</span>, step size <span class="math notranslate nohighlight">\(\eta=1\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 &amp;= x_0 -\eta f'(x_0) = 4 -  4 = 0\\
x_2 &amp; = x_1 -\eta f'(x_1)= 0 - (-4) = 4\\
x_3 &amp; = x_2 -\eta f'(x_2)= 4 -  4 = 0
\end{align*}\]</div>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-5c9c32d94367c05d9e953cdea333f0f4ccf9317a.png" alt="Figure made with TikZ" /></p>
</div><p>The iterates are oscilliating between the values <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(4\)</span>. Hence, the iterates will never converge when using a step-size of <span class="math notranslate nohighlight">\(\eta =1\)</span>. The step-size is too large.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(x_0=3\)</span>, step size <span class="math notranslate nohighlight">\(\eta=\frac54\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x_1 &amp;= x_0 -\eta f'(x_0) = 3 - \frac54 2 = \frac12\\
x_2 &amp; = x_1 -\eta f'(x_1)= \frac12 -\frac54 (-3) = 4.25\\
x_3 &amp; = x_2 -\eta f'(x_2)= 4.25 -\frac54 4.5 = -1.375
\end{align*}\]</div>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-21fd702016cec681f26071740d0b646fdac1bcf2.png" alt="Figure made with TikZ" /></p>
</div><p>The iterates are oscilliating and the function values are diverging (going to infinity). Every gradient step is actually increasing the objective function since the step size is far too large.</p>
</li>
</ol>
</div>
</li>
</ol>
</section>
<section id="computing-the-gradients">
<span id="opt-exercises-gradients"></span><h2>Computing the Gradients<a class="headerlink" href="#computing-the-gradients" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p>What is the Jacobian of the squared Euclidean norm <span class="math notranslate nohighlight">\(f(\vvec{x})=\lVert\vvec{x}\rVert^2\)</span>?</p>
<div class="toggle docutils container">
<p>Given a vector <span class="math notranslate nohighlight">\(\vvec{x}\in\mathbb{R}^d\)</span>, then the squared Euclidean norm is defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \lVert\vvec{x}\rVert^2 = \sum_{i=1}^d x_i^2.
\end{align*}\]</div>
<p>We compute the partial derivative with respect to <span class="math notranslate nohighlight">\(x_k\)</span>, treating the terms <span class="math notranslate nohighlight">\(x_i\)</span> as constants for <span class="math notranslate nohighlight">\(i\neq k\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial}{\partial x_k} \lVert\vvec{x}\rVert^2 &amp; = \frac{\partial}{\partial x_k} \sum_{i=1}^d x_i^2 = \frac{\partial}{\partial x_k} x_k^2  = 2x_k. 
\end{align*}\]</div>
<p>Hence, the Jacobian is given by
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \vvec{x}}\lVert\vvec{x}\rVert^2 = \begin{pmatrix} 2x_1&amp;\ldots &amp; 2x_d\end{pmatrix} = 2\vvec{x}^\top.\]</div>

Correspondingly, we can denote the gradient now as the transposed of the Jacobian:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \nabla \lVert\vvec{x}\rVert^2 = 2\vvec{x}.
\end{align*}\]</div>
</div>
</li>
<li><p>What is the Jacobian of the function <span class="math notranslate nohighlight">\(f:\mathbb{R}\rightarrow \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(f(x) = \vvec{b}-\vvec{a}x\)</span> for vectors <span class="math notranslate nohighlight">\(\vvec{a},\vvec{b}\in\mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(x\in\mathbb{R}\)</span>?</p>
<div class="toggle docutils container">
<p>We write the function <span class="math notranslate nohighlight">\(f\)</span> as a vector of one-dimensional functions:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(x) = \vvec{b}-\vvec{a}x = \begin{pmatrix}f_1(x)\\\vdots\\f_n(x)\end{pmatrix} = \begin{pmatrix}b_1 - a_1x\\\vdots\\b_n-a_nx\end{pmatrix}.
\end{align*}\]</div>
<p>The derivative of the one-dimensional functions <span class="math notranslate nohighlight">\(f_i\)</span> is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial}{\partial x}f_i(x)
= \frac{\partial}{\partial x}(b_i-a_ix)
=-a_i.
\end{align*}\]</div>
<p>Hence, the Jacobian of <span class="math notranslate nohighlight">\(f\)</span> is equal to the vector
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial x}f(x) 
= -\vvec{a}.\]</div>
</p>
</div>
</li>
<li><p>What is the Jacobian of the function <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\rightarrow \mathbb{R}^n\)</span>,
<span class="math notranslate nohighlight">\(f(\vvec{x}) = \vvec{b} -A\vvec{x}\)</span>, (A is an <span class="math notranslate nohighlight">\((n\times d)\)</span> matrix)?</p>
<div class="toggle docutils container">
<p>There are multiple ways to derive the Jacobian of this function. I believe the shortest, but not necessarily most obvious way is to use here the result from the exercise above and to employ the matrix product definition given by the outer-product in the column-times-row scheme:
<div class="math notranslate nohighlight">
\[ A\vvec{x} = A_{\cdot 1}x_1 + \ldots + A_{\cdot d}x_d.\]</div>

Now we can apply the linearity of the partial derivative of <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial}{\partial x_k} f(\vvec{x}) 
    &amp;= \frac{\partial}{\partial x_k} (\vvec{b} - A\vvec{x})
    = \frac{\partial}{\partial x_k} \vvec{b} - \frac{\partial}{\partial x_k}A\vvec{x}
    =  \vvec{0} - \frac{\partial}{\partial x_k}(A_{\cdot 1}x_1 + \ldots + A_{\cdot d}x_d)\\
    &amp;= - \frac{\partial}{\partial x_k}A_{\cdot 1}x_1 - \ldots - \frac{\partial}{\partial x_k} A_{\cdot k}x_k-\ldots - \frac{\partial}{\partial x_k}A_{\cdot d}x_d\\
    &amp;=-\frac{\partial}{\partial x_k} A_{\cdot k}x_k\\
    &amp;= -A_{\cdot k},
\end{align*}\]</div>
<p>where we applied for the partial derivatives the rule which we derived in the previous exercise for the Jacobian of a function from a scalar to a vector.</p>
<p>Now the question is how we have to arrange the partial derivatives to form the Jacobian. We can either look up in the slides how that goes, or we remember from the lecture that the dimensionality of the Jacobian is swapping the dimensionality from the input- and output space. Our function <span class="math notranslate nohighlight">\(f\)</span> maps from the <span class="math notranslate nohighlight">\(d\)</span>-dimensional space to the <span class="math notranslate nohighlight">\(n\)</span>-dimensional space. Hence, the dimensionality of the Jacobian is <span class="math notranslate nohighlight">\((n\times d)\)</span>, the same like our matrix <span class="math notranslate nohighlight">\(A\)</span>. Thus, the <span class="math notranslate nohighlight">\(n\)</span>-dimensional partial derivatives have to be concatenated horizontally:
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \vvec{x}} f(\vvec{x}) =  \begin{pmatrix} -A_{\cdot 1}&amp;\ldots&amp; -A_{\cdot d}\end{pmatrix} = -A.\]</div>
</p>
</div>
</li>
<li><p>What is the gradient of the function <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\rightarrow \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(f(\vvec{x}) = \lVert \vvec{b}-A\vvec{x}\rVert^2 \)</span>?</p>
<div class="toggle docutils container">
<p>Here, we can apply now the chainrule to the inner function <span class="math notranslate nohighlight">\(g(\vvec{x})= \vvec{b}-A\vvec{x}\)</span> and the outer function <span class="math notranslate nohighlight">\(h(\vvec{y})=\lVert \vvec{y}\rVert^2\)</span>. From the exercises before, we know the gradients of both functions:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \nabla_\vvec{x} g(\vvec{x}) 
    &amp;= \left(\frac{\partial}{\partial \vvec{x}}(\vvec{b}-A\vvec{x})\right)^\top = (-A)^\top = -A^\top\\
    \nabla_\vvec{y} h(\vvec{y}) 
    &amp;= \left(\frac{\partial}{\partial \vvec{y}}\lVert\vvec{y}\rVert^2\right)^\top=2\vvec{y}
\end{align*}\]</div>
<p>In the chain rule, the inner and outer gradients are multiplied. You can either look up the definition or deduce how the gradients have to be multiplied from the dimensionalities. The gradient of a function to the real values has the same dimensionality like the input space. Hence, we have a look how we can multiply the inner and outer gradients such that we get a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector. This is only the case if we multiply the gradient of the inner function with the gradient of the outer function. Therewith, we get:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \nabla_\vvec{x} h(g(\vvec{x}))
    &amp;= \nabla_\vvec{x} g(\vvec{x})\cdot \nabla_{g(\vvec{x})}h(g(\vvec{x}))\\
    &amp;= -A^\top(2(\vvec{b}-A\vvec{x}))\\
    &amp;= -2A^\top(\vvec{b}-A\vvec{x}).
\end{align*}\]</div>
</div>
</li>
<li><p>What is the gradient of the function <span class="math notranslate nohighlight">\(f:\mathbb{R}^{d\times r}\rightarrow \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(f(X)=\lVert D - YX^\top\rVert^2\)</span>, where <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}, Y\in\mathbb{R}^{n\times r}\)</span>?</p>
<div class="toggle docutils container">
<p>Let’s have first a look at the dimensionality of the resulting gradient. Since the function <span class="math notranslate nohighlight">\(f\)</span> is mapping to the real values, the dimensionality of the gradient is the same as the one of the input space: <span class="math notranslate nohighlight">\((n\times r)\)</span>. Since we do not know any gradients subject to matrices yet, we divide the problem and compute the gradient row-wise. Every row of <span class="math notranslate nohighlight">\(X\)</span> is mapped to the corresponding row of the gradient:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4bd3e305-a1da-4c84-b24a-127f63496fc2">
<span class="eqno">(5)<a class="headerlink" href="#equation-4bd3e305-a1da-4c84-b24a-127f63496fc2" title="Permalink to this equation">#</a></span>\[\begin{align}
    \nabla_Xf(X) = \begin{pmatrix}
- &amp;\nabla_{X_{1\cdot }}f(X) &amp;-\\&amp;\vdots&amp;\\-&amp;\nabla_{X_{d\cdot }}f(X)&amp;-\end{pmatrix}.\label{eq:gradX}
\end{align}\]</div>
<p>The gradient with regard to  row <span class="math notranslate nohighlight">\(X_{ k\cdot}\)</span> of the function <span class="math notranslate nohighlight">\(f\)</span> is equal to</p>
<div class="amsmath math notranslate nohighlight" id="equation-5ff77ce6-6164-4108-a43d-05f8366e6806">
<span class="eqno">(6)<a class="headerlink" href="#equation-5ff77ce6-6164-4108-a43d-05f8366e6806" title="Permalink to this equation">#</a></span>\[\begin{align}
    \nabla_{X_{k \cdot }f(X)} &amp;= \nabla_{X_{k \cdot }}\lVert D-YX^\top\rVert^2\nonumber\\
    &amp;= \nabla_{X_{k\cdot }}\sum_{i=1}^d\lVert D_{\cdot i}-YX_{i\cdot }^\top\rVert^2\label{eq:normdecomp}\\ 
    &amp;= \nabla_{X_{k\cdot }}\lVert D_{\cdot k}-YX_{k\cdot }^\top\rVert^2, \label{eq:lingrad}
\end{align}\]</div>
<p>where Eq.~\eqref{eq:lingrad}  derives from the linearity of the gradient. Eq.~\eqref{eq:normdecomp} follows from the fact that the squared Frobenius norm (matrix <span class="math notranslate nohighlight">\(L_2\)</span>-norm) is the sum of the squared Euclidean norms over all column- or row-vectors of a matrix. That is for any matrix <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times d}\)</span> we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \lVert A\rVert^2 =\sum_{i=1}^d\sum_{j=1}^n A_{ji}^2 
    = \sum_{i=1}^d\lVert A_{\cdot i}\rVert^2 
    = \sum_{j=1}^n\lVert A_{j\cdot}\rVert^2. 
\end{align*}\]</div>
<p>We can denote the gradient of the term in Eq.~\eqref{eq:lingrad}, as we have derived it in the previous exercise. We only have to keep in mind that we derived the gradient in the previous exercise subject to a column-vector and here we have the gradient with regard to the row vector <span class="math notranslate nohighlight">\(X_{k\cdot}\)</span>. Hence, we have to transpose the result from the previous exercise to get the gradient for our row-vector:
<div class="math notranslate nohighlight">
\[\nabla_{X_{k\cdot}}\lVert D_{\cdot k}-YX_{k\cdot}^\top\rVert^2
=(-2Y^\top(D_{\cdot k}-YX_{k\cdot}^\top))^\top
= -2 (D_{\cdot k}-YX_{k\cdot}^\top)^\top Y.\]</div>

We insert this result now in Eq.~\eqref{eq:gradX} and obtain the final result:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \nabla_Xf(X) &amp;= \begin{pmatrix}
- &amp;\nabla_{X_{1\cdot }}f(X) &amp;-\\&amp;\vdots&amp;\\-&amp;\nabla_{X_{d\cdot }}f(X)&amp;-\end{pmatrix}\\
&amp;= \begin{pmatrix}
-2 (D_{\cdot 1}-YX_{1\cdot}^\top)^\top Y \\\vdots\\
-2 (D_{\cdot d}-YX_{d\cdot}^\top)^\top Y\end{pmatrix}\\
&amp;=-2\begin{pmatrix}
 (D_{\cdot 1}-YX_{1\cdot}^\top)^\top  \\\vdots\\
 (D_{\cdot d}-YX_{d\cdot}^\top)^\top \end{pmatrix}Y\\
 &amp;=-2 (D-YX^\top)^\top Y.
\end{align*}\]</div>
</div>
</li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="optimization_gradients.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Matrix Derivatives</p>
      </div>
    </a>
    <a class="right-next"
       href="regression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-functions">Convex Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-optimization">Numerical Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-gradients">Computing the Gradients</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sibylle Hess
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022. Eindhoven University of Technology.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>