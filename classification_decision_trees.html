
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Decision Trees &#8212; Data Mining and Machine Learning Jupyter Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'classification_decision_trees';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Random Forests" href="classification_random_forests.html" />
    <link rel="prev" title="Naive Bayes" href="classification_naive_bayes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Data Mining and Machine Learning Jupyter Book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Data Mining and Machine Learning Jupyter Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linalg.html">Linear Algebra</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="linalg_spaces.html">Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_normed_vs.html">Normed Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="optimization.html">Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="optimization_problems.html">Optimization Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_convex.html">Convex Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_analytic.html">Analytic Solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_numerical.html">Numerical Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_gradients.html">Matrix Derivatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="regression_objective.html">Regression Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_functions.html">Regression Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_optimization.html">Minimizing the RSS</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_bias_var.html">The Bias-Variance Tradeoff</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_sparse.html">The Sparse Regression Task</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_ridge.html">Ridge Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_lasso.html">Lasso</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="classification.html">Classification</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="classification_problem.html">Classification Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_knn.html">K-Nearest Neighbor</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_naive_bayes.html">Naive Bayes</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_random_forests.html">Random Forests</a></li>

<li class="toctree-l2"><a class="reference internal" href="classification_svms.html">Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_kernel_svm.html">Kernel SVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="neuralnets.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_intro.html">From Linear Models to Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_mlps.html">MLPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_backprop.html">Backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_sgd.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_conv.html">Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_pooling.html">Pooling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="dim_reduction.html">Dimensionality Reduction Techniques</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_mf.html">Low Rank Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_pca.html">Principal Component Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="clustering.html">Clustering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="clustering_k_means.html">k-Means</a></li>



<li class="toctree-l2"><a class="reference internal" href="clustering_k_means_mf.html">k-Means is MF</a></li>


<li class="toctree-l2"><a class="reference internal" href="clustering_kernel_kmeans.html">Kernel k-means</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering_spectral.html">Spectral Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tue-ml/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/edit/main/classification_decision_trees.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/issues/new?title=Issue%20on%20page%20%2Fclassification_decision_trees.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/classification_decision_trees.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Decision Trees</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-splits">Finding Splits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-gain">Information Gain</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-criterion">Stopping Criterion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">Decision Boundary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="decision-trees">
<h1>Decision Trees<a class="headerlink" href="#decision-trees" title="Link to this heading">#</a></h1>
<figure class="align-center" id="got-decision-tree">
<a class="reference internal image-reference" href="_images/GOT_DT.png"><img alt="_images/GOT_DT.png" src="_images/GOT_DT.png" style="height: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">A decision tree Game of Thrones personality test.</span><a class="headerlink" href="#got-decision-tree" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Decision trees are a fundamental method in machine learning due to their inherently interpretable structure. They model decisions in a tree-like structure, where each internal node splits the outcomes based on a feature value, until we arrive at a leaf node that assigns a class prediction. This structure is so intuitive, that no further information is needed to perform the inference step. As an example, have a look at the Game of Thrones personality test decision tree above. You probably already got your character prediction without having to know what a decision tree is. This is particularly useful when you work in application domains where your project partners have no deepened mathematical training, for example in the medical domain. You can show a decision tree to experts of their field and they will be able to assess whether the predictions are sensible. Of course, this advantage gets lost if the decision tree is very big.</p>
<p>One of the most widely used algorithms for constructing decision trees is the CART (Classification and Regression Trees) algorithm, which builds binary decision trees using impurity measures such as Gini impurity (for classification).</p>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Link to this heading">#</a></h2>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 23 </span> (Decision Tree)</p>
<section class="definition-content" id="proof-content">
<p>A Decision Tree is a directed binary tree that represents a classifying function <span class="math notranslate nohighlight">\(f_{dt}:\mathbb{R}^d\rightarrow [0,1]^c\)</span>. The classifying function is defined recursively:
<div class="math notranslate nohighlight">
\[f_{dt}(\vvec{x})=q(\vvec{x})f_0(\vvec{x}) + (1-q(\vvec{x}))f_1(\vvec{x})\]</div>

where</p>
<ul class="simple">
<li><p>the <strong>decision function</strong> <span class="math notranslate nohighlight">\(q:\mathbb{R}^d\rightarrow \{0,1\}\)</span> performs a test on a feature of the input <span class="math notranslate nohighlight">\(\vvec{x}\)</span> that is either true (returning 1) or false (returning 0).</p></li>
<li><p>the function <span class="math notranslate nohighlight">\(f_b:\mathbb{R}^d\rightarrow [0,1]^c\)</span> returns either directly a probability vector over all classes, or it is recursively defined as
<div class="math notranslate nohighlight">
\[f_b(\vvec{x}) = q_b(\vvec{x}){f_b}_0(\vvec{x}) + (1-q_b(\vvec{x})){f_b}_1(\vvec{x}).\]</div>
</p></li>
</ul>
<p>In the tree, these functions are represented by</p>
<ul class="simple">
<li><p><strong>Decision nodes:</strong> each decision function <span class="math notranslate nohighlight">\(q_b\)</span> is represented by an internal node, performing a split based on feature values.</p></li>
<li><p><strong>Prediction nodes:</strong> each function <span class="math notranslate nohighlight">\(f_b(\vvec{x})=\vvec{p}\)</span> that directly returns a probability vector over all classes, (that is hence not defined over further recursions) is represented by a leaf node.</p></li>
</ul>
</section>
</div><p>Although decision trees are often represented as if they only store the predicted label at a leaf, in practice, the class probabilities are stored. Class probabilities are useful as a confidence measure of the prediction <span class="math notranslate nohighlight">\( \hat{y} = \argmax_y\ f_{dt}(\vvec{x}) \)</span>.</p>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 19 </span></p>
<section class="example-content" id="proof-content">
<p>The data underlying the GOT personality test dataset could for example have the features <span class="math notranslate nohighlight">\(\mathtt{x}_1,\ldots,\mathtt{x}_4\)</span>, where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_1\)</span> represents the glasses of wine per day (contiuous feature)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_2\)</span> represents the ability to get others to do their work (binary feature)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_3\)</span> represents the characteristic to burn everything down if issues persist (binary feature)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_4\)</span> represents the reliance on lists to organize big projects (binary feature).</p></li>
</ul>
<p>The root node (a decision node) of our GOT personality test could then be expressed by the split function
<div class="math notranslate nohighlight">
\[\begin{split}q(\vvec{x})=\begin{cases}1 &amp; \text{if }x_1\geq 5\\0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>

We build the decision tree classifier now recursively. We have
<div class="math notranslate nohighlight">
\[f_{dt}(\vvec{x})=q(\vvec{x})f_0(\vvec{x}) + (1-q(\vvec{x}))f_1(\vvec{x}).\]</div>

The function <span class="math notranslate nohighlight">\(f_0\)</span> represents the left subtree and the function <span class="math notranslate nohighlight">\(f_1\)</span> the right subtree. The left subtree is defined as
<div class="math notranslate nohighlight">
\[f_0(\vvec{x})=q_0(\vvec{x}){f_0}_0(\vvec{x})+(1-q_0(\vvec{x})){f_0}_1(\vvec{x}),\]</div>

where
<div class="math notranslate nohighlight">
\[\begin{split}q_0(\vvec{x})=\begin{cases}1 &amp; \text{if }x_2=1\\0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>

and <span class="math notranslate nohighlight">\({f_0}_0(\vvec{x})\)</span> and <span class="math notranslate nohighlight">\({f_0}_1(\vvec{x})\)</span> are returning a probability vector over the five classes, where the highest probability of <span class="math notranslate nohighlight">\({f_0}_0(\vvec{x})\)</span> is assigned for class <code class="docutils literal notranslate"><span class="pre">Cersei</span></code> and the highest probability of <span class="math notranslate nohighlight">\({f_0}_1(\vvec{x})\)</span> is assigned for class <code class="docutils literal notranslate"><span class="pre">Tyrion</span></code>.</p>
<p>The right subtree is defined as
<div class="math notranslate nohighlight">
\[f_1(\vvec{x})=q_1(\vvec{x}){f_1}_0(\vvec{x})+(1-q_1(\vvec{x})){f_0}_1(\vvec{x}),\]</div>

where
<div class="math notranslate nohighlight">
\[\begin{split}q_1(\vvec{x})=\begin{cases}1 &amp; \text{if }x_3=1\\0 &amp; \text{ otherwise}\end{cases}.\end{split}\]</div>

<span class="math notranslate nohighlight">\({f_1}_0(\vvec{x})\)</span> is represented by a prediction node and returns a probability vector where the highest probability is assigned to <code class="docutils literal notranslate"><span class="pre">Daenerys</span></code>. The function <span class="math notranslate nohighlight">\({f_0}_1\)</span> returns a subtree that is defined as
<div class="math notranslate nohighlight">
\[{f_0}_1(\vvec{x})={q_0}_1(\vvec{x}){{f_0}_1}_0(\vvec{x})+(1-{q_0}_1(\vvec{x})){{f_0}_1}_1(\vvec{x}),\]</div>

where
<div class="math notranslate nohighlight">
\[\begin{split}{q_0}_1(\vvec{x})=\begin{cases}1 &amp; \text{if }x_4=1\\0 &amp; \text{ otherwise}\end{cases}\end{split}\]</div>

and <span class="math notranslate nohighlight">\({{f_0}_1}_0(\vvec{x})\)</span> returns a probability vector with the highest probability assigned to <code class="docutils literal notranslate"><span class="pre">Arya</span></code> and <span class="math notranslate nohighlight">\({{f_0}_1}_1(\vvec{x})\)</span> assigns the highest probability to <code class="docutils literal notranslate"><span class="pre">John</span> <span class="pre">Snow</span></code>.</p>
</section>
</div></section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h2>
<p>We provide the pseudocode for the recursive creation of the Classification and Regression Tree (CART) algorithm <span id="id1">[<a class="reference internal" href="bibliography.html#id8" title="Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classification and regression trees. Wadsworth Inc, 1984.">2</a>]</span> in <a class="reference internal" href="#dt_training">Algorithm 8</a>. The input of CART is here specified to be the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> only, but in principle CART has various hyperparameters that are here summarized by the hard-coded stopping criterion. We discuss possible stopping criteria in a subsection below. If the stopping criterium is satisfied, then the CART algorithm returns a probability vector that reflects the empirical class-distribution in the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{\cal D} (y) = \frac{\lvert\{({\bf x}, l) \in {\cal D}\mid  l = y \}\rvert}{\lvert{\cal D}\rvert}. 
\end{align*}\]</div>
<p>Otherwise, a split is found (to be discussed below), returning the decision function <span class="math notranslate nohighlight">\(q\)</span> and the split datasets <span class="math notranslate nohighlight">\(\mathcal{L}_0\cup \mathcal{L}_1 = \mathcal{D}\)</span>. CART proceeds then with the recursion.</p>
<div class="proof algorithm admonition" id="dt_training">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Classification and Regression Tree (CART))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> The dataset <span class="math notranslate nohighlight">\({\cal D} \)</span><br />
<strong>Function</strong> <span class="math notranslate nohighlight">\(\mathtt{CART}({\cal D})\)</span></p>
<ol class="arabic simple">
<li><p><strong>if</strong> stopping criterion is satisfied:</p>
<ol class="arabic simple">
<li><p><strong>return</strong> class probability vector <span class="math notranslate nohighlight">\(\vvec{p}\in[0,1]^c\)</span> such that <span class="math notranslate nohighlight">\(p_y = p_\mathcal{D}(y)\)</span> for <span class="math notranslate nohighlight">\(1\leq y\leq c\)</span></p></li>
</ol>
</li>
<li><p><strong>else</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(q,\ \mathcal{L}_0\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1\leftarrow\mathtt{split}(\mathcal{D})\)</span></p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(q\circ \mathtt{CART}(\mathcal{L}_0) + (1-q) \circ \mathtt{CART}(\mathcal{L}_1) \)</span></p></li>
</ol>
</li>
</ol>
</section>
</div><section id="finding-splits">
<h3>Finding Splits<a class="headerlink" href="#finding-splits" title="Link to this heading">#</a></h3>
<p>Decision tree splits are determined using a greedy approach, meaning each split is chosen based on the immediate best improvement for the classification task. Greedy methods are usually easy to implement and to understand and provide good enough solutions, but the model decisions are often short sighted, such that the resulting tree might be bigger than it would be needed. The improvement is measured by the information gain (<span class="math notranslate nohighlight">\(IG\)</span>), which is high when the resulting child nodes have a purer class distribution than the parent node. A more detailed discussion of this concept follows below.</p>
<div class="proof algorithm admonition" id="dt_split">
<p class="admonition-title"><span class="caption-number">Algorithm 9 </span> (Split)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> A dataset <span class="math notranslate nohighlight">\({\cal D}\)</span><br />
<strong>Function</strong> <span class="math notranslate nohighlight">\(\mathtt{split}({\cal D})\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathtt{max\_ig}\)</span>, <span class="math notranslate nohighlight">\(q^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_0^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1^*\)</span> <span class="math notranslate nohighlight">\(\leftarrow 0,\ \emptyset,\ \emptyset,\ \emptyset\)</span></p></li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(k\in\{1,\ldots, d\}\)</span></p>
<ol class="arabic simple">
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(t\in\mathtt{thresholds}(\mathtt{x}_k, \mathcal{D})\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(q(\vvec{x})=\begin{cases}1&amp; \text{ if } x_k= t\ (\mathtt{x}_k \text{ is discrete) or } x_k\geq t\ (\mathtt{x}_k \text{ is continuous})\\0&amp; \text{ otherwise}\end{cases}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_0\leftarrow \{(\vvec{x},y)\in\mathcal{D}\mid q(\vvec{x})=1\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_1\leftarrow \mathcal{D}\setminus \mathcal{L}_0\)</span></p></li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(IG(\mathcal{D},\mathcal{L}_0,\mathcal{L}_1)&gt;\mathtt{max\_ig}\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathtt{max\_ig}\leftarrow IG(\mathcal{L}_0,\mathcal{L}_1,\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_0^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1^*\leftarrow \mathcal{L}_0,\ \mathcal{L}_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(q^*\gets q\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(q^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_0^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1^*\)</span></p></li>
</ol>
</section>
</div><p>The function <span class="math notranslate nohighlight">\(\mathtt{thresholds}(\mathtt{x}_k,\mathcal{D})\)</span> can be defined in various ways. A straightforward choice is for discrete features <span class="math notranslate nohighlight">\(\mathtt{x}_k\)</span> to return the set of unique values in the dataset:
<div class="math notranslate nohighlight">
\[\mathtt{thresholds}(\mathtt{x}_k,\mathcal{D}) = \{x_k\mid (\vvec{x},y)\in\mathcal{D}\}.\]</div>

If the feature is continuous, then we can try as thresholds the midpoints between consecutive values of the feature. Let <span class="math notranslate nohighlight">\(\vvec{x}_{(i)}\)</span> denote the observation with the <span class="math notranslate nohighlight">\(i\)</span>-th smallest feature value <span class="math notranslate nohighlight">\(x_k\)</span>, then we define
<div class="math notranslate nohighlight">
\[\mathtt{thresholds}(\mathtt{x}_k,\mathcal{D}) = \left\{\frac{{x_{(i)}}_k + {x_{(i+1)}}_k}{2}\mid (\vvec{x}_{(i)},y),(\vvec{x}_{(i+1)},y')\in\mathcal{D}\right\}.\]</div>
</p>
</section>
<section id="information-gain">
<h3>Information Gain<a class="headerlink" href="#information-gain" title="Link to this heading">#</a></h3>
<p>The information gain measures how beneficial a split is to the classification task. A split is defined over a data sub-set <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> that is split into disjoint data sub-sets <span class="math notranslate nohighlight">\(\mathcal{L}=\mathcal{L}_0\cup\mathcal{L}_1\)</span>.  The more <em>pure</em> the split datasets <span class="math notranslate nohighlight">\(\mathcal{L}_0\)</span> and <span class="math notranslate nohighlight">\(\mathcal{L}_1\)</span> are in comparison to their parent dataset <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>, the higher the information gain.</p>
<div class="math notranslate nohighlight" id="equation-information-gain">
<span class="eqno">(36)<a class="headerlink" href="#equation-information-gain" title="Link to this equation">#</a></span>\[IG(\mathcal{L},{\cal L}_0, {\cal L}_1) = \mathrm{Impurity}({\cal L}) - \left\{ \frac{\lvert {\cal L}_0\rvert}{\lvert{\cal L}\rvert} \mathrm{Impurity}({\cal L}_0) + \frac{\lvert{\cal L}_1\rvert}{\lvert{\cal L}\rvert} \mathrm{Impurity}({\cal L}_{1}) \right\} \]</div>
<p>The function <span class="math notranslate nohighlight">\(\mathrm{Impurity}\)</span> measures the uncertainty of the prediction of the class with the highest empirical disctribution <span class="math notranslate nohighlight">\(p_{\cal L}(y)\)</span>. If the empirical distribution is very high for one class and very low for the remaining classes, then the impurity is low. We consider the following definitions of impurity:</p>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 24 </span> (Impurity measures)</p>
<section class="definition-content" id="proof-content">
<p>We define the following three impurity measures that can be used to compute the information gain:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
G({\cal L}) &amp; = 1 - \sum_{y=1}^c p_{\cal L}(y)^{2} &amp;\text{ (Gini impurity)}\\
E({\cal L}) &amp;= - \sum_{y =1}^c p_{\cal L}(y) \log p_{\cal L}(y) &amp;\text{ (Entropy)}\\
C({\cal L}) &amp;= 1 - \max_{1\leq y \leq c} p_{\cal L}(y) &amp;\text{(Self classification error)}
\end{align*}\]</div>
</section>
</div><p>For a comparison of the impurity measures, we plot them for the case of two classes. The horizontal axis indicates the empirical distribution of one of the two classes (class <span class="math notranslate nohighlight">\(y=0\)</span>).</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-24d617119f0db07dcb518ed21653fadc6a1354f3.png" alt="Figure made with TikZ" /></p>
</div><p>We observe that there are no big differences between the three impurity plots. Gini impurity and entropy increase visibly more steeply from the completely pure class distributions where <span class="math notranslate nohighlight">\(p_{\mathcal{L}}(0)\)</span> is either zero or one.</p>
</section>
<section id="stopping-criterion">
<h3>Stopping Criterion<a class="headerlink" href="#stopping-criterion" title="Link to this heading">#</a></h3>
<p>Common stopping criterions, that can also be combined, are the following:</p>
<ul class="simple">
<li><p>Maximum depth is reached: in this case, we should keep track of the recursion level to see whether we exceed a predefined hyperparameter indicating the maximum tree depth.</p></li>
<li><p>Minimum number of samples per split: stop if one of the split sets <span class="math notranslate nohighlight">\(\mathcal{L}_0\)</span> or <span class="math notranslate nohighlight">\(\mathcal{L}_1\)</span> contain fewer observations than a predefined threshold.</p></li>
<li><p>No significant impurity gain: test after evoking the <span class="math notranslate nohighlight">\(\mathtt{split}\)</span> function if the impurity gain is exceeding a specified threshold.</p></li>
</ul>
<p>The stopping criterion is the main property to prohibit overfitting, which is generally associated with a big tree. In addition, pruning strategies exist to remove less significant branches after the training. But this is out of the scope of this section.</p>
</section>
</section>
<section id="decision-boundary">
<h2>Decision Boundary<a class="headerlink" href="#decision-boundary" title="Link to this heading">#</a></h2>
<p>The plot below indicates the decision boundary and the corresponding tree for the two moons dataset, setting the maximum tree depth to five. We observe the typical rectangular decision boundaries of the decision tree, that stem from feature-wise partitioning of the space. Each decision node partitions the feature space in two halves. The root node splits at <span class="math notranslate nohighlight">\(x[1]\leq 0.218\)</span>, resulting in a horizontal split that separates roughly the two moons from each other. The next split in the lower horizontal half (for <span class="math notranslate nohighlight">\(x[1]\leq 0.218\)</span>) is a vertical split at <span class="math notranslate nohighlight">\(x[0]\leq 0.363\)</span>. The rectangle defined by <span class="math notranslate nohighlight">\(x[1]\leq 0.218\)</span> and <span class="math notranslate nohighlight">\(x[0]\leq 0.363\)</span> is then assigned to the blue class by the prediction node when going twice left in the decision tree. This way, rectangular decision boundaries are created.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.inspection</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">ListedColormap</span><span class="p">,</span> <span class="n">LinearSegmentedColormap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>


<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#a0c3ff&quot;</span><span class="p">,</span> <span class="s2">&quot;#ffa1cf&quot;</span><span class="p">])</span>
<span class="n">cm_points</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#007bff&quot;</span><span class="p">,</span> <span class="s2">&quot;magenta&quot;</span><span class="p">])</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
    <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
    <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">cm_points</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Decision tree training data and decision boundary.</span><span class="se">\n</span><span class="s2">Test Acc </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;scaled&#39;</span><span class="p">)</span>

<span class="c1"># Plot tree</span>
<span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">impurity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DT&quot;</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/f8ca6f6b2920c8ac98471ad25e175b0ee5b56c31eec5c7a372f7a55025fbbbcc.png" src="_images/f8ca6f6b2920c8ac98471ad25e175b0ee5b56c31eec5c7a372f7a55025fbbbcc.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="classification_naive_bayes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Naive Bayes</p>
      </div>
    </a>
    <a class="right-next"
       href="classification_random_forests.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Random Forests</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-splits">Finding Splits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-gain">Information Gain</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-criterion">Stopping Criterion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">Decision Boundary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sibylle Hess
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022. Eindhoven University of Technology.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>