{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercises",
   "id": "f13efdfb5214c6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:08.750301Z",
     "start_time": "2025-08-25T16:44:08.747523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.datasets import load_iris"
   ],
   "id": "5a3d388bd27cbd75",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clone this notebook and implement the exercises yourself, then check the results with the checking methods. Answers are provided below the exercises.",
   "id": "8fb7d3219f28f423"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Type Classification\n",
    "Classify each dataset as nominal, ordinal, discrete, or continuous. Understanding these fundamental data types is crucial for selecting appropriate preprocessing techniques. Replace 'your_category_here' with the correct classification for each dataset in the list."
   ],
   "id": "c150e091d8e548a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:08.799070Z",
     "start_time": "2025-08-25T16:44:08.794303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _ex_1_error_message():\n",
    "    raise ValueError(\"Exercise 1: Incorrect classification\")\n",
    "\n",
    "def check_ex_1(data):\n",
    "    categories = [\"nominal\", \"ordinal\", \"discrete\", \"continuous\"]\n",
    "    for d in data:\n",
    "        if d[0] not in categories:\n",
    "            raise ValueError(f\"All data classifications should be one of the following: {categories}\")\n",
    "\n",
    "    # Answers here\n",
    "    if not data[0][0] == \"nominal\":\n",
    "        _ex_1_error_message()\n",
    "    if not data[1][0] == \"ordinal\":\n",
    "        _ex_1_error_message()\n",
    "    if not data[2][0] == \"ordinal\":\n",
    "        _ex_1_error_message()\n",
    "    if not data[3][0] == \"nominal\":\n",
    "        _ex_1_error_message()\n",
    "    if not data[4][0] == \"discrete\":\n",
    "        _ex_1_error_message()\n",
    "    if not data[5][0] == \"continuous\":\n",
    "        _ex_1_error_message()\n",
    "\n",
    "    print(\"Exercise 1: Correct!\")"
   ],
   "id": "e37c6a08a08d43ef",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:08.855790Z",
     "start_time": "2025-08-25T16:44:08.847991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_1():\n",
    "    \"\"\"For each of the data types below, classify them as\n",
    "    nominal, ordinal, discrete or continuous.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        ('your_category_here', ['red', 'blue', 'green', 'red', 'blue']),\n",
    "        ('your_category_here', ['low', 'medium', 'high', 'low', 'medium']),\n",
    "        ('your_category_here', ['fast', 'slow', 'slowest']),\n",
    "        ('your_category_here', ['happy', 'disgusted', 'angry', 'sad', 'happy']),\n",
    "        ('your_category_here', [1, 2, 3, 1, 2]),\n",
    "        ('your_category_here', [1.5, 2.7, 3.1, 1.8, 2.9]),\n",
    "    ]\n",
    "    # Uncomment check\n",
    "    # check_ex_1(data)\n",
    "\n",
    "ex_1()"
   ],
   "id": "55d5e0528bc675b6",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ordinal Encoding\n",
    "Transform categorical education levels into numerical values that preserve their natural ordering. Create a mapping where 'high school' = 0, 'bachelor' = 1, 'master' = 2, and 'phd' = 3, then encode the given data accordingly."
   ],
   "id": "bd24c4febede61f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:08.909439Z",
     "start_time": "2025-08-25T16:44:08.904498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_2(data):\n",
    "    answer = [1, 0, 2, 3, 0]\n",
    "    if not answer == data:\n",
    "        raise ValueError(\"Exercise 2: Incorrect labels\")\n",
    "    print(\"Exercise 2: Correct!\")"
   ],
   "id": "49bfe23b5ecba114",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:08.962940Z",
     "start_time": "2025-08-25T16:44:08.958113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_2():\n",
    "    \"\"\"Encode the following values using ordinal encoding (and pandas, for practice).\"\"\"\n",
    "    data = ['high school', 'bachelor', 'master', 'phd', 'bachelor']\n",
    "\n",
    "    # Encode - Your implementation here\n",
    "    encoded_data = [0, 0, 0, 0, 0]\n",
    "\n",
    "    # Check the mapping, which should be a list of length 'data' where every entry is the correct encoding\n",
    "    # Uncomment check\n",
    "    # check_ex_2(encoded_data)\n",
    "\n",
    "ex_2()"
   ],
   "id": "c6727de44dabb75f",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## One-Hot Encoding\n",
    "Convert categorical animal names into binary feature vectors where each unique category gets its own column. Each row should have exactly one '1' and the rest '0's, creating a sparse representation suitable for machine learning algorithms."
   ],
   "id": "5fd6aa7a7dc506cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.008562Z",
     "start_time": "2025-08-25T16:44:09.006361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_3(data):\n",
    "    \"\"\"Check one-hot encoding implementation.\"\"\"\n",
    "    expected = [\n",
    "        [1, 0, 0, 0],  # cat (in alphabetical order: bird, cat, dog, fish)\n",
    "        [0, 1, 0, 0],  # dog\n",
    "        [0, 0, 1, 0],  # bird\n",
    "        [1, 0, 0, 0],  # cat\n",
    "        [0, 0, 0, 1]   # fish\n",
    "    ]\n",
    "\n",
    "    if data != expected:\n",
    "        raise ValueError(\"Exercise 3: Incorrect one-hot encoding\")\n",
    "    print(\"Exercise 3: Correct!\")"
   ],
   "id": "8cc24a3fd7918839",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.055412Z",
     "start_time": "2025-08-25T16:44:09.053059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_3():\n",
    "    \"\"\"Implement one-hot encoding for the given data.\"\"\"\n",
    "    data = ['cat', 'dog', 'bird', 'cat', 'fish']\n",
    "\n",
    "    # Create one-hot encoding - Your implementation here\n",
    "    one_hot_data = [\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 0, 0]\n",
    "    ]\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_3(one_hot_data)\n",
    "\n",
    "ex_3()"
   ],
   "id": "8efea29cc42ee312",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Basic Statistics Calculation\n",
    "Compute fundamental statistical measures (mode, median, mean, variance) for both discrete and continuous datasets. Pay attention to how these statistics differ between data types and what insights they provide about data distribution."
   ],
   "id": "d525d96cf68529a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.101721Z",
     "start_time": "2025-08-25T16:44:09.098376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_4(discrete_stats, continuous_stats, discrete_data, continuous_data):\n",
    "    \"\"\"Check statistics calculation using numpy.\"\"\"\n",
    "    expected_discrete = {\n",
    "        'mode': max(set(discrete_data), key=discrete_data.count),\n",
    "        'median': np.median(discrete_data),\n",
    "        'mean': np.mean(discrete_data),\n",
    "        'variance': np.var(discrete_data)\n",
    "    }\n",
    "\n",
    "    expected_continuous = {\n",
    "        'mode': max(set(continuous_data), key=continuous_data.count),\n",
    "        'median': np.median(continuous_data),\n",
    "        'mean': np.mean(continuous_data),\n",
    "        'variance': np.var(continuous_data)\n",
    "    }\n",
    "\n",
    "    # Check discrete stats\n",
    "    for key in expected_discrete:\n",
    "        if abs(discrete_stats[key] - expected_discrete[key]) > 1e-6:\n",
    "            raise ValueError(f\"Exercise 4: Incorrect {key} for discrete data\")\n",
    "\n",
    "    # Check continuous stats\n",
    "    for key in expected_continuous:\n",
    "        if abs(continuous_stats[key] - expected_continuous[key]) > 1e-6:\n",
    "            raise ValueError(f\"Exercise 4: Incorrect {key} for continuous data\")\n",
    "\n",
    "    print(\"Exercise 4: Correct!\")"
   ],
   "id": "4382998d1e304e52",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.153354Z",
     "start_time": "2025-08-25T16:44:09.150945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_4():\n",
    "    \"\"\"Calculate basic statistics for the given data.\"\"\"\n",
    "    discrete_data = [1, 2, 2, 3, 4, 4, 4, 5]\n",
    "    continuous_data = [1.1, 2.3, 2.3, 3.7, 4.2, 4.2, 4.8, 5.1]\n",
    "\n",
    "    # Calculate statistics - Your implementation here\n",
    "    discrete_stats = {\n",
    "        'mode': 0,\n",
    "        'median': 0,\n",
    "        'mean': 0,\n",
    "        'variance': 0\n",
    "    }\n",
    "\n",
    "    # Calculate statistics for continuous data\n",
    "    continuous_stats = {\n",
    "        'mode': 0,\n",
    "        'median': 0,\n",
    "        'mean': 0,\n",
    "        'variance': 0\n",
    "    }\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_4(discrete_stats, continuous_stats, discrete_data, continuous_data)\n",
    "\n",
    "ex_4()"
   ],
   "id": "98537376f141ec09",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train-Test Split Implementation\n",
    "Manually implement an 80/20 train-test split without using sklearn. Ensure your split maintains the original data relationships and provides representative samples for both training and testing phases."
   ],
   "id": "a22668fa681c839f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.205274Z",
     "start_time": "2025-08-25T16:44:09.199962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_5(training_features, testing_features, training_labels, testing_labels):\n",
    "    \"\"\"Check train-test split implementation.\"\"\"\n",
    "    # Check shapes\n",
    "    if training_features.shape[0] != 4 or testing_features.shape[0] != 2:\n",
    "        raise ValueError(\"Exercise 5: Incorrect train-test split sizes\")\n",
    "\n",
    "    if training_labels.shape[0] != 4 or testing_labels.shape[0] != 2:\n",
    "        raise ValueError(\"Exercise 5: Incorrect train-test split sizes for labels\")\n",
    "\n",
    "    # Check that all data is included\n",
    "    all_features = np.vstack([training_features, testing_features])\n",
    "    all_labels = np.concatenate([training_labels, testing_labels])\n",
    "\n",
    "    if len(all_features) != 6 or len(all_labels) != 6:\n",
    "        raise ValueError(\"Exercise 5: Data loss in train-test split\")\n",
    "\n",
    "    print(\"Exercise 5: Correct!\")"
   ],
   "id": "273af6436a34e5c5",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.265815Z",
     "start_time": "2025-08-25T16:44:09.260822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_5():\n",
    "    \"\"\"Implement 80/20 train-test split manually.\"\"\"\n",
    "    # Create sample data\n",
    "    feature_matrix = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "    target_labels = np.array([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "    # Split data - Your implementation here\n",
    "    training_features = np.array([])\n",
    "    testing_features = np.array([])\n",
    "    training_labels = np.array([])\n",
    "    testing_labels = np.array([])\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_5(training_features, testing_features, training_labels, testing_labels)\n",
    "\n",
    "ex_5()"
   ],
   "id": "23110cbe9965740",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Min-Max Scaling\n",
    "Scale the iris dataset features to a [0,1] range using the min-max normalization formula: (x - min) / (max - min). This technique preserves the original distribution shape while standardizing the scale across all features."
   ],
   "id": "9806c1fbf12183c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.313579Z",
     "start_time": "2025-08-25T16:44:09.310534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_6(minmax_scaled_features):\n",
    "    \"\"\"Check min-max scaling implementation using sklearn.\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Use sklearn MinMaxScaler\n",
    "    sklearn_scaler = MinMaxScaler()\n",
    "    sklearn_scaled = sklearn_scaler.fit_transform(feature_data)\n",
    "\n",
    "    # Check that scaled data is between 0 and 1\n",
    "    if np.any(minmax_scaled_features < 0) or np.any(minmax_scaled_features > 1):\n",
    "        raise ValueError(\"Exercise 6: Min-max scaled values should be between 0 and 1\")\n",
    "\n",
    "    # Check that at least one value is 0 and one is 1 for each feature\n",
    "    for col in range(minmax_scaled_features.shape[1]):\n",
    "        if not (np.min(minmax_scaled_features[:, col]) == 0 and np.max(minmax_scaled_features[:, col]) == 1):\n",
    "            raise ValueError(\"Exercise 6: Min-max scaling should map min to 0 and max to 1\")\n",
    "\n",
    "    # Compare with sklearn implementation\n",
    "    if not np.allclose(minmax_scaled_features, sklearn_scaled, rtol=1e-10):\n",
    "        raise ValueError(\"Exercise 6: Implementation doesn't match sklearn MinMaxScaler\")\n",
    "\n",
    "    print(\"Exercise 6: Correct!\")"
   ],
   "id": "20d995f9e23d5d91",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.370786Z",
     "start_time": "2025-08-25T16:44:09.364784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_6():\n",
    "    \"\"\"Implement min-max scaling.\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Implement min-max scaling - Your implementation here\n",
    "    minmax_scaled_features = feature_data\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_6(minmax_scaled_features)\n",
    "\n",
    "ex_6()"
   ],
   "id": "38711eb3d905e05e",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Standardization\n",
    "Transform the iris dataset to have zero mean and unit variance using the formula: (x - mean) / std. This technique is particularly useful when features have different units or vastly different scales."
   ],
   "id": "9db49c4b180a8019"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.422669Z",
     "start_time": "2025-08-25T16:44:09.417206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_7(standardized_features):\n",
    "    \"\"\"Check standardization implementation using sklearn.\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Use sklearn StandardScaler\n",
    "    sklearn_scaler = StandardScaler()\n",
    "    sklearn_standardized = sklearn_scaler.fit_transform(feature_data)\n",
    "\n",
    "    # Check that standardized data has mean close to 0 and std close to 1\n",
    "    for col in range(standardized_features.shape[1]):\n",
    "        if abs(np.mean(standardized_features[:, col])) > 1e-10:\n",
    "            raise ValueError(\"Exercise 7: Standardized data should have mean close to 0\")\n",
    "        if abs(np.std(standardized_features[:, col]) - 1) > 1e-10:\n",
    "            raise ValueError(\"Exercise 7: Standardized data should have standard deviation close to 1\")\n",
    "\n",
    "    # Compare with sklearn implementation\n",
    "    if not np.allclose(standardized_features, sklearn_standardized, rtol=1e-10):\n",
    "        raise ValueError(\"Exercise 7: Implementation doesn't match sklearn StandardScaler\")\n",
    "\n",
    "    print(\"Exercise 7: Correct!\")"
   ],
   "id": "28dbff68b5a51f16",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.481543Z",
     "start_time": "2025-08-25T16:44:09.474634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_7():\n",
    "    \"\"\"Implement standardization (z-score normalization).\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Implement standardization - Your implementation here\n",
    "    standardized_features = feature_data\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_7(standardized_features)\n",
    "\n",
    "ex_7()"
   ],
   "id": "c8e462bc13945aeb",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Robust Scaling\n",
    "Apply robust scaling using median and interquartile range (IQR) instead of mean and standard deviation. This method is less sensitive to outliers: (x - median) / IQR, making it ideal for datasets with extreme values."
   ],
   "id": "d6e4c8a63cd63a60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.531423Z",
     "start_time": "2025-08-25T16:44:09.525375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_8(robust_scaled_features):\n",
    "    \"\"\"Check robust scaling implementation using sklearn.\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Use sklearn RobustScaler\n",
    "    sklearn_scaler = RobustScaler()\n",
    "    sklearn_robust = sklearn_scaler.fit_transform(feature_data)\n",
    "\n",
    "    # Check that robust scaled data has median close to 0\n",
    "    for col in range(robust_scaled_features.shape[1]):\n",
    "        if abs(np.median(robust_scaled_features[:, col])) > 1e-10:\n",
    "            raise ValueError(\"Exercise 8: Robust scaled data should have median close to 0\")\n",
    "\n",
    "    # Compare with sklearn implementation\n",
    "    if not np.allclose(robust_scaled_features, sklearn_robust, rtol=1e-10):\n",
    "        raise ValueError(\"Exercise 8: Implementation doesn't match sklearn RobustScaler\")\n",
    "\n",
    "    print(\"Exercise 8: Correct!\")"
   ],
   "id": "ecccbf1e63e974f5",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.589527Z",
     "start_time": "2025-08-25T16:44:09.581741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_8():\n",
    "    \"\"\"Implement robust scaling using median and IQR.\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Implement robust scaling - Your implementation here\n",
    "    robust_scaled_features = feature_data\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_8(robust_scaled_features)\n",
    "\n",
    "ex_8()"
   ],
   "id": "c95ea5ae3cc55952",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Variance Thresholding\n",
    "Implement feature selection by removing features with low variance (below 0.5). Low-variance features provide little information for distinguishing between samples and can be safely removed to reduce dimensionality."
   ],
   "id": "6a7731472da75369"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.643568Z",
     "start_time": "2025-08-25T16:44:09.637115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_9(selected_feature_matrix, high_variance_features):\n",
    "    \"\"\"Check variance thresholding implementation.\"\"\"\n",
    "    # Check that low variance feature (index 2) is removed\n",
    "    if high_variance_features[2]:\n",
    "        raise ValueError(\"Exercise 9: Low variance feature should be removed\")\n",
    "\n",
    "    # Check that high variance features are kept\n",
    "    if not high_variance_features[0] or not high_variance_features[1]:\n",
    "        raise ValueError(\"Exercise 9: High variance features should be kept\")\n",
    "\n",
    "    # Check shape of selected data\n",
    "    if selected_feature_matrix.shape[1] != 2:\n",
    "        raise ValueError(\"Exercise 9: Should select 2 features\")\n",
    "\n",
    "    print(\"Exercise 9: Correct!\")"
   ],
   "id": "7f86862d7a235104",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.709970Z",
     "start_time": "2025-08-25T16:44:09.699847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_9():\n",
    "    \"\"\"Implement variance thresholding for feature selection.\"\"\"\n",
    "    # Create sample data with low variance feature\n",
    "    feature_matrix = np.array([\n",
    "        [1, 2, 0.1],\n",
    "        [2, 3, 0.1],\n",
    "        [3, 4, 0.1],\n",
    "        [4, 5, 0.1],\n",
    "        [5, 6, 0.1]\n",
    "    ])\n",
    "\n",
    "    # Select features with variance > threshold (e.g., 0.5) - Your implementation here\n",
    "    variance_threshold = 0.5\n",
    "\n",
    "    high_variance_features = np.array([])\n",
    "    selected_feature_matrix = np.array([])\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_9(selected_feature_matrix, high_variance_features)\n",
    "\n",
    "ex_9()"
   ],
   "id": "3bcd039244fe6703",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Correlation-Based Feature Selection\n",
    "Identify and remove highly correlated features to reduce redundancy in your dataset. Calculate pairwise correlations and eliminate features that are highly correlated with others, keeping only the most informative ones."
   ],
   "id": "8b6c6811bde971d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.765055Z",
     "start_time": "2025-08-25T16:44:09.757391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_10(selected_feature_matrix, features_to_keep):\n",
    "    \"\"\"Check correlation-based feature selection.\"\"\"\n",
    "    # Check that some correlated features are removed\n",
    "    if len(features_to_keep) < 2:\n",
    "        raise ValueError(\"Exercise 10: Should keep at least 2 features\")\n",
    "\n",
    "    # Check shape of selected data\n",
    "    if selected_feature_matrix.shape[1] != len(features_to_keep):\n",
    "        raise ValueError(\"Exercise 10: Shape mismatch between selected data and features to keep\")\n",
    "\n",
    "    print(\"Exercise 10: Correct!\")"
   ],
   "id": "15e587fe0280d6dc",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.826865Z",
     "start_time": "2025-08-25T16:44:09.819281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_10():\n",
    "    \"\"\"Implement correlation-based feature selection.\"\"\"\n",
    "    # Create sample data with two feature pairs that are highly correlated\n",
    "    feature_matrix = np.array([\n",
    "        [1, 2.1, 0.5, 4],\n",
    "        [2, 4.2, 1.2, 8],\n",
    "        [34, 116.1, 12.1, 912],\n",
    "        [10, 8.3, 99.2, 45],\n",
    "        [20.5, 16.2, 200.1, 90]\n",
    "    ])\n",
    "\n",
    "    # Select features to keep - Your implementation here\n",
    "    features_to_keep = np.array([])\n",
    "    selected_feature_matrix = np.array([])\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_10(selected_feature_matrix, features_to_keep)\n",
    "\n",
    "ex_10()"
   ],
   "id": "2e9d888e41ce5b4a",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Space Transformation\n",
    "Transform a 2-feature matrix $[F1, F2]$ into a new feature space $[F1², F1*F2, F2², log(F2)]$. This polynomial and logarithmic transformation can help capture non-linear relationships in the data."
   ],
   "id": "da307999cce6fb22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.883881Z",
     "start_time": "2025-08-25T16:44:09.875502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_ex_11(transformed_feature_matrix):\n",
    "    \"\"\"Check feature space transformation.\"\"\"\n",
    "    # Check shape (should be 4 rows, 4 columns)\n",
    "    if transformed_feature_matrix.shape != (4, 4):\n",
    "        raise ValueError(\"Exercise 11: Incorrect shape of transformed matrix\")\n",
    "\n",
    "    # Check specific transformations for first row [1, 2]\n",
    "    expected_first_row = [1, 2, 4, np.log(2)]  # F1^2, F1F2, F2^2, log(F2)\n",
    "\n",
    "    for i, val in enumerate(expected_first_row):\n",
    "        if abs(transformed_feature_matrix[0, i] - val) > 1e-6:\n",
    "            raise ValueError(f\"Exercise 11: Incorrect transformation at position [0, {i}]\")\n",
    "\n",
    "    print(\"Exercise 11: Correct!\")"
   ],
   "id": "cc4b416b46cb87f9",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.939924Z",
     "start_time": "2025-08-25T16:44:09.934347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_11():\n",
    "    \"\"\"Transform feature space using polynomial and logarithmic transformations.\"\"\"\n",
    "    # Create sample data with 2 features\n",
    "    original_feature_matrix = np.array([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 4],\n",
    "        [4, 5]\n",
    "    ])\n",
    "\n",
    "    # Create transformed matrix - Your implementation here\n",
    "    transformed_feature_matrix = np.array([])\n",
    "\n",
    "    # Uncomment check\n",
    "    # check_ex_11(transformed_feature_matrix)\n",
    "\n",
    "ex_11()"
   ],
   "id": "8d0c0bf0991acb07",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Answers",
   "id": "4f90a568a6218a5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:09.996458Z",
     "start_time": "2025-08-25T16:44:09.989312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_1():\n",
    "    \"\"\"For each of the data types below, classify them as\n",
    "    nominal, ordinal, discrete or continuous.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        ('nominal', ['red', 'blue', 'green', 'red', 'blue']),\n",
    "        ('ordinal', ['low', 'medium', 'high', 'low', 'medium']),\n",
    "        ('ordinal', ['fast', 'slow', 'slowest']),\n",
    "        ('nominal', ['happy', 'disgusted', 'angry', 'sad', 'happy']),\n",
    "        ('discrete', [1, 2, 3, 1, 2]),\n",
    "        ('continuous', [1.5, 2.7, 3.1, 1.8, 2.9]),\n",
    "    ]\n",
    "    check_ex_1(data)"
   ],
   "id": "5f2fcc72cdb47be9",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.055970Z",
     "start_time": "2025-08-25T16:44:10.047854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_2():\n",
    "    \"\"\"Encode the following values using ordinal encoding (and pandas, for practice).\"\"\"\n",
    "    data = ['high school', 'bachelor', 'master', 'phd', 'bachelor']\n",
    "\n",
    "    # Encode\n",
    "    pd_data = pd.DataFrame({\"education\": data})\n",
    "    unique_values = pd_data['education'].unique()\n",
    "    mapping = {val: idx for idx, val in enumerate(sorted(unique_values))}\n",
    "    encoded_data = pd_data['education'].map(mapping)\n",
    "\n",
    "    # Check the mapping, which should be a list of length 'data' where every entry is the correct encoding\n",
    "    check_ex_2(encoded_data.to_list())"
   ],
   "id": "73ca74430be25fe",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.111695Z",
     "start_time": "2025-08-25T16:44:10.105175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_3():\n",
    "    \"\"\"Implement one-hot encoding for the given data.\"\"\"\n",
    "    data = ['cat', 'dog', 'bird', 'cat', 'fish']\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'animal': data})\n",
    "\n",
    "    # Get unique values in alphabetical order\n",
    "    unique_animals = df['animal'].unique()\n",
    "\n",
    "    # Create one-hot encoding\n",
    "    one_hot_data = []\n",
    "    for animal in data:\n",
    "        encoding = [1 if animal == unique_animal else 0 for unique_animal in unique_animals]\n",
    "        one_hot_data.append(encoding)\n",
    "\n",
    "    check_ex_3(one_hot_data)"
   ],
   "id": "241078105623a420",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.167317Z",
     "start_time": "2025-08-25T16:44:10.164211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_4():\n",
    "    \"\"\"Calculate basic statistics for the given data.\"\"\"\n",
    "    discrete_data = [1, 2, 2, 3, 4, 4, 4, 5]\n",
    "    continuous_data = [1.1, 2.3, 2.3, 3.7, 4.2, 4.2, 4.8, 5.1]\n",
    "\n",
    "    # Calculate statistics for discrete data\n",
    "    discrete_stats = {\n",
    "        'mode': max(set(discrete_data), key=discrete_data.count),\n",
    "        'median': np.median(discrete_data),\n",
    "        'mean': np.mean(discrete_data),\n",
    "        'variance': np.var(discrete_data)\n",
    "    }\n",
    "\n",
    "    # Calculate statistics for continuous data\n",
    "    continuous_stats = {\n",
    "        'mode': max(set(continuous_data), key=continuous_data.count),\n",
    "        'median': np.median(continuous_data),\n",
    "        'mean': np.mean(continuous_data),\n",
    "        'variance': np.var(continuous_data)\n",
    "    }\n",
    "\n",
    "    check_ex_4(discrete_stats, continuous_stats, discrete_data, continuous_data)"
   ],
   "id": "4ab7dee934e76d89",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.221705Z",
     "start_time": "2025-08-25T16:44:10.216329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_5():\n",
    "    \"\"\"Implement 80/20 train-test split manually.\"\"\"\n",
    "    # Create sample data\n",
    "    feature_matrix = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "    target_labels = np.array([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "    # Create indices and shuffle\n",
    "    sample_indices = np.arange(len(feature_matrix))\n",
    "\n",
    "    # Split indices (80% train, 20% test)\n",
    "    train_split_index = int(0.8 * len(feature_matrix))\n",
    "    training_indices = sample_indices[:train_split_index]\n",
    "    testing_indices = sample_indices[train_split_index:]\n",
    "\n",
    "    # Split data\n",
    "    training_features = feature_matrix[training_indices]\n",
    "    testing_features = feature_matrix[testing_indices]\n",
    "    training_labels = target_labels[training_indices]\n",
    "    testing_labels = target_labels[testing_indices]\n",
    "\n",
    "    check_ex_5(training_features, testing_features, training_labels, testing_labels)"
   ],
   "id": "b3338a085c49b70f",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.273894Z",
     "start_time": "2025-08-25T16:44:10.269267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_6():\n",
    "    \"\"\"Implement min-max scaling.\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Implement min-max scaling\n",
    "    feature_minimums = feature_data.min(axis=0)\n",
    "    feature_maximums = feature_data.max(axis=0)\n",
    "    minmax_scaled_features = (feature_data - feature_minimums) / (feature_maximums - feature_minimums)\n",
    "\n",
    "    check_ex_6(minmax_scaled_features)"
   ],
   "id": "dd20809877245a3a",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.327464Z",
     "start_time": "2025-08-25T16:44:10.322349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_7():\n",
    "    \"\"\"Implement standardization (z-score normalization).\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Implement standardization\n",
    "    feature_means = feature_data.mean(axis=0)\n",
    "    feature_standard_deviations = feature_data.std(axis=0)\n",
    "    standardized_features = (feature_data - feature_means) / feature_standard_deviations\n",
    "\n",
    "    check_ex_7(standardized_features)"
   ],
   "id": "45901e2913701b16",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.379075Z",
     "start_time": "2025-08-25T16:44:10.376273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_8():\n",
    "    \"\"\"Implement robust scaling using median and IQR.\"\"\"\n",
    "    # Load iris data\n",
    "    iris = load_iris()\n",
    "    feature_data = iris.data\n",
    "\n",
    "    # Implement robust scaling\n",
    "    feature_medians = np.median(feature_data, axis=0)\n",
    "    first_quartiles = np.percentile(feature_data, 25, axis=0)\n",
    "    third_quartiles = np.percentile(feature_data, 75, axis=0)\n",
    "    interquartile_ranges = third_quartiles - first_quartiles\n",
    "    robust_scaled_features = (feature_data - feature_medians) / interquartile_ranges\n",
    "\n",
    "    check_ex_8(robust_scaled_features)"
   ],
   "id": "8236e0756d95b006",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.432696Z",
     "start_time": "2025-08-25T16:44:10.427202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_9():\n",
    "    \"\"\"Implement variance thresholding for feature selection.\"\"\"\n",
    "    # Create sample data with low variance feature\n",
    "    feature_matrix = np.array([\n",
    "        [1, 2, 0.1],\n",
    "        [2, 3, 0.1],\n",
    "        [3, 4, 0.1],\n",
    "        [4, 5, 0.1],\n",
    "        [5, 6, 0.1]\n",
    "    ])\n",
    "\n",
    "    # Calculate variance for each feature\n",
    "    feature_variances = np.var(feature_matrix, axis=0)\n",
    "\n",
    "    # Select features with variance > threshold (e.g., 0.5)\n",
    "    variance_threshold = 0.5\n",
    "    high_variance_features = feature_variances > variance_threshold\n",
    "    selected_feature_matrix = feature_matrix[:, high_variance_features]\n",
    "\n",
    "    check_ex_9(selected_feature_matrix, high_variance_features)"
   ],
   "id": "bbf029f7dd4ac5dc",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.490864Z",
     "start_time": "2025-08-25T16:44:10.482577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_10():\n",
    "    \"\"\"Implement correlation-based feature selection.\"\"\"\n",
    "    # Create sample data with two feature pairs that are highly correlated\n",
    "    feature_matrix = np.array([\n",
    "        [1, 2.1, 0.5, 4],\n",
    "        [2, 4.2, 1.2, 8],\n",
    "        [34, 116.1, 12.1, 912],\n",
    "        [10, 8.3, 99.2, 45],\n",
    "        [20.5, 16.2, 200.1, 90]\n",
    "    ])\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = np.corrcoef(feature_matrix.T)\n",
    "\n",
    "    # Find highly correlated features (correlation > 0.9)\n",
    "    correlation_threshold = 0.9\n",
    "    num_features = feature_matrix.shape[1]\n",
    "    features_to_remove = set()\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(i+1, num_features):\n",
    "            if abs(correlation_matrix[i, j]) > correlation_threshold:\n",
    "                features_to_remove.add(j)  # Drop the second feature\n",
    "\n",
    "    # Select features to keep\n",
    "    features_to_keep = [i for i in range(num_features) if i not in features_to_remove]\n",
    "    selected_feature_matrix = feature_matrix[:, features_to_keep]\n",
    "\n",
    "    check_ex_10(selected_feature_matrix, features_to_keep)"
   ],
   "id": "b2ee07c6551408b3",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.545207Z",
     "start_time": "2025-08-25T16:44:10.539398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex_11():\n",
    "    \"\"\"Transform feature space using polynomial and logarithmic transformations.\"\"\"\n",
    "    # Create sample data with 2 features\n",
    "    original_feature_matrix = np.array([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 4],\n",
    "        [4, 5]\n",
    "    ])\n",
    "\n",
    "    feature_one = original_feature_matrix[:, 0]\n",
    "    feature_two = original_feature_matrix[:, 1]\n",
    "\n",
    "    # Apply transformations\n",
    "    feature_one_squared = feature_one ** 2\n",
    "    feature_one_two_product = feature_one * feature_two\n",
    "    feature_two_squared = feature_two ** 2\n",
    "    log_feature_two = np.log(feature_two)\n",
    "\n",
    "    # Create transformed matrix\n",
    "    transformed_feature_matrix = np.column_stack([feature_one_squared, feature_one_two_product, feature_two_squared, log_feature_two])\n",
    "\n",
    "    check_ex_11(transformed_feature_matrix)"
   ],
   "id": "6d4fb7b36bfbba94",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:44:10.613708Z",
     "start_time": "2025-08-25T16:44:10.596702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_all():\n",
    "    ex_1()\n",
    "    ex_2()\n",
    "    ex_3()\n",
    "    ex_4()\n",
    "    ex_5()\n",
    "    ex_6()\n",
    "    ex_7()\n",
    "    ex_8()\n",
    "    ex_9()\n",
    "    ex_10()\n",
    "    ex_11()\n",
    "\n",
    "run_all()"
   ],
   "id": "5664918d9c9fc266",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1: Correct!\n",
      "Exercise 2: Correct!\n",
      "Exercise 3: Correct!\n",
      "Exercise 4: Correct!\n",
      "Exercise 5: Correct!\n",
      "Exercise 6: Correct!\n",
      "Exercise 7: Correct!\n",
      "Exercise 8: Correct!\n",
      "Exercise 9: Correct!\n",
      "Exercise 10: Correct!\n",
      "Exercise 11: Correct!\n"
     ]
    }
   ],
   "execution_count": 89
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
