{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d457dce5",
   "metadata": {},
   "source": [
    "# Optimization Problems\n",
    "\n",
    "````{figure} /images/optimization/mountain.jpg\n",
    "---\n",
    "height: 200px\n",
    "name: mountain\n",
    "align: left\n",
    "---\n",
    "````     \n",
    "Imagine yourself standing in the middle of a vast, rugged mountain landscape. Your goal is to reach the lowest point in this terrainâ€”a hidden valley where the air is still. Easy, you might think. I'll just look at the map and see where the lowest point is. But what if the map is infinitely big? And what if the mountain landscape is in a high-dimensional hyperspace, where you can't see anything but three dimensional projections of this hyperspace? The optimization challenges in machine learning are a bit like that: the task to find the lowest valley an infinitely large space extending within hundreds of dimensions. What we need to solve this task is to get some hints where to look for the valley, and this is what optimization theory can deliver.\n",
    "\n",
    "We start with looking at vanilla optimization problems that consist only of an objective function that is to be minimized, having no additional constraints on the solution.\n",
    "## Unconstrained Optimization Problems\n",
    "We define an optimization problem, also called *the objective* as follows. \n",
    "````{prf:definition} unconstrained optimization objective\n",
    ":label: objective\n",
    "Given an **objective function** $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$, the **objective of an unconstrained optimization problem** is:\n",
    "    $$\\begin{align*}\n",
    "        \\min_{x\\in\\mathbb{R}^n} f(\\vvec{x})\n",
    "    \\end{align*}$$\n",
    "We say that:\n",
    "-  $\\displaystyle \\vvec{x}^*\\in\\argmin_{x\\in\\mathbb{R}^n}f(\\vvec{x})$ is a **minimizer**\n",
    "- $\\displaystyle \\min_{\\vvec{x}\\in\\mathbb{R}^n}f(\\vvec{x})$ is the **minimum** \n",
    "````  \n",
    "   \n",
    "A minimizer can be *local* or *global*. Following our analogy, the global minimizer is the lowest valley and a local minimizer can be any valley. \n",
    "````{prf:definition} minimizers\n",
    ":label: minimizers\n",
    "Given an unconstrained objective as defined above. A **global minimizer** is a vector $\\vvec{x}^*$ satisfying \n",
    "$$\n",
    "  f(\\vvec{x}^*)\\leq f(\\vvec{x}) \\text{ for all } \\vvec{x}\\in\\mathbb{R}^n\n",
    "$$\n",
    "A **local minimizer** is a vector $\\vvec{x}_0$ satisfying\n",
    "$$f(\\vvec{x}_0)\\leq f(\\vvec{x}) \\text{ for all } \\vvec{x}\\in\\mathcal{N}_\\epsilon(\\vvec{x}_0),$$\n",
    "where $\\mathcal{N}_\\epsilon(\\vvec{x}_0) = \\{\\vvec{x}\\in\\mathbb{R}^n\\vert \\lVert x-x_0\\rVert\\leq \\epsilon\\}$\n",
    "```` \n",
    "In an unconstrained optimization problem, the minimizers are the points where the function does not decrease in any direction. These points are a subset of the *stationary points*, which can be identified by solving a mathematical equation.\n",
    "\n",
    "### FONC & SONC\n",
    "You probably know from your highschool math classes that every local minimizer $x_0$ of a function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a stationary point: $\\frac{d}{dx}f(x_0)=0$. This is known as the first order neccessary condition. This property is easily understood, considering that the derivative indicates the slope of a function at a specified point. If we have a real-valued minimizer, then the slope is zero at that minimizer. If the slope would be positive, then we can go to the left to decrease the function value further, and if the slope is negative, we can go to the right. By means of this property, we can identify all the candidates that could be minimizers. Maximizers and saddle points are stationary points too. With the second order neccessary condition, we can filter further the minimizers from the pool of candidates. The second order neccessary condition states that at a minimizer the second derivative is nonnegative $\\frac{d^2}{dx^2}f(x_0)\\geq 0$. The second derivative is the slope of the slope. If we have a minimizer, then the slope increases: first we go down, and then we go up. Hence, we need that the slope of the slope does at least not decrease. \n",
    "\n",
    "````{prf:example}\n",
    "Let's have a look at a seemingly simple example. The function $f(x) = \\frac14x^4 + \\frac13x^3 -x^2$ is plotted below and we see that there are two minimizers $x_1=-2$ and $x_2=1$. The question is just if those are all minimizers, or if there is another one beyond the scope of what is plotted here.\n",
    "```{tikz}\n",
    "\\begin{axis}[width=.8\\textwidth,xlabel=$x$,ylabel=$y$,axis lines = center, \n",
    "domain=-3:2,yticklabels={,,},xticklabels={,,}]\n",
    "\\addplot[blue,thick]\n",
    "{x^4/2 + 2*x^3/3 - 2*x^2};\n",
    "\\end{axis}\n",
    "```\n",
    "To find all the minimizers of the function, we apply the first and second order neccessary condition. We compute the first and second derivative.\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dx} f(x) &= x^3 + x^2 -2x \\\\\n",
    "    \\frac{d^2}{dx^2}f(x) & = 3x^2 + 2x -2\n",
    "\\end{align*}\n",
    "Now we solve the equation setting the first derivative to zero and get three stationary points:\n",
    "$$\\frac{d}{dx} f(x) =0 \\quad \\Leftrightarrow \\quad x_1=-2, x_2 = 0, x_3=1$$\n",
    "Given the plot, we already know which of these are minimizers, but to conclude our example, we apply the second order sufficient condition to identify the local minimizers $x_1=-2$ and $x_2=3$.\n",
    "\n",
    "$$\\frac{d^2}{dx^2}f(-2)=6\\geq 0,\\quad \\frac{d^2}{dx^2}f(0)=-2< 0,\\quad \\frac{d^2}{dx^2}f(1)=3\\geq 0 $$\n",
    "````\n",
    "\n",
    "### FONC & SONC in higher dimensions\n",
    "The principles of the first and second order conditions can be generalized to functions $f:\\mathbb{R}^d\\rightarrow \\mathbb{R}$ mapping from a $d$-dimensional vector space to real values. The main difficulty is that we have now more to consider than just left and right when looking for a direction into which we could minimize the function further. In fact, any vector $\\vvec{v}\\in\\mathbb{R}^d$ could indicate a possible direction in which the function might decrease. Luckily, we can show that we just have to check one direction, given by the negative *gradient*, which points into the direction of steepest descent. The gradient indicates the slope of a function in the directions of the coordinates, which are called the *partial derivatives*. A partial derivative $\\frac{\\partial f(\\vvec{x})}{\\partial x_i}$ is computed like a one-dimensional derivative by treating all variables except for $x_i$ as  a constant. The gradient gathers those partial derivatives in a vector. The transposed of the gradient is called the *Jacobian*.\n",
    "    \n",
    "\\begin{align*}\n",
    "    \\frac{\\partial f(\\vvec{x})}{\\partial \\vvec{x}} &=\n",
    "    \\begin{pmatrix}\n",
    "    \\frac{\\partial f(\\vvec{x})}{\\partial x_1} & \\ldots & \\frac{\\partial f(\\vvec{x})}{\\partial x_d}\n",
    "    \\end{pmatrix}\\in\\mathbb{R}^{1\\times d} &\\text{(Jacobian)}\\\\\n",
    "      \\nabla_\\vvec{x} f(\\vvec{x}) &=\n",
    "    \\begin{pmatrix}\n",
    "    \\frac{\\partial f(\\vvec{x})}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f(\\vvec{x})}{\\partial x_d}\n",
    "    \\end{pmatrix}\\in\\mathbb{R}^{d} &\\text{(Gradient)}\n",
    "\\end{align*}\n",
    "\n",
    "With the gradient, we get a first order neccessary condition (FONC) for functions mapping from a vector space $\\mathbb{R}^d$. \n",
    "````{prf:theorem} FONC   \n",
    "If $\\vvec{x}$ is a local  minimizer of $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ and $f$ is continuously \n",
    "differentiable in an open neighborhood of $\\vvec{x}$, then\n",
    "    $$\\nabla f(\\vvec{x})=0$$\n",
    "````    \n",
    "Likewise, a vector $\\vvec{x}$ is called *stationary point* if $\\nabla f(\\vvec{x})=0$. The second order neccessary condition (SONC) uses the generation of the second order derivative to vector spaces, called the *Hessian*. We state this condition here for reasons of completeness, but we will not need this property for the machine learning models that we discuss in this course.\n",
    "\n",
    "````{prf:theorem} SONC\n",
    "If $\\vvec{x}$ is a local  minimizer of $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ and $\\nabla^2f$ is continuous in an open \n",
    "neighborhood of $\\vvec{x}$, then\n",
    "$$\\nabla f(\\vvec{x})=0 \\text{ and } \\nabla^2f(\\vvec{x}) \\text{ is positive semidefinite}$$\n",
    "````   \n",
    "\n",
    "A matrix $A\\in\\mathbb{R}^{d\\times d}$ is **positive semidefinite** if\n",
    "$$\\vvec{x}^\\top A \\vvec{x}\\geq 0 \\text{ for all } \\vvec{x}\\in\\mathbb{R}^d$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d9e02",
   "metadata": {},
   "source": [
    "````{prf:example}\n",
    ":label: expl_fonc\n",
    "```{figure} /images/optimization/rosenbrock.png\n",
    "---\n",
    "height: 200px\n",
    "name: rosenbrock\n",
    "align: left\n",
    "---\n",
    "The Rosenbrock function\n",
    "```\n",
    "In this example we apply FONC and SONC to find the minimizers of the Rosenbrock function, which is given by\n",
    "\\begin{align*}\n",
    "    f(\\vvec{x})&= 100(x_2-x_1^2)^2 +(1-x_1)^2.\n",
    "\\end{align*}\n",
    "In order to apply FONC, we need to compute the gradient. We do so by computing the partial derivatives. The partial derivatives are computed by the same rules as you know it from computing the derivative of a one-dimensional function.\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial x_1}f(\\vvec{x})&= 400x_1(x_1^2-x_2) +2(x_1-1)\\\\\n",
    "    \\frac{\\partial}{\\partial x_2}f(\\vvec{x})&= 200(x_2-x_1^2)\n",
    "\\end{align*}\n",
    "FONC says that every minimizer has to be a stationary point. Stationary points are the vectors at which the gradient of $f$ is zero. We compute the set of stationary points by setting the gradient to zero and solving for $\\vvec{x}$.\n",
    "\\begin{align*}\n",
    "      \\frac{\\partial}{\\partial x_2}f(\\vvec{x})&=200(x_2-x_1^2)=0\n",
    "      &\\Leftrightarrow x_2 =x_1^2\\\\\n",
    "      \\frac{\\partial}{\\partial x_1}f\\begin{pmatrix}x_1\\\\x_1^2\\end{pmatrix}&= 2(x_1-1) =0 \n",
    "      &\\Leftrightarrow x_1=1\n",
    "\\end{align*}\n",
    "\n",
    "According to FONC we have a stationary point at $\\vvec{x}=(1,1)$. Now we check with SONC if the stationary point could be a minimizer (it could also be a maximizer or a saddle point). SONC says that every minimizer has a positive definite Hessian. Hence, we require the Hessian, the second derivative of the Rosenbrock function. To that end, we compute the partial derivatives of the partial derivatives: \n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2}{\\partial^2 x_1}f(\\vvec{x})&= \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_1}f(\\vvec{x})\\right)= 1200x_1^2-400x_2 +2\\\\\n",
    "\\frac{\\partial^2}{\\partial^2 x_2}f(\\vvec{x})&=  \\frac{\\partial}{\\partial x_2} \\left(\\frac{\\partial}{\\partial x_2}f(\\vvec{x})\\right)= 200\\\\\n",
    "\\frac{\\partial^2}{\\partial x_1\\partial x_2}f(\\vvec{x})&=\\frac{\\partial^2}{\\partial x_2\\partial x_1}f(\\vvec{x})= -400x_1\n",
    "\\end{align*}\n",
    "The Hessian is given by\n",
    "\\begin{align*}\n",
    "    \\nabla^2 f(\\vvec{x})&=  \\begin{pmatrix}\\frac{\\partial^2}{\\partial^2 x_1} f(\\vvec{x}) & \\frac{\\partial^2}{\\partial x_1x_2} f(\\vvec{x})\\\\ \\frac{\\partial^2}{\\partial x_2x_1}f(\\vvec{x}) & \\frac{\\partial^2}{\\partial^2 x_2} f(\\vvec{x})\\end{pmatrix}\\\\\n",
    "    &=200\\begin{pmatrix} 6x_1^2-2x_2 + 0.01& -2x_1\\\\ -2x_1 &1 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "We insert our stationary point $\\vvec{x}_0=(1,1)$ into the Hessian and get\n",
    "$$\\nabla^2f(\\vvec{x}_0)= 200\\begin{pmatrix} 4.01& -2\\\\ -2 & 1\\end{pmatrix}$$\n",
    "Now we check if the Hessian at the stationary point is positive definite. Let $\\vvec{x}\\in\\mathbb{R}^2$, then \n",
    "\\begin{align*}\n",
    "    \\vvec{x}^\\top \\nabla^2f(\\vvec{x}_0) \\vvec{x} &= 200 \\begin{pmatrix}x_1 & x_2\\end{pmatrix} \\begin{pmatrix}\n",
    "     4.01& -2\\\\ -2 & 1\n",
    "    \\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\\\\\n",
    "    &= 200\\begin{pmatrix}x_1 & x_2\\end{pmatrix} \\begin{pmatrix}\n",
    "    4.01x_1-2x_2\\\\ -2x_1+ x_2\n",
    "    \\end{pmatrix}\\\\\n",
    "    &=200(4.01x_1^2 -2x_1x_2 -2x_1x_2 +x_2^2)\\\\\n",
    "    &= 200(4.01x_1^2 -4x_1x_2 + x_2^2)\\\\\n",
    "    &= 200((2x_1-x_2)^2 +0.01x_1^2) \\geq 0\n",
    "\\end{align*}\n",
    "The last inequality follows because the sum of quadratic terms can not be negative.\n",
    "We conclude that the Hessian at our stationary point is positive semi-definite. As a result, FONC and SONC yield that $\\vvec{x}=(1,1)$ is the only possible local minimizer of $f$.\n",
    "````\n",
    "Nice, we have now a strategy yo find local minimizers if we have an unconstrained objective with an objective function which is continuously differentiable. Let's consider a more complex setting, introducing constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb980485",
   "metadata": {},
   "source": [
    "## Constrained Optimization Problems\n",
    "````{prf:definition} Constrained Objective\n",
    ":label: constr_objective\n",
    "Given an objective function $f:\\mathbb{R}^d\\rightarrow \\mathbb{R}$ and constraint functions $c_i,g_k:\\mathbb{R}^d\\rightarrow \\mathbb{R}$, then  the **objective** of an constrained optimization problem is\n",
    "\\begin{align*}\n",
    "    \\min_{x\\in\\mathbb{R}^n}&\\ f(\\vvec{x}) \\\\\n",
    "    \\text{s.t. }& c_i(\\vvec{x}) =0  &\\text{ for } 1\\leq i \\leq m,\\\\\n",
    "                & g_k(\\vvec{x})\\geq 0 &\\text{ for }1\\leq k\\leq l\n",
    "\\end{align*}\n",
    "We call the set of vectors satisfying the constraints the **feasible set**:\n",
    "$$\\mathcal{C}=\\{\\vvec{x}\\mid c_i(\\vvec{x})=0, g_k(\\vvec{x})\\geq 0 \\text{ for }1\\leq i\\leq m, 1\\leq k\\leq l\\}.$$\n",
    "````\n",
    "Adding constraints to an optimization problem often makes it more challenging. Now, there are two types of minimizers to consider:\n",
    "\n",
    "1. Interior Minimizers: Points that lie in a \"valley\" of the objective function, where the function value only increases or remains the same when moving in any direction.\n",
    "2. Boundary Minimizers: Points on the edge of the feasible set, where the function value increases or stays the same when moving *inward* along the feasible region (the function value might decrease when moving out of the feasible set).\n",
    "\n",
    "Boundary minimizers can be difficult to identify because traditional methods like the First-Order Necessary Condition (FONC) do not directly apply. However, by using the Lagrangian approach, we can transform a constrained objective into an (almost) unconstrained one, making it possible to find solutions at the boundary.\n",
    "````{prf:definition} Lagrangian\n",
    ":label: lagrangian\n",
    "Given a constrained optimization objective as in {prf:ref}`constr_objective`, then  the the **Lagrangian function** is defined as\n",
    "$$\\mathcal{L}(\\vvec{x},\\bm\\lambda,\\bm\\mu) = f(\\vvec{x}) - \\sum_{i=1}^m\\bm\\lambda_i c_i(\\vvec{x}) - \\sum_{k=1}^l\\bm\\mu_k g_k(\\vvec{x}).$$\n",
    "The parameters $\\lambda_i\\in\\mathbb{R}$ and $\\mu_k\\geq 0$ are called _Lagrange multipliers_.\n",
    "````     \n",
    "The Lagrangian introduces an adversarial approach to handling constraints. We remove the constraints on $\\vvec{x}$, allowing us to minimize over all $\\vvec{x}\\in\\mathbb{R}^d$, while simultaneously maximizing over the Lagrange multipliers $\\bm\\lambda\\in\\mathbb{R}^m$ and $\\bm\\mu\\geq\\vvec{0}$.      \n",
    "\n",
    "Imagine a scenario where you are trying to minimize the Lagrangian by adjusting $\\vvec{x}$, while an \"adversary\" seeks to maximize it by adjusting $\\bm\\lambda$ and $\\bm\\mu$. If $\\vvec{x}$ lies outside the feasible set, the adversary can exploit this by driving up the Lagrangian value. For instance, if a constraint $c_i(\\vvec{x})=0.1\\neq 0$ is unmet, the adversary could set $\\lambda_i=-10000$, raising the Lagrangian by $-(-10000)\\cdot 0.1=1000$.     \n",
    "\n",
    "To prevent these large increases, we need to ensure $\\vvec{x}$ is within the feasible set, effectively limiting the adversary's ability to inflate the Lagrangian.     \n",
    "\n",
    "The Lagrangian can be used to transform the *primal problem*, which is the original unconstrained objective from {prf:ref}`constr_objective`, to the dual problem.\n",
    "````{prf:definition} Dual Problem\n",
    ":label: dual_problem\n",
    "Given a primal optimization objective as defined in {prf:ref}`constr_objective`, we define the **dual objective function** $\\mathcal{L}_{dual}$, returning the minimum (infimum to be precise) of the Lagrangian, given any $\\bm\\lambda,\\bm\\mu\\geq\\vvec{0}$:\n",
    "$$\\inf_{\\vvec{x}\\in\\mathbb{R}^d}\\mathcal{L}(\\vvec{x},\\bm\\lambda,\\bm\\mu) = \\mathcal{L}_{dual}(\\bm\\lambda,\\bm\\mu).$$ the **dual optimization objective** is defined as\n",
    "\\begin{align*}\n",
    "\\max_{\\bm\\lambda, \\bm\\mu }&\\ \\mathcal{L}_{dual}(\\bm\\lambda,\\bm\\mu) \\\\\n",
    "\\text{s.t. }& \\bm\\lambda\\in\\mathbb{R}^m, \\bm\\mu\\in\\mathbb{R}_+^l\n",
    "\\end{align*}\n",
    "````\n",
    "```{note}\n",
    "The defintion of the dual objective uses the infimum and not the minimum, because there might be some cases where you don't reach the minimum of the Lagrangian for any specific $\\vvec{x}$, but only in a limit, e.g. $\\vvec{x}\\rightarrow\\infty$. The infimum returns then the minimum in the limit. If you're not familiar with the concept of the infimum, you can think of it as the minimum.\n",
    "```\n",
    "The solution to the dual objective are saddle points: points $(\\vvec{x},\\bm\\lambda,\\bm\\mu)$ that minimize the Lagrangian subject to $\\vvec{x}$ and maximize it subject to $(\\bm\\lambda,\\bm\\mu)$. This allows us to formulate necessary conditions (similar to FONC), called the Karush-Kuhn-Tukker (KKT) conditions that have to be met for a solution to the dual problem.\n",
    "````{prf:theorem} KKT conditions\n",
    "Suppose that the objective function $\\displaystyle f\\colon \\mathbb {R} ^{n}\\rightarrow \\mathbb {R}$ and the constraint functions $\\displaystyle c_{i}\\colon \\mathbb{R} ^{n}\\rightarrow \\mathbb {R} $ and $\\displaystyle g_{k}\\colon \\mathbb {R} ^{n}\\rightarrow \\mathbb{R}$ are continuously differentiable. If $ \\vvec{x}^*$ is a local minimum, and the constraint functions $c_{i},g_k$ are affine functions, then there exist multipliers $\\bm\\lambda^*$ and $\\bm\\mu^*$ such that the following conditions hold: \n",
    "\n",
    "1. **Stationarity**:\n",
    "   $$\n",
    "   \\nabla_{\\vvec{x}} \\mathcal{L}(\\vvec{x}^*,\\bm\\lambda^*,\\bm\\mu^*) = \\nabla f(x^*) + \\sum_{i=1}^m \\lambda_i^* \\nabla c_i(x^*) + \\sum_{k=1}^l \\mu_k^* \\nabla g_k(x^*) = 0\n",
    "   $$\n",
    "\n",
    "2. **Primal feasibility**:\n",
    "   \\begin{align*}\n",
    "   c_i(\\vvec{x}^*) = 0, \\quad \\forall i\n",
    "   g_k(\\vvec{x}^*) \\geq 0, \\quad \\forall k\n",
    "   \\end{align*}\n",
    "\n",
    "3. **Dual feasibility**:\n",
    "   $$\n",
    "   \\mu_k^* \\geq 0, \\quad \\forall k\n",
    "   $$\n",
    "\n",
    "4. **Complementary slackness**:\n",
    "   $$\n",
    "   \\mu_k^* g_k(\\vvec{x}^*) = 0, \\quad \\forall k\n",
    "   $$\n",
    "````\n",
    "The KKT conditions state a set of linear equations, that can be used to obtain candidates for potential minimizers $\\vvec{x}^*$. \n",
    "We can easily show that the Lagrangian forms a lower bound of the objective function. For feasible $\\vvec{x}\\in\\mathcal{C}$ and $\\bm\\lambda\\in\\mathbb{R}^m,\\bm\\mu\\in\\mathbb{R}^l$, $\\bm\\mu\\geq \\vvec{0}$ we have\n",
    "$$\\mathcal{L}(\\vvec{x},\\bm\\lambda,\\bm\\mu) = f(\\vvec{x}) - \\sum_{i=1}^m\\bm\\lambda_i \\underbrace{c_i(\\vvec{x}}_{=0}) - \\sum_{k=1}^l\\underbrace{\\bm\\mu_k}_{\\geq 0} \\underbrace{g_k(\\vvec{x})}_{\\geq 0}\\leq f(\\vvec{x})$$\n",
    "In this course, we will need to solve a dual problem only for a linear optimization problem, where the objective function and the constraint functions are affine functions. In this case, *strong duality* holds, such that every minimizer of the primal objective is a maximizer of the dual objective $f(\\vvec{x}^*)= \\mathcal{L}_{dual}(\\bm\\lambda^*,\\bm\\mu^*)$. In this case, we know that every point $\\vvec{x}^*$ obtained by the KKT conditions is a solution to the primal problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
