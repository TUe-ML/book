
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Spectral Clustering &#8212; Data Mining and Machine Learning Jupyter Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'clustering_spectral';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Exercises" href="clustering_exercises.html" />
    <link rel="prev" title="Kernel k-means" href="clustering_kernel_kmeans.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Data Mining and Machine Learning Jupyter Book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Data Mining and Machine Learning Jupyter Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linalg.html">Linear Algebra</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="linalg_spaces.html">Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_normed_vs.html">Normed Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="optimization.html">Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="optimization_problems.html">Optimization Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_convex.html">Convex Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_analytic.html">Analytic Solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_numerical.html">Numerical Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_gradients.html">Matrix Derivatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="regression_objective.html">Regression Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_functions.html">Regression Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_optimization.html">Minimizing the RSS</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_bias_var.html">The Bias-Variance Tradeoff</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_sparse.html">The Sparse Regression Task</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_ridge.html">Ridge Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_lasso.html">Lasso</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="classification.html">Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="classification_problem.html">Classification Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_knn.html">K-Nearest Neighbor</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_naive_bayes.html">Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_random_forests.html">Random Forests</a></li>

<li class="toctree-l2"><a class="reference internal" href="classification_svms.html">Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_kernel_svm.html">Kernel SVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="neuralnets.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_intro.html">From Linear Models to Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_mlps.html">MLPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_backprop.html">Backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_sgd.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_conv.html">Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_pooling.html">Pooling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="dim_reduction.html">Dimensionality Reduction Techniques</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_mf.html">Low Rank Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_pca.html">Principal Component Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="clustering.html">Clustering</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="clustering_k_means.html">k-Means</a></li>



<li class="toctree-l2"><a class="reference internal" href="clustering_k_means_mf.html">k-Means is MF</a></li>


<li class="toctree-l2"><a class="reference internal" href="clustering_kernel_kmeans.html">Kernel k-means</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Spectral Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tue-ml/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/edit/main/clustering_spectral.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/issues/new?title=Issue%20on%20page%20%2Fclustering_spectral.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/clustering_spectral.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Spectral Clustering</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-graph-construction">Similarity Graph Construction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-neighborhood-graph">Epsilon-Neighborhood Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbor-graph">k-Nearest Neighbor Graph</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formal-objective-definitions">Formal Objective Definitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-similarity-within-clusters">Maximum Similarity Within Clusters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum-cut-between-clusters">Minimum Cut Between Clusters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-graph-laplacian">The Graph Laplacian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relationship-to-kernel-k-means">Relationship to Kernel k-means</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering-algorithm">Spectral Clustering Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pseudocode">Pseudocode</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-the-two-circles-dataset">Application to the Two Circles Dataset</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="spectral-clustering">
<h1>Spectral Clustering<a class="headerlink" href="#spectral-clustering" title="Link to this heading">#</a></h1>
<figure class="align-center" id="social-graph">
<a class="reference internal image-reference" href="_images/facebook.jpg"><img alt="_images/facebook.jpg" src="_images/facebook.jpg" style="height: 300px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">Graph clustering is relevant for example in social relation analysis</span><a class="headerlink" href="#social-graph" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In many real-world scenarios, data is best described not as a list of independent examples, but as a network of relationships. A prominent example arises in social networks, where users are connected based on interactions, friendships, or shared interests. Imagine analyzing user interactions on platforms like Instagram or LinkedIn, where individuals follow each other, like posts, or exchange messages. These interactions can be naturally represented as a graph: nodes correspond to users, and edges indicate some form of connection or similarity.</p>
<p>A common goal in analyzing such graphs is to identify communities — tightly connected groups of users that likely share common interests or roles. This task is known as graph clustering, and it helps uncover the hidden structure of the network. For instance, in a professional network like LinkedIn, clustering might reveal groups of people working in the same industry or region. On Instagram, it could identify clusters of users centered around specific topics, such as memes, politics, or influencers.</p>
<p>Unlike traditional clustering methods that operate on vector data, graph clustering works directly with the connectivity information. One powerful approach is spectral clustering, which uses the graph’s Laplacian matrix to map nodes into a geometric space where standard clustering algorithms like k-means can be applied. In a way, spectral clustering extends the idea of kernel k-means to more general applications of similarity matrices (that are not necessarily a kernel matrix), while making the algorithm more efficient and robust.</p>
<section id="similarity-graph-construction">
<h2>Similarity Graph Construction<a class="headerlink" href="#similarity-graph-construction" title="Link to this heading">#</a></h2>
<p>If we are not given a graph directly, we need to construct a similarity graph that captures how similar or related different data points are. This is done by defining a similarity matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}^{n\times n}\)</span>, where each entry <span class="math notranslate nohighlight">\(W_{ij}\)</span> indicates the similarity between data points <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>There are several common ways to define this similarity.</p>
<section id="epsilon-neighborhood-graph">
<h3>Epsilon-Neighborhood Graph<a class="headerlink" href="#epsilon-neighborhood-graph" title="Link to this heading">#</a></h3>
<p>The <span class="math notranslate nohighlight">\(\epsilon\)</span>-neighborhood graph connects two points if their distance is below the threshold <span class="math notranslate nohighlight">\(\epsilon\)</span>:
<div class="math notranslate nohighlight">
\[\begin{split}W_{ij}= \begin{cases}1&amp; \text{if } \lVert D_{i\cdot}-D_{j\cdot}\rVert&lt;\epsilon\\
0&amp; \text{otherwise}\end{cases}\end{split}\]</div>

This results in an unweighted graph that captures local neighborhoods. It’s simple, but sensitive to the choice of <span class="math notranslate nohighlight">\(\epsilon\)</span>: too small, and the graph may become disconnected; too large, and it may lose local structure. The plot below shows the <span class="math notranslate nohighlight">\(\epsilon\)</span>-neighborhood graph on the two-circles dataset.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/ccf8b519deac7319c3726e326c475cfa42ca6e36b16138b1e41d98c2951ba12e.png" src="_images/ccf8b519deac7319c3726e326c475cfa42ca6e36b16138b1e41d98c2951ba12e.png" />
</div>
</div>
<p>Although the hyperparameter <span class="math notranslate nohighlight">\(\epsilon\)</span> is chosen well in this example, we can observe one of the drawbacks of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-neighborhood graph: it doesn’t deal well with data that hasvarying levels of similarities. The points in the inner circle are in tendency closer to each other than the points on the outer circle. Correspondingly, the inner circle points are connected by many edges, but the outer circle points are not.</p>
</section>
<section id="k-nearest-neighbor-graph">
<h3>k-Nearest Neighbor Graph<a class="headerlink" href="#k-nearest-neighbor-graph" title="Link to this heading">#</a></h3>
<p>Alternatively, we can connect each point to its <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors. Since this may result in an asymmetric matrix, we symmetrize it:
<div class="math notranslate nohighlight">
\[\begin{split}N_{ij}= \begin{cases}1&amp; \text{if } D_{i\cdot} \in KNN(D_{j\cdot})\\
0&amp; \text{otherwise}\end{cases},\quad W=\frac12(N+N^\top)\end{split}\]</div>

This method ensures that each point is connected to at least <span class="math notranslate nohighlight">\(k\)</span> neighbors, and the resulting graph reflects local density. The plot below shows the 6-nearest neighbor graph for the two circles dataset</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/111b2da107b15a3185839c4e8facbe217a311320cf3772d77e4ecbfda31efecb.png" src="_images/111b2da107b15a3185839c4e8facbe217a311320cf3772d77e4ecbfda31efecb.png" />
</div>
</div>
<p>We see that the <span class="math notranslate nohighlight">\(k\)</span>NN graph can deal with the varying cluster densities better than the <span class="math notranslate nohighlight">\(\epsilon\)</span>-neighborhood graph: the inner and the outer circle are strongly connected. The intuitive idea for this behavior is that the really close neighbors are more likely found within a point’s cluster than in another cluster.</p>
</section>
</section>
<section id="formal-objective-definitions">
<h2>Formal Objective Definitions<a class="headerlink" href="#formal-objective-definitions" title="Link to this heading">#</a></h2>
<p>Once we have a similarity matrix WW that encodes the strength of connections between nodes, we can define what it means to find “good” clusters in the graph. There are two common objectives for graph clustering. We will discuss them on the basis of an example graph given by the similarity matrix (weighted adjacency matrix)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
W = \begin{pmatrix}
0 &amp; 6 &amp; 0 &amp; 0 &amp; 5 &amp; 0\\
6 &amp; 0 &amp; 1 &amp; 0 &amp; 7 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 9 &amp; 8 &amp; 2\\
0 &amp; 0 &amp; 9 &amp; 0 &amp; 4 &amp; 3\\
5 &amp; 7 &amp; 8 &amp; 4 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 2 &amp; 3 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}\]</div>
<p>The weighted adjacency matrix indicates a graph that looks as follows:</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-cbbe8e859691edb42482980a4be6e867ab91b285.png" alt="Figure made with TikZ" /></p>
</div><section id="maximum-similarity-within-clusters">
<h3>Maximum Similarity Within Clusters<a class="headerlink" href="#maximum-similarity-within-clusters" title="Link to this heading">#</a></h3>
<p>One approach is to group nodes such that the total similarity within each cluster is maximized. Let <span class="math notranslate nohighlight">\(Y\in\mathbb{R}^{n\times r}\)</span> be a cluster indicator matrix, where each row indicates the cluster membership of a data point (exactly one nonzero entry per row). Then, the similarity within cluster <span class="math notranslate nohighlight">\(s\)</span> is computed by summing up all edge weights for nodes in cluster <span class="math notranslate nohighlight">\(\mathcal{C}_{s}\)</span>, indicated by <span class="math notranslate nohighlight">\(Y_{\cdot s}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{1}{\lvert \mathcal{C}_{s}\rvert}\sum_{i,j\in\mathcal{C}_s} W_{ji}
= \frac{Y_{\cdot s}^\top W Y_{\cdot s}}{\lvert Y_{\cdot s}\rvert}.
\end{align*}\]</div>
<p>Summing over all clusters gives the total within-cluster similarity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Sim(Y;W)&amp;=\tr(Y^\top W Y(Y^\top Y)^{-1})\\
&amp;=\sum_{s=1}^r\frac{Y_{\cdot s}^\top W Y_{\cdot s}}{\lvert Y_{\cdot s}\rvert}
=\sum_{s=1}^r\frac{1}{\lvert \mathcal{C}_{s}\rvert}\sum_{i,j\in\mathcal{C}_s} W_{ji}
\end{align*}\]</div>
<p>The maximum similarity graph clustering problem is then given as follows.</p>
<div class="tip admonition">
<p class="admonition-title">Task (Maximum Similarity Graph Clustering)</p>
<p><strong>Given</strong> a graph indicated by a symmetric, nonnegative similarity matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}_+^{n\times n}\)</span>, and the number of clusters <span class="math notranslate nohighlight">\(r\)</span>.<br />
<strong>Find</strong> clusters indicated by the matrix <span class="math notranslate nohighlight">\(Y\in\mathbb{1}^{n\times r}\)</span> which maximize the similarity of points within a cluster</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
 \max_Y\ Sim(Y;W)&amp;=\tr(Y^\top W Y(Y^\top Y)^{-1}) &amp;\text{s.t. } Y\in\mathbb{1}^{n\times r}
\end{align*}\]</div>
<p><strong>Return</strong> the clustering <span class="math notranslate nohighlight">\(Y\in\mathbb{1}^{n\times r}\)</span></p>
</div>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 23 </span></p>
<section class="example-content" id="proof-content">
<p>We compute the similarity within the cluster  <span class="math notranslate nohighlight">\(Y_{\cdot s}=(1, 1, 0, 0, 1, 0)\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{Y_{\cdot s}^\top WY_{\cdot s}}{\lvert Y_{\cdot s}\rvert}
&amp;= \frac{2(5+6+7)}{3} = 12
\end{align*}\]</div>
<p>The graph below visualizes the points belonging to cluster <span class="math notranslate nohighlight">\(Y_{\cdot s}\)</span> with the red nodes. We see how the similarity is computed by summing up the edges connecting nodes in this cluster: 5,6, and 7. Those edge weights are added twice, because we have an undirected graph, and every edge is counted once from point <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, and once from point <span class="math notranslate nohighlight">\(j\)</span> to <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-272624caa14464cb9bcd576f5e8771e0b3252ddc.png" alt="Figure made with TikZ" /></p>
</div></section>
</div></section>
<section id="minimum-cut-between-clusters">
<h3>Minimum Cut Between Clusters<a class="headerlink" href="#minimum-cut-between-clusters" title="Link to this heading">#</a></h3>
<p>An alternative is to minimize the cut, which measures how strongly clusters are connected to the rest of the graph. For a given cluster <span class="math notranslate nohighlight">\(\mathcal{C}_s\)</span>, the cut for this cluster sums up all the weight edges that would be cut if we cut out cluster <span class="math notranslate nohighlight">\(s\)</span> from the graph:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{1}{\lvert \mathcal{C}_s\rvert}\sum_{i\notin\mathcal{C}_s}\sum_{j\in\mathcal{C}_s}W_{ij} = \frac{(\vvec{1}-Y_{\cdot s})^\top WY_{\cdot s}}{\lvert Y_{\cdot s}\rvert}
\end{align*}\]</div>
<p>The total cut value of a clustering sums up all the weights that would be cut if we cut all clusters out of the graph:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Cut(Y;W)&amp;=\tr((\vvec{1}-Y)^\top W Y(Y^\top Y)^{-1}) \\
&amp;= \sum_{s=1}^r\frac{(\vvec{1}-Y_{\cdot s})^\top WY_{\cdot s}}{\lvert Y_{\cdot s}\rvert} 
= \sum_{s=1}^r\frac{1}{\lvert \mathcal{C}_s\rvert}\sum_{i\notin\mathcal{C}_s}\sum_{j\in\mathcal{C}_s}W_{ij}
\end{align*}\]</div>
<p>The minimum cut graph clustering problem is then:</p>
<div class="tip admonition">
<p class="admonition-title">Task (Minimum Cut Graph Clustering)</p>
<p><strong>Given</strong> a graph indicated by a symmetric, nonnegative similarity matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}_+^{n\times n}\)</span>, and the number of clusters <span class="math notranslate nohighlight">\(r\)</span>.<br />
<strong>Find</strong> clusters indicated by the matrix <span class="math notranslate nohighlight">\(Y\in\mathbb{1}^{n\times r}\)</span> which minimize the cut of all clusters</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
 \min_Y\ Cut(Y;W)&amp;=\tr((\vvec{1}-Y)^\top W Y(Y^\top Y)^{-1}) &amp;\text{s.t. } Y\in\mathbb{1}^{n\times r}
\end{align*}\]</div>
<p><strong>Return</strong> the clustering <span class="math notranslate nohighlight">\(Y\in\mathbb{1}^{n\times r}\)</span></p>
</div>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 24 </span></p>
<section class="example-content" id="proof-content">
<p>We compute the cut of cluster <span class="math notranslate nohighlight">\(Y_{\cdot s}=(1, 1, 0, 0, 1, 0)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{Y_{\cdot s}^\top W (\vvec{1}-Y_{\cdot s})}{\lvert Y_{\cdot s}\rvert}
&amp;= \frac{2(1+8+4)}{3}=8\frac{2}{3}
\end{align*}\]</div>
<p>The graph below visualizes the nodes of the cluster and the edges that would have to be cut when we cut out cluster <span class="math notranslate nohighlight">\(Y_{\cdot s}\)</span>. Note that we count every edge weight twice again, since we cut strictly speaking an edge from point <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span> and also the edge from <span class="math notranslate nohighlight">\(j\)</span> to <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-5cc90d0122ddace51b473c92363780c14db6f748.png" alt="Figure made with TikZ" /></p>
</div></section>
</div></section>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h2>
<p>The maximum similarity and minimum cut formulations give us clear goals for clustering a graph, but solving them directly is difficult due to the discrete nature of the cluster indicator matrix <span class="math notranslate nohighlight">\(Y\)</span>. Spectral clustering uses the same trick as kernel k-means to optimize the trace objective. This seems probably straightforward for the maximum similarity objective, but not so much for the minimum cut objective.</p>
<section id="the-graph-laplacian">
<h3>The Graph Laplacian<a class="headerlink" href="#the-graph-laplacian" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 36 </span> (Degree Matrix and Graph Laplacian)</p>
<section class="definition-content" id="proof-content">
<p>Given a graph indicated by a weighted adjacency matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}_+^{n\times n}\)</span>.<br />
The <strong>degree matrix</strong> is the diagonal matrix having the sum of all connecting edges for each node on the diagonal: <div class="math notranslate nohighlight">
\[I_W = \mathrm{diag}\left(\sum_{i=1}^n W_{1i},\ldots,\sum_{i=1}^n W_{ni}\right).\]</div>
<br />
The <strong>unnormalized graph Laplacian</strong>, also called <strong>difference Laplacian</strong> is the matrix <div class="math notranslate nohighlight">
\[L=W-I_W.\]</div>
</p>
</section>
</div><p>In practice, the weighted adjacency matrix is often normalized. The corresponding Graph Laplacian is often denoted by
<div class="math notranslate nohighlight">
\[ L_{sym}= I-I_W^{-1/2}WI_W^{-1/2}.\]</div>

All of the following results can be shown for the symmetrically normalized graph Laplacian as well.</p>
<div class="proof example admonition" id="example-3">
<p class="admonition-title"><span class="caption-number">Example 25 </span></p>
<section class="example-content" id="proof-content">
<p>We compute the degree matrix and the graph Laplacian for our example graph</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-cbbe8e859691edb42482980a4be6e867ab91b285.png" alt="Figure made with TikZ" /></p>
</div><div class="math notranslate nohighlight">
\[\begin{split}I_W = 
\begin{pmatrix}
    11 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 14 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 20 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 16 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 24 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 5
\end{pmatrix}\end{split}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L = \begin{pmatrix}
11 &amp; -6 &amp; 0 &amp; 0 &amp; -5 &amp; 0\\
-6 &amp; 14 &amp; -1 &amp; 0 &amp; -7 &amp; 0\\
0 &amp; -1 &amp; 20 &amp; -9 &amp; -8 &amp; -2\\
0 &amp; 0  &amp; -9 &amp; 16 &amp; -4 &amp; -3\\
-5&amp; -7 &amp; -8 &amp; -4 &amp; 24 &amp; 0\\
0 &amp; 0 &amp; -2 &amp; -3 &amp; 0 &amp; 5
\end{pmatrix}.
\end{align*}\]</div>
</section>
</div><div class="proof lemma admonition" id="lemma-4">
<p class="admonition-title"><span class="caption-number">Lemma 11 </span> (Positive Definiteness of Laplacians)</p>
<section class="lemma-content" id="proof-content">
<p>Given a symmetric similarity matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}^{n\times n}_+\)</span>, the Laplacian <span class="math notranslate nohighlight">\(L=I_W -W\)</span> is positive semi-definite.</p>
</section>
</div><div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. Let <span class="math notranslate nohighlight">\(0\neq v\in\mathbb{R}^n\)</span>, then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
v^\top Lv &amp;= v^\top I_W v - v^\top W v 
= \sum_{i=1}^n v_i^2 \lvert W_{i\cdot}\rvert -\sum_{1\leq i,j\leq n} v_iv_jW_{ij}\\
&amp;= \frac{1}{2} \sum_{1\leq i,j \leq n} (v_i^2 W_{ij} -2 v_iv_jW_{ij} +v_j^2W_{ij}) \\
&amp;= \frac{1}{2} \sum_{1\leq i,j \leq n} W_{ij} (v_i-v_j)^2 \geq 0.
\end{align*}\]</div>
</div>
</div>
<p>The positive definiteness of the Laplacian means in particular that all eigenvalues of the graph Laplacian are positive. The smallest eigenvalue of the graph Laplacian is zero, and it has a specific relationship to the graph.</p>
<div class="proof theorem admonition" id="theorem-5">
<p class="admonition-title"><span class="caption-number">Theorem 42 </span> (Connected Components and Eigenvectors)</p>
<section class="theorem-content" id="proof-content">
<p>Given a graph indicated by the symmetric matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}^{n\times n}_+\)</span>, then the indicator vectors of the connected components are eigenvectors of the Laplacian <span class="math notranslate nohighlight">\(L=I_W-W\)</span> to the smallest eigenvalue <span class="math notranslate nohighlight">\(0\)</span>.</p>
</section>
</div><div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. For every connected component there exists an order of columns and rows such that <span class="math notranslate nohighlight">\(W\)</span> has a block-diagonal form:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3e9afa73-cfb5-4dbe-8cbf-1e9143e31462">
<span class="eqno">(78)<a class="headerlink" href="#equation-3e9afa73-cfb5-4dbe-8cbf-1e9143e31462" title="Permalink to this equation">#</a></span>\[\begin{align} 
Wv = 
\left(
\begin{array}{c:r}
\begin{matrix}
 W_{11}&amp;\ldots&amp;W_{1c}  \\
 \vdots&amp;&amp;\vdots \\
 W_{c1}&amp;\ldots&amp;W_{cc} \\
\end{matrix}
&amp; \mathbf{0}\\
\mathbf{0} &amp; \widehat{W} \\
\end{array}
\right)
\begin{pmatrix}
 1\\
 \vdots\\
 1 \\
 \mathbf{0} 
\end{pmatrix}
=
\begin{pmatrix}
 \lvert W_{1\cdot}\rvert\\
 \vdots\\
 \lvert W_{c\cdot}\rvert \\
 \mathbf{0} 
\end{pmatrix}
=I_Wv.
\end{align}\]</div>
</div>
</div>
<p>The indication of connected components by the first eigenvector(s) of the negative graph Laplacian creates a bridge from the spectrum of the graph Laplacian to the clustering objective. For any binary cluster indicator matrix <span class="math notranslate nohighlight">\(Y_{\cdot s}\)</span> the similarity of points within that cluster are at most the sum of all the degrees of nodes in that cluster: <div class="math notranslate nohighlight">
\[Y_{\cdot s}^\top WY_{\cdot s}\leq Y_{\cdot s}I_WY_{\cdot s} .\]</div>

We have equality <span class="math notranslate nohighlight">\(Y_{\cdot s}^\top WY_{\cdot s}=Y_{\cdot s}^\top I_WY_{\cdot s}\)</span> <strong>if and only if</strong> <span class="math notranslate nohighlight">\(Y_{\cdot s}\)</span> indicates a connected component. This is equivalent to
<div class="math notranslate nohighlight">
\[Y_{\cdot s}^\top \underbrace{(I_W - W)}_{=L}Y_{\cdot s }=0.\]</div>

This suggests that connected components are optimal clusters: they incur zero cut cost and hence minimize the minimum cut objective. However, in practice, we rarely want to recover exact connected components. Real-world graphs are often noisy, or constructed using heuristics and hyperparameters (like radius or number of neighbors). A single misplaced or missing edge can split or merge components. Therefore, connected components are typically not reliable indicators of meaningful clusters.</p>
<p>To overcome this, we assume the graph has only one connected component (or is artificially connected using small edge weights), so that we can go beyond trivial solutions and focus on uncovering meaningful cluster structure.</p>
<p>Spectral clustering is commonly introduced as a relaxation of the minimum cut objective. Instead of recovering the first eigenvectors of <span class="math notranslate nohighlight">\(−L\)</span>, which correspond to connected components, it uses the next <span class="math notranslate nohighlight">\(r\)</span> eigenvectors (those with the smallest non-zero eigenvalues). These eigenvectors form a continuous embedding of the data. k-means is then applied to this embedding to discretize the solution and obtain clusters. However, this interpretation only tells half the story. In this narrative, it’s not clear why k-means is used for the discretization of eigenvectors. This makes sense however, when we derive the equivalence of graph clustering approaches to kernel k-means.</p>
</section>
<section id="relationship-to-kernel-k-means">
<h3>Relationship to Kernel k-means<a class="headerlink" href="#relationship-to-kernel-k-means" title="Link to this heading">#</a></h3>
<p>First, we observe that the minimum cut and the maximum similarity objective are equivalent for specific choices of the adjacency matrix:</p>
<div class="proof theorem admonition" id="theorem-6">
<p class="admonition-title"><span class="caption-number">Theorem 43 </span> (Minimum Cut and Maximum Similarity Equivalences)</p>
<section class="theorem-content" id="proof-content">
<p>Given a symmetric similarity matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}^{n\times n}_+\)</span>, the degree matrix <span class="math notranslate nohighlight">\(I_W\)</span> and the Graph Laplacian <span class="math notranslate nohighlight">\(L=I_W-W\)</span>, then the following objectives are equivalent:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \min_YCut(Y;W)  &amp;= \tr((\vvec{1}-Y)^\top WY(Y^\top Y)^{-1}) &amp;\text{ s.t }Y\in\mathbb{1}^{n\times r} \\
    \max_Y Sim(Y;-L) &amp;= \tr(Y^\top (-L) Y(Y^\top Y)^{-1}) &amp;\text{ s.t }Y\in\mathbb{1}^{n\times r}
\end{align*}\]</div>
</section>
</div><div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. Follows from the fact that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\tr(Y^\top I_W Y(Y^\top Y)^{-1}) &amp;= \sum_{s=1}^r\sum_{i=1}^n \frac{Y_{is}\lvert W_{i\cdot}}\rvert}{\lvert Y_{\cdot s}\rvert}\\
&amp;= \sum_{s=1}^r\sum_{i=1}^n \frac{\mathbf{1}^\top W Y_{\cdot s}}{\lvert Y_{\cdot s}\rvert}\\
&amp;= \tr (\mathbf{1}^\top W Y(Y^\top Y)^{-1})
\end{align*}\]</div>
</div>
</div>
<p>The maximum similarity objective is equal to the kernel <span class="math notranslate nohighlight">\(k\)</span>-means objective. However, note that <span class="math notranslate nohighlight">\(-L\)</span> is not a kernel matrix (it’s negative semi-definite). But we can make <span class="math notranslate nohighlight">\(-L\)</span> into a kernel, that doesn’t change the objective.</p>
<div class="proof corollary admonition" id="corollary-7">
<p class="admonition-title"><span class="caption-number">Corollary 8 </span> (Graph Clustering and k-means equivalence)</p>
<section class="corollary-content" id="proof-content">
<p>Given a symmetric matrix <span class="math notranslate nohighlight">\(W\in\mathbb{R}^{n\times n}_+\)</span>, having the smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_{min}\)</span>. If <span class="math notranslate nohighlight">\(\lambda_{min}\)</span> is nonnegative, then the maximum similarity graph clustering objective is equivalent to the kernel k-means objective for <span class="math notranslate nohighlight">\(K=W\)</span>.<br />
If <span class="math notranslate nohighlight">\(\lambda_{min}\)</span> is negative, then the maximum similarity graph clustering objective is equivalent to the kernel k-means objective with <span class="math notranslate nohighlight">\(K=W-\lambda_{min}I\)</span>.</p>
</section>
</div></section>
</section>
<section id="spectral-clustering-algorithm">
<h2>Spectral Clustering Algorithm<a class="headerlink" href="#spectral-clustering-algorithm" title="Link to this heading">#</a></h2>
<p>As a result, we have that the minimum cut objective is equivalent to kernel k-means with the kernel <span class="math notranslate nohighlight">\(K=-L-\lambda_{min}I\)</span>. That is, we can compute a symmetric factorization of<br />
<div class="math notranslate nohighlight">
\[K=-L-\lambda_{min}I = AA^\top\]</div>

and apply k-means on <span class="math notranslate nohighlight">\(A\)</span> to obtain the solution. Spectral clustering introduces now two changes to the kernel k-mean method:</p>
<ol class="arabic simple">
<li><p>Use only the eigenvectors <span class="math notranslate nohighlight">\(V\)</span> of <span class="math notranslate nohighlight">\(K=V\Lambda V^\top\)</span> instead of the scaled eigenvectors <span class="math notranslate nohighlight">\(V\Lambda^{1/2}\)</span>. This might be due to the fact that the equivalence stated above was not clear when spectral clustering was introduced. The eigendecomposition of <span class="math notranslate nohighlight">\(-L=V(\Lambda+\lambda_{min}I)V^\top\)</span> has only negative eigenvalues, which does not allow for the application of the square root.</p></li>
<li><p>Use a truncated approximation of <span class="math notranslate nohighlight">\(K\approx V_{cdot \mathcal{R}}\Lambda V_{\cdot \mathcal{R}}^\top\)</span> where <span class="math notranslate nohighlight">\(\mathcal{R}={2,\ldots, r+1}\)</span> excludes the first eigenvector that indicates the connected component. The truncated eigendecomposition speeds up the process and it allows k-means to focus the clustering search to a subspace that is relevant for clustering.</p></li>
</ol>
<section id="pseudocode">
<h3>Pseudocode<a class="headerlink" href="#pseudocode" title="Link to this heading">#</a></h3>
<p>The pseudocode below details the method of spectral clustering.</p>
<div class="proof algorithm admonition" id="algorithm-8">
<p class="admonition-title"><span class="caption-number">Algorithm 17 </span> (spectral clustering)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: data matrix <span class="math notranslate nohighlight">\(D\)</span>, number of clusters <span class="math notranslate nohighlight">\(r\)</span>, similarity matrix <span class="math notranslate nohighlight">\(W\)</span><br />
<strong>Require</strong>: the similarity matrix should indicate a connected graph</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(L\gets I_W-W\)</span> #<em>Compute Graph Laplacian - other graph Laplacians are also possible</em></p></li>
<li><p><span class="math notranslate nohighlight">\((V,\Lambda) \gets\)</span> <code class="docutils literal notranslate"><span class="pre">TruncatedEigendecomposition</span></code><span class="math notranslate nohighlight">\((L,r+1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A\leftarrow V_{\cdot\{2,\ldots, r+1\}}\)</span> #<em>Remove connected component</em></p></li>
<li><p><span class="math notranslate nohighlight">\((X,Y)\gets\)</span><code class="docutils literal notranslate"><span class="pre">kMeans</span></code><span class="math notranslate nohighlight">\((A,r)\)</span></p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(Y\)</span></p></li>
</ol>
</section>
</div></section>
</section>
<section id="application-to-the-two-circles-dataset">
<h2>Application to the Two Circles Dataset<a class="headerlink" href="#application-to-the-two-circles-dataset" title="Link to this heading">#</a></h2>
<p>We have a look at the clustering obtained by spectral clustering and the embedding on the two circles dataset. The code below implements spectral clustering for the difference Laplacian. This code is not optimized for efficiency and uses here a full eigendecomposition instead of computing the truncated eigendecomposition directly (which is much faster).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics.pairwise</span><span class="w"> </span><span class="kn">import</span> <span class="n">rbf_kernel</span>

<span class="n">D</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.08</span><span class="p">)</span>
<span class="n">W</span><span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">L</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="n">W</span>
<span class="n">lambdas</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/01568ad2632c1b7bf1e6616b9e1c24df39bd7a236281ee6363007104d4d6b4b3.png" src="_images/01568ad2632c1b7bf1e6616b9e1c24df39bd7a236281ee6363007104d4d6b4b3.png" />
</div>
</div>
<p>The plots below indicate the ground truth clustering in the original feature space and in the transformed feature space, spanned by the second and third eigenvector of <span class="math notranslate nohighlight">\(L\)</span>. Note that k-means clustering in the transformed feature space indicates directly the clustering in the original feature space.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/c7832394133a4e1a42ddbd92b37971582d066e3875e009684011bad6f1215cc7.png" src="_images/c7832394133a4e1a42ddbd92b37971582d066e3875e009684011bad6f1215cc7.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="clustering_kernel_kmeans.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Kernel k-means</p>
      </div>
    </a>
    <a class="right-next"
       href="clustering_exercises.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Exercises</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-graph-construction">Similarity Graph Construction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-neighborhood-graph">Epsilon-Neighborhood Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbor-graph">k-Nearest Neighbor Graph</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formal-objective-definitions">Formal Objective Definitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-similarity-within-clusters">Maximum Similarity Within Clusters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum-cut-between-clusters">Minimum Cut Between Clusters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-graph-laplacian">The Graph Laplacian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relationship-to-kernel-k-means">Relationship to Kernel k-means</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering-algorithm">Spectral Clustering Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pseudocode">Pseudocode</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-the-two-circles-dataset">Application to the Two Circles Dataset</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sibylle Hess
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022. Eindhoven University of Technology.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>