
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Support Vector Machines &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Neural Networks" href="neuralnets.html" />
    <link rel="prev" title="Random Forests" href="classification_random_forests.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_gradients.html">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_sparse.html">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_L1vsL2.html">
     L1 vs L2 Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_decision_trees.html">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Support Vector Machines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     Neural Networks Intro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_comp_graphs.html">
     Computational graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_func_approx.html">
     Function approximator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     Multi-Layer Perceptrons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_backprop.html">
     Backpropagation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_mf.html">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml/issues/new?title=Issue%20on%20page%20%2Fclassification_svms.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/20214358/dmml/edit/master/classification_svms.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/classification_svms.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-hyperplanes-mathematically">
   Defining Hyperplanes Mathematically
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-of-a-linear-svm-for-binary-classification">
   Inference of a Linear SVM for Binary Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-of-an-svm-when-classes-are-separable">
   Training of an SVM when Classes are Separable
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svm-for-two-non-separable-classes">
     SVM for Two Non-Separable Classes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dual-svm-formulation">
   Dual SVM formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-kernels">
   A word on kernels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-trick">
   Kernel trick
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Support Vector Machines</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-hyperplanes-mathematically">
   Defining Hyperplanes Mathematically
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-of-a-linear-svm-for-binary-classification">
   Inference of a Linear SVM for Binary Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-of-an-svm-when-classes-are-separable">
   Training of an SVM when Classes are Separable
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svm-for-two-non-separable-classes">
     SVM for Two Non-Separable Classes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dual-svm-formulation">
   Dual SVM formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-kernels">
   A word on kernels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-trick">
   Kernel trick
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="support-vector-machines">
<h1>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">#</a></h1>
<p>The classifiers we have seen so far have used various ways to define a decision boundary. KNN classifiers use a local view given by the neighbors, resulting in a nonlinear, typically fractured decision boundary, Naive Bayes has used elliptic (Gaussian) definitions of the likelihood to delineate between classes, and decision trees use hypercubed to define their decision boundary. Support Vector Machines (SVMs) use a hyperplane to delineate the classes, and the main motivation of the SVM is to ask what the best hyperplane is if the classes is separable. Letâ€™s have a look at a simple example with two classes and some options for defining the separating hyperplane.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<link rel="stylesheet"
href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<script language="javascript">
  function isInternetExplorer() {
    ua = navigator.userAgent;
    /* MSIE used to detect old browsers and Trident used to newer ones*/
    return ua.indexOf("MSIE ") > -1 || ua.indexOf("Trident/") > -1;
  }

  /* Define the Animation class */
  function Animation(frames, img_id, slider_id, interval, loop_select_id){
    this.img_id = img_id;
    this.slider_id = slider_id;
    this.loop_select_id = loop_select_id;
    this.interval = interval;
    this.current_frame = 0;
    this.direction = 0;
    this.timer = null;
    this.frames = new Array(frames.length);

    for (var i=0; i<frames.length; i++)
    {
     this.frames[i] = new Image();
     this.frames[i].src = frames[i];
    }
    var slider = document.getElementById(this.slider_id);
    slider.max = this.frames.length - 1;
    if (isInternetExplorer()) {
        // switch from oninput to onchange because IE <= 11 does not conform
        // with W3C specification. It ignores oninput and onchange behaves
        // like oninput. In contrast, Microsoft Edge behaves correctly.
        slider.setAttribute('onchange', slider.getAttribute('oninput'));
        slider.setAttribute('oninput', null);
    }
    this.set_frame(this.current_frame);
  }

  Animation.prototype.get_loop_state = function(){
    var button_group = document[this.loop_select_id].state;
    for (var i = 0; i < button_group.length; i++) {
        var button = button_group[i];
        if (button.checked) {
            return button.value;
        }
    }
    return undefined;
  }

  Animation.prototype.set_frame = function(frame){
    this.current_frame = frame;
    document.getElementById(this.img_id).src =
            this.frames[this.current_frame].src;
    document.getElementById(this.slider_id).value = this.current_frame;
  }

  Animation.prototype.next_frame = function()
  {
    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));
  }

  Animation.prototype.previous_frame = function()
  {
    this.set_frame(Math.max(0, this.current_frame - 1));
  }

  Animation.prototype.first_frame = function()
  {
    this.set_frame(0);
  }

  Animation.prototype.last_frame = function()
  {
    this.set_frame(this.frames.length - 1);
  }

  Animation.prototype.slower = function()
  {
    this.interval /= 0.7;
    if(this.direction > 0){this.play_animation();}
    else if(this.direction < 0){this.reverse_animation();}
  }

  Animation.prototype.faster = function()
  {
    this.interval *= 0.7;
    if(this.direction > 0){this.play_animation();}
    else if(this.direction < 0){this.reverse_animation();}
  }

  Animation.prototype.anim_step_forward = function()
  {
    this.current_frame += 1;
    if(this.current_frame < this.frames.length){
      this.set_frame(this.current_frame);
    }else{
      var loop_state = this.get_loop_state();
      if(loop_state == "loop"){
        this.first_frame();
      }else if(loop_state == "reflect"){
        this.last_frame();
        this.reverse_animation();
      }else{
        this.pause_animation();
        this.last_frame();
      }
    }
  }

  Animation.prototype.anim_step_reverse = function()
  {
    this.current_frame -= 1;
    if(this.current_frame >= 0){
      this.set_frame(this.current_frame);
    }else{
      var loop_state = this.get_loop_state();
      if(loop_state == "loop"){
        this.last_frame();
      }else if(loop_state == "reflect"){
        this.first_frame();
        this.play_animation();
      }else{
        this.pause_animation();
        this.first_frame();
      }
    }
  }

  Animation.prototype.pause_animation = function()
  {
    this.direction = 0;
    if (this.timer){
      clearInterval(this.timer);
      this.timer = null;
    }
  }

  Animation.prototype.play_animation = function()
  {
    this.pause_animation();
    this.direction = 1;
    var t = this;
    if (!this.timer) this.timer = setInterval(function() {
        t.anim_step_forward();
    }, this.interval);
  }

  Animation.prototype.reverse_animation = function()
  {
    this.pause_animation();
    this.direction = -1;
    var t = this;
    if (!this.timer) this.timer = setInterval(function() {
        t.anim_step_reverse();
    }, this.interval);
  }
</script>

<style>
.animation {
    display: inline-block;
    text-align: center;
}
input[type=range].anim-slider {
    width: 374px;
    margin-left: auto;
    margin-right: auto;
}
.anim-buttons {
    margin: 8px 0px;
}
.anim-buttons button {
    padding: 0;
    width: 36px;
}
.anim-state label {
    margin-right: 8px;
}
.anim-state input {
    margin: 0;
    vertical-align: middle;
}
</style>

<div class="animation">
  <img id="_anim_img5e1aa03096434066a570480411c28485">
  <div class="anim-controls">
    <input id="_anim_slider5e1aa03096434066a570480411c28485" type="range" class="anim-slider"
           name="points" min="0" max="1" step="1" value="0"
           oninput="anim5e1aa03096434066a570480411c28485.set_frame(parseInt(this.value));">
    <div class="anim-buttons">
      <button title="Decrease speed" aria-label="Decrease speed" onclick="anim5e1aa03096434066a570480411c28485.slower()">
          <i class="fa fa-minus"></i></button>
      <button title="First frame" aria-label="First frame" onclick="anim5e1aa03096434066a570480411c28485.first_frame()">
        <i class="fa fa-fast-backward"></i></button>
      <button title="Previous frame" aria-label="Previous frame" onclick="anim5e1aa03096434066a570480411c28485.previous_frame()">
          <i class="fa fa-step-backward"></i></button>
      <button title="Play backwards" aria-label="Play backwards" onclick="anim5e1aa03096434066a570480411c28485.reverse_animation()">
          <i class="fa fa-play fa-flip-horizontal"></i></button>
      <button title="Pause" aria-label="Pause" onclick="anim5e1aa03096434066a570480411c28485.pause_animation()">
          <i class="fa fa-pause"></i></button>
      <button title="Play" aria-label="Play" onclick="anim5e1aa03096434066a570480411c28485.play_animation()">
          <i class="fa fa-play"></i></button>
      <button title="Next frame" aria-label="Next frame" onclick="anim5e1aa03096434066a570480411c28485.next_frame()">
          <i class="fa fa-step-forward"></i></button>
      <button title="Last frame" aria-label="Last frame" onclick="anim5e1aa03096434066a570480411c28485.last_frame()">
          <i class="fa fa-fast-forward"></i></button>
      <button title="Increase speed" aria-label="Increase speed" onclick="anim5e1aa03096434066a570480411c28485.faster()">
          <i class="fa fa-plus"></i></button>
    </div>
    <form title="Repetition mode" aria-label="Repetition mode" action="#n" name="_anim_loop_select5e1aa03096434066a570480411c28485"
          class="anim-state">
      <input type="radio" name="state" value="once" id="_anim_radio1_5e1aa03096434066a570480411c28485"
             >
      <label for="_anim_radio1_5e1aa03096434066a570480411c28485">Once</label>
      <input type="radio" name="state" value="loop" id="_anim_radio2_5e1aa03096434066a570480411c28485"
             checked>
      <label for="_anim_radio2_5e1aa03096434066a570480411c28485">Loop</label>
      <input type="radio" name="state" value="reflect" id="_anim_radio3_5e1aa03096434066a570480411c28485"
             >
      <label for="_anim_radio3_5e1aa03096434066a570480411c28485">Reflect</label>
    </form>
  </div>
</div>


<script language="javascript">
  /* Instantiate the Animation class. */
  /* The IDs given should match those used in the template above. */
  (function() {
    var img_id = "_anim_img5e1aa03096434066a570480411c28485";
    var slider_id = "_anim_slider5e1aa03096434066a570480411c28485";
    var loop_select_id = "_anim_loop_select5e1aa03096434066a570480411c28485";
    var frames = new Array(5);

  frames[0] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAA0c0lEQVR4nO3deZgU5b328e/jAogo0XMSosET9U1eX5WTxIwakxhlXxUQRUFZ\
BBEQUFCRRUBRARUXNhEQ2UFABGEUTRSFGI9bXFAJaNTEEzTuyDLsML/3j6oeimaWnpnurqqe+3Nd\
fU13rb+ubuqmnnq6ypkZIiIicXNY2AWIiIhUhAJMRERiSQEmIiKxpAATEZFYUoCJiEgsKcBERCSW\
FGAiIhJLCjAREYklBZiIiMSSAkxERGJJASYiIrGkABMRkVhSgImISCwpwEREJJYUYCIiEksKMBER\
iSUFmIiIxJICTEREYkkBFmPOOXPO/SxDy/6Dc+7DNC3rU+dc4zQtK2PvORc55/7mnKufgeVe7Zx7\
Od3LzQR9Z3KXAiwinHNDnXPPJg37qIRhHdKwvlJDxcz+YmanVWC5s51zoypXXXbFsebiFPc+zOxM\
M1sTUklVhnPul865551zm5xzG51zV4ddU1WgAIuOl4DfOecOB3DOnQAcCZyVNOxn/rRShTjnjgi7\
hihK/NuIgJOAycAJwBXANOfcieGWVAWYmR4ReADVgB1Anv/6cmAW8OekYR8H5jGgN/ARsBnvH5Dz\
x/0f4EXgO+BbYAHwA3/cPKAQ2AkUAIOKqac+8Fng9WDgc2Ab8CHQqJh5egJ7gT3+cp/yh38KDATe\
A7YAi4EagfkuAtb67+EV4BelbCcDbgD+4b+v+4DDAuO7AxuA74E/AT/1hztgHPA1sBV4H6hXUs1J\
6yx2Xn9cdeB+4F/AV8BU4KjgNgRu9Wv9FLgqsNxWwDv+MjcCIwPjTvbf6zX+sl/yhy8BvvS340vA\
mSls+8b+85HA48Bc/3P8G3B2YJ2/9uvZ5q9nMTCqhM/hauBl/71/D/wTaOGPaw+8lTT9TcAK//ls\
fzs976/rz4nPyR////xxm/C+a5cHxs0GpgDPANuBxiksz4CflWObd/W3+bfAsMD4w4AhwCd4/64e\
B44vZtsc4X8G/x32fiXXH6EXoEfgw4DVwI3+84fwdsajk4bNDExvwNPAD4D/Ar4BmvvjfgY0wdvB\
/tDf2Y0PzFu0Yyuhlvr4AQac5v9jP9F/fTLwf0qYb3byTs9f1xvAicDxeAHT2x93Fl4w/AY43N95\
fApUL2H55m+n4/33/Heghz+uDfAxcLq/ExkOvOKPawa85W8r509zQkk1J62ztHnHAfl+PccATwF3\
B7bhPuBB/3O4EG+ne1pg/H/7O8Zf4AVg28A2NrywOZoDodjdX091YDywNoVtHwywXUBLf1vfDbzm\
j6sG/C/QH+/Ivx1eGJYWYHuBa/1lXQf8298+1fHC5/TA9O8Alwbq3AZc4E87AXjZH3c03netm/8Z\
noUXJGcE5t0C/N7fbjVKW17gO/Ozcmzz6cBRwC+B3Yn34W+b14C6/nqmAQuL2TYT8b7vhxW37fRI\
4z4z7AL0CHwY3g7mSf/5u8DPgeZJw7oGpjfg/MDrx4EhJSy7LfBO4HXRjq2E6etzIMB+hhcyjYEj\
y3gPs5N3ev66OgVejwWm+s+nAHclTf8hcGEJyzf8kPZf9wFe8J8/C1wTGHcY3lHtT4GGeGF3XvKO\
pbiak8YXOy/ezno7gTAHfgv8M7AN9wFHJ31GI0pYz3hgnP88sTM9tZS6fuBPU7uMbR8MsFWBcWcA\
O/3nF+AdYbvA+JdL2i54ARZsDajp1/LjwOc62n9+Jt5RWvVAnYsC89YC9uM1w10B/CVpXdOA2wPz\
zi3m8yt2eYHvzM/Ksc3rBsa/AXTwn28g0PKA11y4FzgiMGwQ3vf3x6X9O9EjPQ+dA4uWl4DznXPH\
Az80s4/wmtR+5w+rx6Hnv74MPN+B948X51wd59wi59znzrmtwHzgPytSlJl9DAzA2wF+7S+3vO37\
xdaJFy43O+c2Jx54O7LSlr8x8Px/A9P+FJgQWM4mvJD5iZm9iHcEO9l/D484545NpfBS5v0h3o77\
rcA6/+gPT/jezLYXV69z7jfOudXOuW+cc1vwmoOTP6Oi9+qcO9w5d49z7hP/M/3UH1WezzX5c6jh\
n187Efjc/L1w8rrLWpaZ7fCfJj7XOcCVzjkHdAYeN7PdxS3bzArwPqsT8T7D3yR9H64CflxGXSUt\
7yApbvPSvqtPBuragBeUdQLTDwCuNbPgMiRDFGDR8ipQG69Z5n8AzGwrXtPMtcC/zeyfKS5rDN7/\
Jv/bzI4FOuHtzBOs2LlKYGaPmdn5eP+IDbi3pEnLs1y8Hc9oM/tB4FHTzBaWMs9Jgef/hbd9Esvq\
lbSso8zsFf89TDSzPLwjj/8L3JJqzSXM+y3eecQzA+urbWa1ArMe55w7uoR6H8NrfjzJzGrjnccJ\
fkbJtV2J10zaGO97crI/3BUzbXl9AfzED5yEk0qauCxm9hpeE+Qf8OqelzRJ0bKdc7XwmmD/jfcZ\
/jnpM6xlZtcFF1/MKktaXrJUtnlJNuKd5wvWVsPMPg9Mc0IJ65UMUIBFiJntBN7EO+H9l8Col/1h\
5el9eAzeieQtzrmfcGBnnfAVcGoqC3LOneaca+icq453DmUnXieQ4qS8XN90oLf/P2PnnDvaOdfK\
OXdMKfPc4pw7zjl3Et55icX+8KnAUOfcmX7dtZ1z7f3n5/jrOBKv2W9X4D2UWnNJ85pZoV//OOfc\
j/xpf+Kca5a0iDucc9Wcc3/A67CyxB9+DLDJzHY5587F29GX5hi8czLf4R35jUkaX95tH/Qq3tFE\
P+fcEc65NsC5FVxWwly8I9e9Zpb8m7GWzrnznXPVgLvwzsVtxDun+3+dc52dc0f6j3Occ6eXsa6S\
lpesvNs8aCow2jn3UwDn3A/97RR0Al6HFskCBVj0/Bn4EV5oJfzFH1aeALsDr1fZFmAlsCxp/N3A\
cL85ZGAZy6oO3IN3xPGlX8vQEqadAZzhL3d5WUWa2Zt4R5cP4Z0n+Rjv/EppVuB1qliL995m+Mt6\
Eu/IcJHfxLYOaOHPcyxe2HyP14z3HV4PxlRqLm3ewX7Nr/nrXIXX6SXhS3++f+P1BO1tZh/44/oA\
dzrntgG34Z0fK81cf/2fA+vxOhQElWvbB5nZHryOG9fg9QbthBcmu0uZrSzz8Jq95xcz7jHgdrym\
vjx/fZjZNqAp0AFvm32J95lWL2NdxS6vGOXd5kET8I7envPnfw2v81HQx3itFJIFiS7XIpJmzrsC\
xnwzqxtyKRXinHsdr7PNrArOfxRe559f++dzE8Nn43UQGp6mOtO6PIkPHYGJCADOuQudcz/2mxC7\
4nUz/2MlFnkd8NdgeImkU6R+3e+cm4l3juBrM6vnD7sPuBjvhPAnQDcz2xxakSK56zS8JrWj8X4o\
fpmZfVGRBTnnPsXrHNE2XcWJJItUE6Jz7gK8jgdzAwHWFHjRzPY55+4FMLPBIZYpIiIREKkmRDN7\
Ce8kbHDYc2a2z3+Z+BW8iIhUcZEKsBR0x7vagoiIVHGROgdWGufcMLzL8iwoZZqeeBc15aijjso7\
6aQDv8MsLCzksMOilddRrAmiWZdqSl1U6tq/fz8bN25k7969nHjiiRx99NFlz5RFUdlOycKu6+9/\
//u3ZvbDsqeMgLCvZZX8wLu6wLqkYVfj/dCyZqrLycvLs6DVq1db1ESxJrNo1qWaUheFur766iur\
V6+eHXXUUfbCCy9EoqZkUazJLPy6gDctAlmQyiN6//1I4pxrjneBzNZ24HprIhJR33zzDY0aNeKT\
Tz7h6aefpmHDhmGXJDkqUgHmnFuId6R1mnPuM+fcNXhXaDgGeN45t9Y5NzXUIkWkRN988w0NGzbk\
k08+4amnnlJ4SUZF6hyYmXUsZvCMrBciIuWWCK+PP/6Yp59+mkaNGoVdkuS4SAWYiMTTt99+S6NG\
jYoNr71791KrVi02bNgQYoWHql27duRqguzVVaNGDerWrcuRRx6Z8XVligJMRCrl22+/pWHDhnz0\
0Uc89dRThxx5ffbZZ9SpU4e6dety8N1awrVt2zaOOaa0mx6EIxt1mRnfffcdn332GaecckpG15VJ\
kToHJiLxkjjy+uijj8jPz6dx48aHTLNr1y5q164dqfCq6pxz/Md//Ae7du0Ku5RKUYCJSIV89913\
NG7cmL///e/k5+fTpEmTEqdVeEVPLnwmCjARKbfvvvuORo0a8eGHH7JixYpSwysKatWqddDr2bNn\
c/PNN4dUDZx88sl8++23GVv+d999R4MGDahVqxb9+vXL2HrCpnNgIlIuifD64IMPyM/Pp2nTpmGX\
FDn79u3jiCPC273WqFGDu+66i3Xr1rFu3brQ6sg0HYGJSMoSzYa5El7btm3jlFNOYe/evQBs3bq1\
6HX9+vXp378/v/rVr6hXrx5vvPEGANu3b6d79+6ce+65nHXWWaxYsQLwjupat25Nw4YNadSoEWvW\
rOGCCy6gVatWnHbaafTu3ZvCwsJDamjbti15eXmceeaZPPLII0XDa9WqxbBhw/jlL3/Jeeedx1df\
fQV4P1e49NJLOeecczjnnHP4n//5n0OWefTRR3P++edTo0aNtG+zKNERmIikZNOmTTRp0oQNGzaw\
YsWKCoXXgAEDWLt2bVrr+tWvfsX48eNLnWbnzp386le/Knq9adMmmjdvzjHHHEP9+vVZuXIlbdu2\
ZdGiRbRr166oa/mOHTtYu3YtL730Et27d2fdunWMHj2ahg0bMnPmTDZv3sy5555b1Hnl7bff5r33\
3uP4449nzZo1vPHGG6xfv56f/vSnNG/enGXLlnHZZZcdVNvMmTM5/vjj2blzJ+eccw5NmzblmGOO\
Yfv27Zx33nmMHj2aQYMGMX36dIYPH07//v258cYbOf/88/nXv/5Fs2bNIvlzgGxQgIlImTZt2kTj\
xo1Zv349K1asoFmzZmGXVC5HHXXUQcE5e/ZsXnnlFQB69OjB2LFjadu2LbNmzWL69OlF03Xs6F1b\
4YILLmDr1q1s3ryZ5557jvz8fO6//37A62X5r3/9C4AmTZpw/PHHF81/7rnncuqppxYt6+WXXz4k\
wCZOnMiTTz4JwMaNG/nkk084+eSTqVatGhdddBEAeXl5PP/88wCsWrWK9evXF82/detWCgoKDjnP\
VxUowESkVMHwWr58eaXCq6wjpTD8/ve/59NPP2XNmjXs37+fevXqFY1L7qnnnMPMWLp0KaeddtpB\
415//fVDrrhf3PxBa9asYdWqVbz66qvUrFmT+vXrs3v3bgCOPPLIoukPP/xw9u3zbotYWFjIa6+9\
lvPNg6nQOTAJVX4+9Ovn/ZXo+f7772nSpElReDVv3jzskjKiS5cuXHnllXTr1u2g4YsXLwbg5Zdf\
pnbt2tSuXZtmzZoxadKkxJ0yeOedd0pc7htvvME///lPCgsLWbx4Meeff/5B47ds2cJxxx1HzZo1\
+eCDD3jttdfKrLVp06ZMmjSp6HW6m2TjRAEmocnPh44dYfJk769CLFq+//57GjduzLp163jyySdz\
NrwArrrqKr7//vuiJsOEGjVqcNZZZ9G7d29mzPAuyzpixAj27t3LL37xC84880xGjBhR4nLPOecc\
+vXrx+mnn84pp5zCJZdcctD45s2bs2/fPk4//XSGDBnCeeedV2atEydO5M033+QXv/gFZ5xxBlOn\
Fn9985NPPpmbbrqJ2bNnU7du3YOaHXNG2PdzydRD9wOruGzV1bevGRx49O0bfk3lEcWazNJT16ZN\
m+zXv/61VatWzZ555plKLWv9+vW2devWSteUbsGalixZYp06dTpo/IUXXmh//etfK7z81atXW6tW\
rSpVV6atX7/+kGHE6H5gOgcmoWnaFGbNgh07oGZN77WEL9FsmDjyatGiRdglZdT111/Ps88+yzPP\
PBN2KVJOCjAJTevWsHAhPPecF16tW4ddkSTC6/3332fZsmW0bNky7JIyLng+KWjNmjWVWm79+vWp\
X79+pZYhpVOASahat1ZwRcX3339P06ZNef/991m6dCmtWrUKuySRUqkTh4iwefNmmjZtynvvvcfS\
pUuLfn+ULub32JPoyIXPRAEmUsVt3ryZJk2a8O6772YkvGrUqMGWLVtyYoeZK8y/H1jcf0umJkSR\
Kixx5PXuu++ybNmytIcXQN26dXn33XcpKChI+7IrY9euXZHcgWerrsQdmeNMASZSRW3evJlmzZqx\
du3ajBx5JRx55JEUFBRw9tlnZ2T5FbVmzRrOOuussMs4RFTriiI1IYpUQVu2bKFZs2a88847PPHE\
E1x88cVhlyRSbpELMOfcTOfc1865dYFhxzvnnnfOfeT/PS7MGiX9dEmp7NmyZQtNmzblnXfeYcmS\
JbRWN1CJqcgFGDAbSL5mzRDgBTP7OfCC/1pyhC4plT2JI6+3336bJUuW0KZNm7BLEqmwyAWYmb0E\
bEoa3AaY4z+fA7TNZk2SWc89512NA7y/zz0Xbj25KhFeb731lsJLckLkAqwEdczsC//5l0CdMIuR\
9Gra1LuUFOiSUpmydetWmjdvXhRebdu2DbskkUpzUfxthnPuZOBpM6vnv95sZj8IjP/ezA45D+ac\
6wn0BKhTp07eokWLisZF8YZvUawJwqlryxbYuhWOPRZq145GTWWJYk1waF3bt29n8ODBfPDBB4wc\
OfKQW3qEUVMURLEmCL+uBg0avGVm0eoyWpKwryZc3AM4GVgXeP0hcIL//ATgw7KWoavRV1wU68pU\
TStWeFfBX7Gi/PNGcTuZHVzXli1b7Le//a0dccQRtmzZskjUFBVRrMks/LqI0dXo49KEmA909Z93\
BVaEWIvkiFzvPJJoNvzrX//K4sWLD7kXlUjcRS7AnHMLgVeB05xznznnrgHuAZo45z4CGvuvJebC\
7jqfy51Htm3bRosWLYrCq127dmGXJJJ2kbsSh5l1LGFUo6wWIhmVOPrZscO7J9iAAd55sGzeViVX\
70e2Y8cOmjdvzuuvv67wkpwWuQCTqiH56GfsWNi3zwuUhQuzE2K5eD+ybdu2MXjwYDZs2MDixYu5\
9NJLwy5JJGMi14QoVUOw6/zhh3vhBdlvymvdGh56KHfCq0WLFqxfv17hJVWCAkwyqqTzXImjn759\
YfBg/Q6ssrZt20bLli157bXXuO222xReUiWoCVEyJnie69FHoVEj6NXrwNFO8G7Mv/lNbjXlZVMi\
vF599VUWLlzID3/4w7BLEskKBZikXX6+F0b//OeB81y7d8Mzz8CaNcWf4wqGmaSuoKDgoPBq3749\
a9asCbsskaxQgElaBY+6qlWD6tW98EpInOPKVFglwrMqHMkFw+uxxx6jffv2YZckklU6ByZpFexd\
uGeP12zYsqUXZlD8Oa50/R4s13+YHJQIr1deeYUFCxZw+eWXh12SSNYpwCStki/M26sXrFwJS5Z4\
HTaSmw/TGTq5/MPkoIKCAlq1alUUXldccUXYJYmEQgEmaRXsXRgMq5K6q6czdKrCVe23b99Oq1at\
ePnllxVeUuXpHJikXXk6ZKTzahi5+MPkIIWXyMEUYBKqdIdOrvZm3L59OxdddBF/+ctfWLBgAR06\
dAi7JJHQKcAkdMmhk62ehHHpsbh9+3YuvvhiXnrpJebPn6/wEvEpwCRUySFS0o+fjz02/esNXkw4\
W9dfLK8dO3Zw8cUX8+c//5m5c+fSsWNJ17oWqXrUiUNCU1wPxGCnjsSPnzt29K5UnzxvZbrex6HH\
4o4dO7jooouKwuuqq64KuySRSFGASWiSQ2TYMKhd+0BPwoQdO2Dr1gOv09H1Puo9FoNHXnPmzFF4\
iRRDASahCYYIwLp1MH68d2+w5B8/B5sQ03H0VFJ3/yhIhNfq1auZM2cOnTp1CrskkUhSgEmFVbYZ\
LxEi9eodGLZjh9dcmPzj59q1D0yTrqOnKN5KZceOHbRu3ZrVq1czd+5chZdIKdSJQyokXZ0gEvMk\
lhUMpGDvxOD1aXP1916J8HrxxRd15CWSAgWYVEhJzXgVCZWKBFKu/d5rx44dtGnTpii8OnfuHHZJ\
IpGnAJMKSb6CRu3alTsiy7VAKo+dO3fSpk0bXnjhBWbPnq3wEkmRzoFJhSR3gtiyJfrd0qMoGF6z\
Zs2iS5cuYZckEhuxOQJzzt0I9AAMeB/oZma7wq2qaks+akrXNQ2rikR4rVq1ilmzZtG1a9ewSxKJ\
lVgcgTnnfgLcAJxtZvWAwwFdTydCotwtPYp27txJ27ZtWbVqFTNnzlR4iVRAbI7A8Go9yjm3F6gJ\
/DvkeiRJps5j5ed7TZT5+bkRjLt27aJt27Y8//zzzJgxg6uvvjrskkRiKRZHYGb2OXA/8C/gC2CL\
meksSxWQ6K7/9de5cZfl5PDq1q1b2CWJxJYzs7BrKJNz7jhgKXAFsBlYAjxhZvOTpusJ9ASoU6dO\
3qJFi4rGFRQUUKtWrWyVnJIo1gTRqmvjRi+86tYt4LPPavGjH8FJJ4Vdlae822nPnj0MHz6cN998\
k4EDB9KyZctI1JUNqil1YdfVoEGDt8zs7NAKKA8zi/wDaA/MCLzuAjxc2jx5eXkWtHr1aouaKNZk\
Fq26Vqwwq1nT7P77V1vNmt7rVObp2ze1aSujPNtp586d1qxZMwNsxowZmSvKovX5Jaim1IVdF/Cm\
RWC/n8ojFk2IeE2H5znnajrnHNAI2BByTZIFic4hP/pRap1D0nGh33TbtWsXl1xyCX/605+YPn06\
3bt3D7skkZwQiwAzs9eBJ4C38brQHwY8EmpRkjWtW3vNhql04IjabVJ27dpFu3bt+OMf/8j06dPp\
0aNHuAWJ5JBYBBiAmd1uZv/PzOqZWWcz2x12TVI+lbn475Ytqc0bpduk7N69m0svvZRnn31W4SWS\
AbEJMIm3yjTt5efDP/6R2rxR+T3a7t27adeuHc888wzTpk1TeIlkgAJMsqIyTXvPPQeFhanPG/Zt\
UpLDq2fPnuEUIpLjFGCSFZVp2mvaFA47rGLzZlui2VDhJZJ5cboSh8RYZe7h1bo1rFjhNQtG+f5f\
ifBauXIlU6dOVXiJZJgCTLKmMpeaql3baxZMyM+P1g0td+/ezWWXXcbKlSuZMmUKvXr1CrskkZyn\
JkSJnaj91mvPnj20b9+ep59+milTptC7d+9wCxKpIhRgEiv5+TBsWHR+67Vnzx4uu+wynnrqKR5+\
+GGFl0gWqQlRYiM/H9q3hz17DgwLs1PH3r17i8Jr8uTJXHfddeEUIlJFKcAkNqZNOzi8/uu/YNKk\
cM6B7dmzh5EjR/LKK68wefJk+vTpk/0iRKo4NSFKbNWrF154XX755bzyyis89NBDCi+RkCjAJDZ6\
9YLq1b3n1at7r7Ntz549XHHFFaxYsYIbbriBvn37Zr8IEQHUhCgx0ro1PP54eN3nE+G1fPlyJk2a\
RL169bJbgIgcRAEmsVKZ35JVxp49e+jQoQPLly9nwoQJ9OvXjzVr1mS/EBEpoiZEkTLs3buXDh06\
8OSTTzJhwgRuuOGGsEsSERRgIqXau3cvV1xxhcJLJIIUYCIlCB55jR8/XuElEjEKMJFi7N27l44d\
O7Js2TLGjRtH//79wy5JRJIowESSJMJr6dKljBs3jgEDBoRdkogUQwEmErB3716uuuoqli5dyoMP\
PqjwEokwBZiILxFeS5Ys4YEHHuDGG28MuyQRKYUCTATYt2/fQeF10003hV2SiJQhNgHmnPuBc+4J\
59wHzrkNzrnfhl2T5AaFl0g8xelKHBOAP5rZZc65akDNsAuS+EuE1+OPP87999+v8BKJkVgEmHOu\
NnABcDWAme0B9pQ2j0hZ9u3bR6dOnYrC6+abbw67JBEph7g0IZ4CfAPMcs6945x71Dl3dNhFSXzt\
27ePzp07s3jxYu677z6Fl0gMOTMLu4YyOefOBl4Dfm9mrzvnJgBbzWxE0nQ9gZ4AderUyVu0aFHR\
uIKCAmrVqpXFqssWxZogmnWls6b9+/czZswYXnzxRXr16kWHDh1CrymdoliXakpd2HU1aNDgLTM7\
O7QCysPMIv8Afgx8Gnj9B2BlafPk5eVZ0OrVqy1qoliTWTTrSldNe/futY4dOxpgY8eOjURN6RbF\
ulRT6sKuC3jTIrDfT+URiyZEM/sS2OicO80f1AhYH2JJEkP79u2jS5cuLFy4kHvvvZdbbrkl7JJE\
pBJi0YnDdz2wwO+B+A+gW8j1SIzs27ePrl27snDhQu655x4GDRoUdkkiUkmxCTAzWwvEo11WImX/\
/v107dqVxx57jLvvvpvBgweHXZKIpEEsmhBFKioYXmPGjGHIkCFhlyQiaaIAk5yVCK8FCxYwZswY\
hg4dGnZJIpJGCjDJSfv37+fqq69mwYIFjB49WuElkoMUYJJz9u/fT7du3Zg/fz6jRo3i1ltvDbsk\
EckABZjklER4zZs3j1GjRjFs2LCwSxKRDFGASc7Yv38/3bt3Z968edx1110KL5EcpwCTnLB//36u\
ueYa5s6dy1133cXw4cPDLklEMkwBJrGXCK85c+Zw5513KrxEqggFmMRaMLzuuOMORowYUfZMIpIT\
FGASW4WFhfTo0aMovG677bawSxKRLFKASSwlwmv27NmMHDlS4SVSBSnAJHYS4TVr1ixuv/12br/9\
9rBLEpEQKMAkVgoLC7n22muLwmvkyJFhlyQiIVGASWwkwmvmzJncdtttCi+RKk4BJrEQDK8RI0Yo\
vEQkPvcDk6qrsLCQBx98kJUrVzJ8+HDuuOMOnHNhlyUiIdMRmERaYWEhvXr1YuXKlQwbNow777xT\
4SUigAJMIqywsJDevXvz6KOP0qlTJ+666y6Fl4gUUROiRFJhYSHXXXcd06dP59Zbb6Vx48YKLxE5\
iI7AJHIKCwvp06cPjzzyCEOHDmXUqFEKLxE5hAJMIiURXtOmTWPo0KGMHj1a4SUixYpVgDnnDnfO\
veOcezrsWiT9CgsL6du3L9OmTWPIkCEKLxEpVawCDOgPbAi7CEm/wsJC+vXrx9SpUxk8eDBjxoxR\
eIlIqWITYM65ukAr4NGwa5H0MjP69evHlClTGDx4MHfffbfCS0TKFJsAA8YDg4DCkOuQNDIz+vbt\
y5QpUxg0aJDCS0RS5sws7BrK5Jy7CGhpZn2cc/WBgWZ2UTHT9QR6AtSpUydv0aJFReMKCgqoVatW\
dgpOURRrguzVZWZMmDCBFStW0KFDB3r27FlieEVxW0WxJohmXaopdWHX1aBBg7fM7OzQCigPM4v8\
A7gb+Az4FPgS2AHML22evLw8C1q9erVFTRRrMstOXYWFhdanTx8D7JZbbrHCwsLQayqvKNZkFs26\
VFPqwq4LeNMisN9P5RGLJkQzG2pmdc3sZKAD8KKZdQq5LKkgM+P666/n4YcfZuDAgdx7771qNhSR\
cotFgEnuMDNuuOEGJk+ezMCBAxk7dqzCS0QqJHYBZmZrrJjzXxJ9ifB66KGHuPnmmxVeuSYf6Of/\
FcmC2AWYxJOZ0b9/fx566CFuuukm7rvvPoVXLskHOgKT/b/5SeM2omCTtFOAScaZGQMGDGDSpEnc\
eOON3H///QqvXPMcXtcq/L/P+c8TwfY1hwabSCUpwCSjEuE1ceJEBgwYwAMPPKDwykVNgZr+85r+\
ayg52ETSQAEmGWNm3HjjjUycOJH+/fvz4IMPKrxyVWtgIdDX/9vaH15SsCXovJlUgu4HJhlhZtx8\
881MmDCB/v37M27cOIVXrmvNgeAKDlsIbOHgYIMDzYs7gFnFjBcpg47AJO0S4TVu3DhuuOEGhVdV\
1xo4iUPDSc2LUkkKMEkrM2PgwIFF4TV+/HiFlxSvrOZFkTKoCVHSxsy45ZZbePDBB7n++usVXlK6\
RPPic3jhpeZDKScFmKRFIrweeOAB+vXrx4QJExReVU0+XhjVxjvnlQilfP91PsWfI1NwSQUpwKTS\
zIxBgwYVhdfEiRMVXlVNsENGwixgAN6NkO4EelN6R41EAOpoTFKkc2BSKYnwuv/+++nbt6/Cq6oK\
dshI2IEXSql01CjtSh4iJVCASYWZGYMHDy4Kr0mTJim8siGKv50KdshIqIl3JJVKRw31SJQKUBOi\
VIiZMWTIEO677z769Omj8MqWqP52KtghI/kc2G8o/ndgQU3x3s8O1CNRUqYAk3JLhNfYsWPp06cP\
Dz30kMIrW4o7UolCgEHJHTJaA2uA+mXMqx6JUk4KMCkXM2Po0KGMHTuW6667TuGVbbl8pKIeiVJO\
CjBJWSK87r33Xnr37q3wCkPYRyrqKSgRogCTlJgZt956K/feey+9evVi8uTJHHaY+gCFIqwjlaie\
f5MqS3sgKZOZMWzYMO655x569uzJww8/rPCqitRTUCJGeyEplZkxfPhw7r77bnr27MmUKVMUXlVV\
cdcuTEeX/ij+LEBiQU2IUiIzY8SIEYwZM4Zrr71W4VXVJZ9/g7KbFBPnzJqUsEw1S0olaG8kxTIz\
brvtNkaPHk2PHj2YOnWqwku8cHnI/1tWk2Lw6hr/oPgjLDVLSiXEYo/knDvJObfaObfeOfc351z/\
sGvKZYnwGjVqFD169GDatGkKLzlUWbdDCYZTIcWHk26pIpUQlybEfcDNZva2c+4Y4C3n3PNmtj7s\
wnKNmTFr1izmzZun8JLSldWlP/ibtcMoPpzC/lmAxFosAszMvgC+8J9vc85tAH4CKMDSyMwYOXIk\
8+bN45prrlF4SdlK69IfDKdTy5hOwSUVELu9k3PuZOAs4PWQS8k5I0eO5M4776RFixY88sgjCi+p\
vMQ5s9phFyK5yJlZ2DWkzDlXC/gzMNrMlhUzvifQE6BOnTp5ixYtKhpXUFBArVq1slVqSqJU0+zZ\
s5kzZw4tWrSgd+/eHHvssWGXdJAobauEKNYE0axLNaUu7LoaNGjwlpmdHVoB5WFmsXgARwJ/Am5K\
Zfq8vDwLWr16tUVNVGq6/fbbDbBu3brZ/v37I1NXkGpKXcbrWmFmff2/KY6L4raKYk1m4dcFvGkR\
2Oen8ohFG5HzLrg3A9hgZg+GXU8uueOOO7jjjjvo1q0bjz76qJoNpXSl3XgyOK4dMCzr1UkVE4tO\
HMDvgc7A+865tf6wW83smfBKir8777yTkSNHcvXVVyu85FCJHyEH7+9V2u1cguP2A2Px7gWmDhqS\
IbEIMDN7GdBlz9Pozjvv5Pbbb6dr164Kr6qqtCvLB6+QkTALGID3e63ibufSFJiKF17g/fglSvcr\
k5yjvVYVdNdddxWF14wZMzj88MPDLkmyrbSmQDj4aCphBwfurNyXQy/71BoYzIH/FuuHyZJhCrAq\
ZtSoUdx222106dJF4VWVlXUJp6ZAtaRh1ThwtJa4nFSy0cBSig+4ytAFf6UYCrAqZNSoUYwYMYIu\
Xbowc+ZMhVdVVtYlnFoDjZOGNSa1QCot4CqirKNFqbIUYFXE6NGjGTFiBJ07d1Z4yYGrZJR2pNSL\
g0OuVwXXlQ9spOLBowv+SgkUYFXA6NGjGT58OJ07d2bWrFkKL/GUdaSUSsiVJXH09DUVP3rSBX+l\
BLHohSgVN2bMGIYPH06nTp0UXlJ+pV2nsLRejAmldbsvTw264K8UQwGWw+6++26GDRvGlVdeyezZ\
sxVekj6p3ogycUV6qNzRky74K8VQE2KOuueee7j11lu58sormTt3rsJL0ivV81KJo6cfobstS9op\
wHLQvffey9ChQxVekjnlOS/VGjgJhZeknZoQc8zYsWMZMmSIwksyS+elJAIUYDlk7NixDB48mI4d\
OzJnzhyFl2SWzktJyNSEmCPuu+++ovCaO3cuRxyh/5tIOelqFxIzCrAccN999zFo0CA6dOig8JKK\
0dUuJIYUYDEXDK958+YpvKRidLULiSEFWIzdf//9DBo0iCuuuELhJZWjq11IDGmPF1MPPPAAt9xy\
C5dffjnz589XeEnllLdXYSpX4RDJMO31YuiBBx5g4MCBXH755SxYsEDhJemRaq/CVK/CIZJhakKM\
mQcffJCBAwfSvn17hZeEQ+fLJCIUYDEybtw4br75Ztq3b89jjz2m8JJw6HyZRIT2gDExbtw4brrp\
Ji677DIdeUm4dBUOiQjtBWNg/Pjx3HTTTVx66aU89thjHHnkkWGXJFWdrsIhERCbJkTnXHPn3IfO\
uY+dc0PCridbxo8fz4033ki7du1YuHChwktExBeLAHPOHY53jYAWwBlAR+fcGeFWlXkTJkwoCq9F\
ixYpvEREAmIRYMC5wMdm9g8z2wMsAtqEXFNGTZw4kQEDBii8JH50TUXJkrgE2E+AjYHXn/nDctKk\
SZPo378/l1xyicJL4kXXVJQscmYWdg1lcs5dBjQ3sx7+687Ab8ysX9J0PYGeAHXq1MlbtGhR0biC\
ggJq1aqVvaJTUFxNy5YtY9KkSfzhD39gxIgRoYRXXLZV2KJYE4Rc10bg68DrHwEnRXNbRbEmCL+u\
Bg0avGVmZ4dWQHmYWeQfwG+BPwVeDwWGljZPXl6eBa1evdqiJrmmSZMmGWBt27a13bt3h1OUxWNb\
RUEUazILua4VZlbTvH+FNf3XFs1tFcWazMKvC3jTIrDfT+URl270fwV+7pw7Bfgc6ABcGW5J6TV5\
8mSuv/562rRpw+LFi6lWrVrYJYmUn34jJlkUiwAzs33OuX7An4DDgZlm9reQy0qbyZMn069fP9q0\
acPjjz+u8JJ402/EJEtiEWAAZvYM8EzYdaTbww8/rPASEamAuPRCzEnLly+nb9++tG7dWuElIlJO\
CrCQTJkyhQkTJnDxxRezZMkShZeISDkpwEIwdepU+vTpw+9+9zueeOIJhZeISAXE5hxYrpg6dSrX\
XXcdF110Eddff73CS0SkgnQElkXTpk3juuuuo1WrVjryEhGpJAVYljzyyCP07t2bVq1asXTpUqpX\
rx52SSIisaYAy4JHHnmEXr160bJlS4WXiEiaKMAybPr06QovEZEMUIBl0KOPPkrPnj1p0aIFS5cu\
pUaNGmGXJCKSMxRgGTJjxgyuvfZaWrRowbJlyxReIiJppgDLgBkzZtCjRw+aN2+u8BIRyRAFWJrN\
nDmTa6+9lubNm/Pkk08qvEREMkQBlkYzZ86kR48eNG3aVOElIpJhCrA0mTVrVlF4LV++XOElIpJh\
CrA0mDVrFtdccw1NmjRReImIZIkCrJJmz56t8BIRCYECrBLmzJlD9+7dady4McuXL+eoo44KuyQR\
kSpDAVZBc+bMoVu3bjRu3JgVK1YovEREskwBVgFz586lW7duNGrUSOElIhISBVg5zZs3j6uvvppG\
jRqRn5+v8BIRCYkCrBzmz59P165dadiwoY68RERCFvkAc87d55z7wDn3nnPuSefcD8KoY/78+XTp\
0oWGDRuSn59PzZo1wyhDRER8kQ8w4Hmgnpn9Avg7MDTbBSTCq0GDBgovEZGIiHyAmdlzZrbPf/ka\
UDeb6080GzZo0ICnnnpK4SUiEhGRD7Ak3YFns7WyBQsW0LVrVy688EKFl4hIxDgzC7sGnHOrgB8X\
M2qYma3wpxkGnA20sxKKds71BHoC1KlTJ2/RokVF4woKCqhVq1bKNa1atYq7776bX/7yl4wZMyYj\
V9gob03ZEsW6VFPqoliXakpd2HU1aNDgLTM7O7QCysPMIv8ArgZeBWqmOk9eXp4FrV692lK1YMEC\
O+yww6xBgwZWUFCQ8nzlVZ6asimKdamm1EWxLtWUurDrAt60COz3U3kcEWZ4psI51xwYBFxoZjsy\
vb6FCxfSuXNnLrjgAp566imOPvroTK9SREQqIA7nwB4CjgGed86tdc5NzeTK9uzZQ/369Xn66acV\
XiIiERb5IzAz+1k219e1a1c6d+7MYYfFIdtFRKou7aWLofASEYk+7alFRCSWItGNPhOcc98A/xsY\
9J/AtyGVU5Io1gTRrEs1pS6Kdamm1IVd10/N7Ichrj9lORtgyZxzb1rEftsQxZogmnWpptRFsS7V\
lLqo1hVFakIUEZFYUoCJiEgsVaUAeyTsAooRxZogmnWpptRFsS7VlLqo1hU5VeYcmIiI5JaqdAQm\
IiI5JKcCzDnX3jn3N+dcoXPu7KRxQ51zHzvnPnTONSth/lOcc6/70y12zlVLc32L/cthrXXOfeqc\
W1vCdJ865973p3sznTWUsL6RzrnPA7W1LGG65v72+9g5NyTDNaV0J+5sbKuy3rdzrrr/2X7sf39O\
zkQdgfWd5Jxb7Zxb73/f+xczTX3n3JbAZ3pbJmsKrLfUz8N5Jvrb6j3n3K8zXM9pgW2w1jm31Tk3\
IGmarGwr59xM59zXzrl1gWHHO+eed8595P89roR5u/rTfOSc65qJ+mIp7KsJp/MBnA6cBqwBzg4M\
PwN4F6gOnAJ8AhxezPyPAx3851OB6zJY6wPAbSWM+xT4zyxut5HAwDKmOdzfbqcC1fzteUYGa2oK\
HOE/vxe4N4xtlcr7BvoAU/3nHYDFGf68TgB+7T8/Bu9O5ck11QeeztZ3KNXPA2iJd08/B5wHvJ7F\
2g4HvsT7nVPWtxVwAfBrYF1g2FhgiP98SHHfc+B44B/+3+P858dl+7ON4iOnjsDMbIOZfVjMqDbA\
IjPbbWb/BD4Gzg1O4JxzQEPgCX/QHKBtJur013U5sDATy8+Qc4GPzewfZrYHWIS3XTPCQr4Td0Aq\
77sN3vcFvO9PI/8zzggz+8LM3vafbwM2AD/J1PrSrA0w1zyvAT9wzp2QpXU3Aj4xs/8tc8oMMLOX\
gE1Jg4PfnZL2Oc2A581sk5l9DzwPNM9UnXGSUwFWip8AGwOvP+PQf/D/AWwO7DSLmyZd/gB8ZWYf\
lTDegOecc2/5N+nMhn5+k87MEpoxUtmGmVLanbgzva1Sed9F0/jfny1436eM85srzwJeL2b0b51z\
7zrnnnXOnZmNeij78wjze9SBkv/TGMa2AqhjZl/4z78E6hQzTZjbLNIifzX6ZC6FuzeHKcX6OlL6\
0df5Zva5c+5HeLeR+cD/31tG6gKmAHfh7Xzuwmve7F6Z9VW2Jjv4Ttz7gAUlLCbt2younHO1gKXA\
ADPbmjT6bbymsgL/nOZy4OdZKCuSn4d/Prs1MLSY0WFtq4OYmTnn1C28HGIXYGbWuAKzfQ6cFHhd\
1x8W9B1ec8YR/v+ii5um0vU5544A2gF5pSzjc//v1865J/GasSq1E0h1uznnpgNPFzMqlW2Y1pqc\
c1cDFwGNzD8ZUMwy0r6tkqTyvhPTfOZ/vrXxvk8Z45w7Ei+8FpjZsuTxwUAzs2eccw875/7TzDJ6\
jb0UPo+0f49S1AJ428y+Sh4R1rbyfeWcO8HMvvCbUr8uZprP8c7TJdTFO89f5VWVJsR8oIPfW+wU\
vP9dvRGcwN9BrgYu8wd1BTJxRNcY+MDMPitupHPuaOfcMYnneJ0Z1hU3bboknYO4pIT1/RX4ufN6\
albDa47Jz2BNiTtxt7YS7sSdpW2VyvvOx/u+gPf9ebGkwE0H//zaDGCDmT1YwjQ/TpyHc86di/dv\
PdOhmsrnkQ908XsjngdsCTShZVKJrR5hbKuA4HenpH3On4Cmzrnj/Ob9pv4wCbsXSTofeDvfz4Dd\
wFfAnwLjhuH1JvsQaBEY/gxwov/8VLxg+xhYAlTPQI2zgd5Jw04EngnU8K7/+Btec1qmt9s84H3g\
Pbx/UCck1+W/bonX4+2TTNflfwYbgbX+Y2pyTdnaVsW9b+BOvHAFqOF/Xz72vz+nZnjbnI/X3Pte\
YPu0BHonvltAP3+bvIvXCeZ3WfgeFft5JNXlgMn+tnyfQG/hDNZ1NF4g1Q4My/q2wgvQL4C9/n7q\
GrxzpS8AHwGrgOP9ac8GHg3M293/fn0MdMv0NovLQ1fiEBGRWKoqTYgiIpJjFGAiIhJLCjAREYkl\
BZiIiMSSAkxERGJJASYiIrGkABMRkVhSgImISCwpwEREJJYUYCIiEksKMBERiSUFmIiIxJICTERE\
YkkBJiIisaQAExGRWFKAiYhILCnAREQklhRgIiISSwowERGJJQWYiIjE0v8HWuOM8KtxzIEAAAAA\
SUVORK5CYII=\
"
  frames[1] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAAxaklEQVR4nO3deZgU5bXH8e8RFESQxQUFjWtCFDWaUeO9GgNq0LiMuxFj3AVU\
DCQuuEYBUXFjNXHfE3EPY9SIxhnUXFFBcQE1Ai4MiiDCACIKcu4fVQ1lM0vPTHdX1czv8zz9THdt\
fbqq6cN537eqzN0RERFJm3XiDkBERKQhlMBERCSVlMBERCSVlMBERCSVlMBERCSVlMBERCSVlMBE\
RCSVlMBERCSVlMBERCSVlMBERCSVlMBERCSVlMBERCSVlMBERCSVlMBERCSVlMBERCSVlMBERCSV\
lMBERCSVlMBERCSVlMBSzMzczLYv0LZ/aWYf5GlbH5vZAXnaVsE+c1NkZtPMrEcBtnuKmb2c7+0W\
gr4zTZcSWEKY2cVm9kzWtA9rmHZ8Ht6v1qTi7i+5e7cGbPceM7uqcdEVVxpjrk51n8Pdu7t7RUwh\
NRtm9jMze87MvjKz2WZ2StwxNQdKYMnxIvC/ZtYCwMw2B9YFdsuatn24rDQjZtYy7hiSKPNvIwG2\
BG4GNgd+C9xqZl3iDakZcHc9EvAA1gOWASXh6+OAu4GJWdNmRNZxoB/wIbCI4B+QhfO2A14AFgBf\
An8DOoTz7gdWAd8AS4ELq4mnB1AZeT0ImAMsAT4A9q9mnT7ACuC7cLtPhtM/Bs4H3gaqgIeA1pH1\
DgWmhp/h/4BdatlPDvwBmBV+ruuBdSLzTwPeAxYCzwJbhdMNGAHMAxYD7wA71RRz1ntWu244rxVw\
A/Ap8AVwC7B+dB8Cl4Sxfgz8LrLdQ4A3w23OBq6MzNs6/Kynh9t+MZz+CDA33I8vAt1z2PcHhM+v\
BB4G7guP4zRg98h7/jyMZ0n4Pg8BV9VwHE4BXg4/+0LgI+A34bxjgSlZy/8JGB8+vyfcT8+F7zUx\
c5zC+T8N531F8F07LjLvHuCvwNPA18ABOWzPge3rsc9PDvf5l8ClkfnrABcBMwn+XT0MdKpm37QM\
j8HOcf+uNPVH7AHoETkYUA78MXw+luDHeFjWtLsiyzvwT6AD8CNgPnBQOG974NcEP7CbhD92IyPr\
rv5hqyGWHoQJDOgW/mPvEr7eGtiuhvXuyf7RC9/rNaAL0IkgwfQL5+1GkBh+AbQIfzw+BlrVsH0P\
91On8DP/FzgjnHc4MAPYIfwRuQz4v3DegcCUcF9ZuMzmNcWc9Z61rTsCKAvjaQc8CVwT2YcrgZvC\
4/Argh/dbpH5O4c/jLsQJMAjIvvYCZLNBqxJiqeF79MKGAlMzWHfRxPYcuDgcF9fA0wK560HfAIM\
IKj8jyJIhrUlsBXAmeG2zgI+C/dPK4Lks0Nk+TeBoyNxLgH2DZcdBbwcztuA4Lt2angMdyNIJDtG\
1q0C9g73W+vathf5zmxfj31+O7A+8DPg28znCPfNJGCL8H1uBR6sZt+MJvi+r1PdvtMjj7+ZcQeg\
R+RgBD8wT4TP3wJ+DByUNe3kyPIO7BN5/TBwUQ3bPgJ4M/J69Q9bDcv3YE0C254gyRwArFvHZ7gn\
+0cvfK8TI6+vA24Jn/8VGJq1/AfAr2rYvhMm6fD12cC/w+fPAKdH5q1DUNVuBexHkOz2yv5hqS7m\
rPnVrkvwY/01kWQO/A/wUWQfrgQ2yDpGl9fwPiOBEeHzzI/ptrXE1SFcpn0d+z6awJ6PzNsR+CZ8\
vi9BhW2R+S/XtF8IEli0NaBNGMtmkeM6LHzenaBKaxWJc1xk3bbA9wTNcL8FXsp6r1uBKyLr3lfN\
8at2e5HvzPb12OdbROa/BhwfPn+PSMsDQXPhCqBlZNqFBN/fzWr7d6JHfh7qA0uWF4F9zKwTsIm7\
f0jQpPa/4bSdWLv/a27k+TKCf7yYWWczG2dmc8xsMfAAsHFDgnL3GcBAgh/AeeF269u+X22cBMnl\
PDNblHkQ/JDVtv3ZkeefRJbdChgV2c5XBEmmq7u/QFDB3hx+htvMbMNcAq9l3U0IfrinRN7zX+H0\
jIXu/nV18ZrZL8ys3Mzmm1kVQXNw9jFa/VnNrIWZXWtmM8Nj+nE4qz7HNfs4tA7717oAczz8Fc5+\
77q25e7LwqeZ43ovcIKZGfB74GF3/7a6bbv7UoJj1YXgGP4i6/vwO2CzOuKqaXs/kOM+r+27+kQk\
rvcIEmXnyPIDgTPdPboNKRAlsGR5BWhP0CzzHwB3X0zQNHMm8Jm7f5Tjtq4m+N/kzu6+IXAiwY95\
hle7Vg3c/e/uvg/BP2IHhte0aH22S/DDM8zdO0Qebdz9wVrW2TLy/EcE+yezrb5Z21rf3f8v/Ayj\
3b2EoPL4CXBBrjHXsO6XBP2I3SPv197d20ZW7WhmG9QQ798Jmh+3dPf2BP040WOUHdsJBM2kBxB8\
T7YOp1s1y9bX50DXMOFkbFnTwnVx90kETZC/JIj7/qxFVm/bzNoSNMF+RnAMJ2Ydw7buflZ089W8\
ZU3by5bLPq/JbIJ+vmhsrd19TmSZzWt4XykAJbAEcfdvgMkEHd4vRWa9HE6rz+jDdgQdyVVm1pU1\
P9YZXwDb5rIhM+tmZvuZWSuCPpRvCAaBVCfn7YZuB/qF/zM2M9vAzA4xs3a1rHOBmXU0sy0J+iUe\
CqffAlxsZt3DuNub2bHh8z3C91iXoNlveeQz1BpzTeu6+6ow/hFmtmm4bFczOzBrE4PNbD0z+yXB\
gJVHwuntgK/cfbmZ7UnwQ1+bdgR9MgsIKr+rs+bXd99HvUJQTfQ3s5ZmdjiwZwO3lXEfQeW6wt2z\
zxk72Mz2MbP1gKEEfXGzCfp0f2JmvzezdcPHHma2Qx3vVdP2stV3n0fdAgwzs60AzGyTcD9FbU4w\
oEWKQAkseSYCmxIkrYyXwmn1SWCDCUaVVQFPAY9nzb8GuCxsDjm/jm21Aq4lqDjmhrFcXMOydwI7\
htv9R11BuvtkgupyLEE/yQyC/pXajCcYVDGV4LPdGW7rCYLKcFzYxPYu8JtwnQ0Jks1Cgma8BQQj\
GHOJubZ1B4UxTwrf83mCQS8Zc8P1PiMYCdrP3d8P550NDDGzJcCfCfrHanNf+P5zgOkEAwqi6rXv\
o9z9O4KBG6cTjAY9kSCZfFvLanW5n6DZ+4Fq5v0duIKgqa8kfD/cfQnQCzieYJ/NJTimrep4r2q3\
V4367vOoUQTV24Rw/UkEg4+iZhC0UkgRZIZci0ieWXAFjAfcfYuYQ2kQM3uVYLDN3Q1cf32CwT8/\
D/tzM9PvIRggdFme4szr9iQ9VIGJCABm9isz2yxsQjyZYJj5vxqxybOA16PJSySfEnV2v5ndRdBH\
MM/ddwqnXQ8cRtAhPBM41d0XxRakSNPVjaBJbQOCE8WPcffPG7IhM/uYYHDEEfkKTiRbopoQzWxf\
goEH90USWC/gBXdfaWbDAdx9UIxhiohIAiSqCdHdXyTohI1Om+DuK8OXmbPgRUSkmUtUAsvBaQRX\
WxARkWYuUX1gtTGzSwkuy/O3WpbpQ3BRU9Zff/2SLbdccx7mqlWrWGedZOXrJMYEyYxLMeUuiXE1\
JKZly5ZRWVnJpptuSocOHRIRUzHEHdd///vfL919k7qXTIC4r2WV/SC4usC7WdNOITjRsk2u2ykp\
KfGo8vJyT5okxuSezLgUU+6SGFd9Y1q1apXvs88+3rVrV1++fHkiYiqWuOMCJnsCckEuj8RXYGZ2\
EMEFMn/la663JiJN2AsvvMDLL7/M2LFjadWqrnOYpblKVP1sZg8SVFrdzKzSzE4nuEJDO+A5M5tq\
ZrfEGqSIFJS7c+WVV9K1a1dOP/30uMORBEtUBebuvauZfGfRAxGR2JSXl6+uvlq3bh13OJJgiarA\
RKR5y1RfXbp0UfUldUpUBSYizVt5eTkvvfQSY8aMUfUldVIFJiKJ4O4MHjyYLl26cMYZZ8QdjqSA\
KjARSYSKigpefPFFVV+SM1VgIpIImb4vVV+SK1VgIhK7TPU1evRoVV+SM1VgIhK7K6+8ks0335wz\
zzwz7lAkRVSBiUisKioqmDhxIqNGjVL1JfWiCkxEYjV48GBVX9IgqsBEJDYVFRVUVFQwatQo1l9/\
/bjDkZRRBSaxKiuD/v2Dv9L8DB48mM0220zVlzSIKjCJTVkZ9O4Ny5bB3XfDgw9CaWncUUmxTJw4\
kYqKCkaOHKnqSxpEFZjEZsKEIHlB8HfChHjjkeLKVF99+vSJOxRJKSUwiU2vXtCmTfC8TZvgtTQP\
EydOpLy8nEGDBqn6kgZTE6LEprQ0aDacMCFIXmo+bD4GDx5M586d6du3b9yhSIopgUmsSkuVuJqb\
F198kfLyckaMGKHqSxpFTYgiUlSqviRfVIGJSNG89NJLvPDCC9x0002qvqTRVIGJSNGo+pJ8UgUm\
IkXx8ssv8+9//5sbb7yRNpnhpyKNoApMRIpi8ODBbLrppvTr1y/uUKSJSFwCM7O7zGyemb0bmdbJ\
zJ4zsw/Dvx3jjFHyT5eUatreeecdnn/+eS688EJVX5I3iUtgwD3AQVnTLgL+7e4/Bv4dvpYmInNJ\
qZtvDv4qiTU99913n6ovybvEJTB3fxH4Kmvy4cC94fN7gSOKGZMUli4p1bT95z//YfLkyVxwwQVs\
sMEGcYcjTUjiElgNOrv75+HzuUDnOIOR/NIlpZq2wYMH06FDB84666y4Q5Emxtw97hjWYmZbA/90\
953C14vcvUNk/kJ3X6sfzMz6AH0AOnfuXDJu3LjV85YuXUrbtm0LHHn9JDEmiCeuqipYvBg23BDa\
t09GTHVJYkyQrLimTZtG//79OfXUUznppJPiDucHkrSfouKOq2fPnlPcfffYAqgPd0/cA9gaeDfy\
+gNg8/D55sAHdW2jpKTEo8rLyz1pkhiTezLjKlRM48e7n3NO8Le+krif3JMVV69evXzjjTf2p59+\
Ou5Q1pKk/RQVd1zAZE9AHsjlkZYmxDLg5PD5ycD4GGORJkKDRwrrlVdeYcKECVx44YW66oYUROIS\
mJk9CLwCdDOzSjM7HbgW+LWZfQgcEL6WlIt76LwGjxTW4MGD2XjjjTn77LPjDkWaqMRdicPde9cw\
a/+iBiIFlX035oEDg36wYt5WpVev4L2XLdPgkXx75ZVXePbZZxk+fLhGHkrBJC6BSfOQXf1cdx2s\
XBkklAcfLE4S0/3ICkfVlxRD4poQpXmIDp1v0SJIXlD8przSUhg7VskrnyZNmsSzzz7L+eefn8hR\
ftJ0KIFJQdXUz5Wpfs45BwYN0nlgTcngwYPZaKONOOecc+IORZo4NSFKwUT7ue64A/bfH/r2XVPt\
RO/G/ItfqCmvKXj11Vf517/+xbXXXqvqSwpOCUzyrqwsSEYffbSmn+vbb+Hpp6Giovo+rmgyk/RS\
9SXFpAQmeRWtutZbD1q1CpJXRqaPq1DJKpM8VckV36uvvsozzzzDNddco+pLikJ9YJJX0dGF330X\
NBsefHCQzKD6Pq58nQ+mE5PjpepLik0JTPIq+8K8ffvCU0/BI48EAzaymw/zmXR0YnJ8XnvtNZ55\
5hnOO+882rVrF3c40kwogUleRUcXRpNVTcPV85l0dFX7+AwePJhOnTrRv3//uEORZkR9YJJ39RmQ\
kc+rYejE5Hi8/vrrPP300wwbNkzVlxSVEpjEKt9JR6MZiy9TfZ177rlxhyLNjBKYxC476RRrJKFG\
LDbe66+/zlNPPaXqS2KhBCaxyk4iNZ38vOGG+X/f6MWEi3X9xaZmyJAh6vuS2GgQh8SmuhGI0UEd\
mZOfe/cOrlSfvW5jht5rxGLjTZ48mX/+85+cd955bJjv/2GI5EAJTGKTnUQuvRTat18zkjBj2TJY\
vHjN63wMvdeIxcZT9SVxUwKT2ESTCMC778LIkcG9wbJPfo7+Bz8f1VNNw/0lN1OmTOHJJ5/kT3/6\
k6oviY0SmDRYY5vxMklkp53WTFu2LGguzD75uX37Ncvkq3rSrVQabvDgwXTs2FEjDyVWGsQhDZKv\
QRCZdTLbiiak6OjEioofrqPzveLzxhtv8OSTT3LVVVep+pJYKYFJg9TUjNeQpNKQhKTzveKj6kuS\
Qk2I0iDZzXjt2zduYIWa89LhjTfeoKysTH1fkghKYNIg2YMgqqo0LL05GDJkiKovSYzUNCGa2R+B\
MwAH3gFOdffl8UbVvGU34+XrmoaSTG+++Sbjx49nyJAhtI+OqhGJSSoqMDPrCvwB2N3ddwJaAMfH\
G5VEaVh60zdkyBA6dOjAH/7wh7hDEQFSVIERxLq+ma0A2gCfxRyPZCnUwIqysqCJsqxMiTEub775\
Jv/4xz8YPHiwqi9JjFRUYO4+B7gB+BT4HKhyd/WyNAOZ4frz5ukuy3FS9SVJZO4edwx1MrOOwGPA\
b4FFwCPAo+7+QNZyfYA+AJ07dy4ZN27c6nlLly6lbdu2xQo5J0mMCZIV1+zZQfLaYoulVFa2ZdNN\
Ycst444qkKT9FJXvuGbMmMGZZ57JKaecwsknn5yImPIhiTFB/HH17NlzirvvHlsA9eHuiX8AxwJ3\
Rl6fBPyltnVKSko8qry83JMmiTG5Jyuu8ePd27Rxv+GGcm/TJnidyzrnnJPbso2RpP0Ule+4jjzy\
SG/fvr0vXLiwwdtI4r5KYkzu8ccFTPYE/O7n8khFEyJB0+FeZtbGzAzYH3gv5pikCDKDQzbdNLfB\
Ifm40K+s8dZbb/HEE08wcOBAOnToEHc4Ij+QigTm7q8CjwJvEAyhXwe4LdagpGhKS4Nmw1wGcOg2\
KfmVGTI/cODAuEMRWUsqEhiAu1/h7j91953c/ffu/m3cMUn9NObiv1VVua2r26Tkz1tvvcXjjz+u\
6ksSKzUJTNKtMU17ZWUwa1Zu6+p8tPwZMmQIG264IQMGDIg7FJFqKYFJUTSmaW/CBFi1Kvd1dV3F\
xotWXx07dow7HJFqKYFJUTSmaa9XL1hnnYatKw2Tqb7U9yVJlqYrcUiKNeYeXqWlMH580Cyo+38V\
3ttvv83jjz/OZZddpupLEk0JTIqmMZeaat8+aBbMKCvTDS0LJVN9/fGPf4w7FJFaqQlRUkfnehXO\
O++8w2OPPcaAAQPo1KlT3OGI1EoJTFKlrAwuvVTnehXKkCFDaNeunfq+JBXUhCipUVYGxx4L3323\
ZpoGdeTPu+++y6OPPspll12m6ktSQRWYpMatt/4wef3oRzrXK58y1Zf6viQtlMAktXbaSckrXzLV\
1x/+8AdVX5IaSmCSGn37QqtWwfNWrYLXkh9Dhw6lbdu2/OlPf4o7FJGcqQ9MUqO0FB5+WMPn823a\
tGk88sgjXHLJJaq+JFWUwCRVGnMumVQvU32p70vSRk2IIs3YtGnTePjhhzn33HPZaKON4g5HpF6U\
wESasaFDh7LBBhuo70tSSQlMpJmaPn26qi9JNSUwkWZK1ZeknRKYSDM0ffp0HnroIc4991w23njj\
uMMRaRAlMJFm6KqrrqJNmzaqviTVlMBEmpn33nuPcePGqfqS1FMCE2lmMtXXeeedF3coIo2SmgRm\
Zh3M7FEze9/M3jOz/4k7JpG0ef/993nwwQfp37+/qi9JvTRdiWMU8C93P8bM1gPaxB2QSNoMHTpU\
1Zc0GamowMysPbAvcCeAu3/n7otiDUokZaLV1yabbBJ3OCKNlooEBmwDzAfuNrM3zewOM9sg7qBE\
0mTo0KGsv/76qr6kyTB3jzuGOpnZ7sAkYG93f9XMRgGL3f3yrOX6AH0AOnfuXDJu3LjV85YuXUrb\
tm2LGHXdkhgTJDMuxZS76uL69NNPOfXUUznuuOPoG8N9aJK4r5IYE8QfV8+ePae4++6xBVAf7p74\
B7AZ8HHk9S+Bp2pbp6SkxKPKy8s9aZIYk3sy41JMuasurt/97nfepk0bnzdvXvED8mTuqyTG5B5/\
XMBkT8Dvfi6PVDQhuvtcYLaZdQsn7Q9MjzEkkdT44IMP1PclTVKaRiGeC/wtHIE4Czg15nhEUmHo\
0KG0bt1afV/S5KQmgbn7VCAd7bIiCZGpvs477zw23XTTuMMRyatUNCGKSMNcddVVtG7dmvPPPz/u\
UETyTglMpIn673//y9///nfOPvtsVV/SJCmBiTRRV111Fa1atVL1JU2WEphIE/Thhx/yt7/9jbPP\
PpvOnTvHHY5IQSiBiTRBmerrggsuiDsUkYJRAhNpYiorK3nggQc466yzVH1Jk6YEJtLE3H///aq+\
pFlQAhNpQmbMmMHzzz/PWWedxWabbRZ3OCIFpQQm0oRcddVVtGzZUtWXNAtKYCJNxIwZM3jggQco\
LS1V9SXNghKYSBMxbNgw1l13XXr37h13KCJFoQQm0gTMmDGD+++/n379+tGpU6e4wxEpCiUwkSYg\
U31deOGFcYciUjRKYCIpN3PmzNXV1+abbx53OCJFowQmknKqvqS5UgITSbGZM2dy33330bdvX1Vf\
0uwogYmk2NVXX03Lli1VfUmzpAQmklKzZs3i3nvvpW/fvnTp0iXucESKTglMJKWGDRtGy5YtGTRo\
UNyhiMRCCUwkhWbNmsV9991Hnz59VH1Js6UEJpJCV199NS1atFD1Jc1aqhKYmbUwszfN7J9xxyIS\
l48++oh7772XPn360LVr17jDEYlNqhIYMAB4L+4gROJ09dVXs84666j6kmYvNQnMzLYADgHuiDsW\
kbh8/PHH3HPPPaq+REhRAgNGAhcCq2KOQyQ2merroosuijsUkdiZu8cdQ53M7FDgYHc/28x6AOe7\
+6HVLNcH6APQuXPnknHjxq2et3TpUtq2bVucgHOUxJggmXEpJpg7dy4nnngihx12GAMGDEhMXLlQ\
TLmLO66ePXtOcffdYwugPtw98Q/gGqAS+BiYCywDHqhtnZKSEo8qLy/3pEliTO7JjEsxuZ955pm+\
3nrr+ezZs2tdTvsqN0mMyT3+uIDJnoDf/VweqWhCdPeL3X0Ld98aOB54wd1PjDkskaL55JNPuPvu\
uznzzDPZYost4g5HJBFSkcBEmjv1fYmsLXUJzN0rvJr+L5Gm6pNPPuGuu+7ijDPOSHb1VQb0D/+K\
FEHqEphIc3PNNdckv/oqA3oDN4d/y7LmzUaJTfJOCUwkwTLV1+mnn86WW24Zdzg1m0AwtIrw74Tw\
eSaxzWPtxCbSSEpgIgl2zTXXAHDxxRfHHEkdegFtwudtwtdQc2ITyQMlMJGE+vTTT9NRfQGUAg8C\
54R/S8PpNSW2DPWbSSO0jDsAEaleaqqvjFLWJK7otAeBKn6Y2GBN8+Iy4O5q5ovUQRWYSAJ9+umn\
3HnnnZx22mn86Ec/ijucxikFtmTt5KTmRWkkJTCRBLr22msBuOSSS2KOpIDqal4UqYOaEEUSZvbs\
2dxxxx1No/qqTaZ5cQJB8lLzodSTEphIwqSu7yujjCAZtSfo88okpbLwdRnV95EpcUkDKYGJJMjs\
2bO58847OfXUU9lqq63iDid30QEZGXcDAwluhDQE6EftAzUyCVDVmORIfWAiCXLttdeyatWq9PV9\
RQdkZCwjSEq5DNSo7UoeIjVQAhNJiMrKSu644466q68knjsVHZCR0YagksploIZGJEoDqAlRJCFy\
qr6Seu5UdEBGdh/YL6j+PLCoXgSfZxkakSg5UwITSYDKykpuv/12Tj31VLbeeuuaF6yuUklCAoOa\
B2SUAhVAjzrW1YhEqSclMJEEGD58eG59X025UtGIRKknJTCRmM2ZM4fbbruNU045pfbqC+KvVDRS\
UBJECUwkZvUeeRhXpZLU/jdptjQKUSRGc+bM4fbbb+fkk09mm222iTuc2mmkoCSMEphIjIYPH873\
33+fjvO+qrt2YT6G9CfxtABJBTUhisTks88+47bbbuPkk09m2223jTucumX3v0HdTYqZPrNf17BN\
NUtKI6gCE4nJ8OHDWblyZTqqr4xSYGz4t64mxejVNWZRfYWlZklphFQkMDPb0szKzWy6mU0zswFx\
xyTSGJ999hm33npreqqv6tR1O5RoclpF9clJt1SRRkhLE+JK4Dx3f8PM2gFTzOw5d58ed2AiDXHd\
ddexcuVKLr300rhDabi6hvRHz1lbh+qTU9ynBUiqpSKBufvnwOfh8yVm9h7QFVACk9T5/PPPufXW\
WznppJPSW31l1DakP5qctq1jOSUuaYBUNCFGmdnWwG7AqzGHItIgw4cPZ8WKFemuvnKV6TNrH3cg\
0hSZu8cdQ87MrC0wERjm7o9XM78P0Aegc+fOJePGjVs9b+nSpbRt27ZYoeYkiTFBMuNqKjEtWLCA\
E044gf32249BgwYlJq5CU0y5izuunj17TnH33WMLoD7cPRUPYF3gWeBPuSxfUlLiUeXl5Z40SYzJ\
PZlxNZWYBg4c6C1atPAZM2bkP6BQwffVeHc/J/yb47ymcvyKIe64gMmegN/8XB6paEI0MwPuBN5z\
95vijkekIT7//HNuueUWfv/737PddtvFHU7D1Hbjyei8o4Bm0EIq8UrFIA5gb+D3wDtmNjWcdom7\
Px1fSCL1c/3116er7ytzEnL0/l613c4lOu974DqCe4FpgIYUSCoSmLu/DFjccYg01Ny5c/nrX//K\
iSeeyPbbbx93OIHariwfvUJGxt3AQILztaq7nUsv4BaC5AXByS9Jul+ZNDmpaEIUSbvrrruOFStW\
cNlll8UdSqC2pkD4YTWVsYw1d1Y+h7Uv+1QKDGLNf4t1YrIUmBKYSIHNnTuXW265JVnVV12XcOoF\
rJc1bT3WVGuZy0llGwY8RvUJrjF0wV+phhKYSIFdf/31fPvtt8nq+6rrEk6lwAFZ0w4gt4RUW4Jr\
iLqqRWm2lMBECuiLL75Y3ff14x//OO5w1shcJaO2SqkvP0xyfRv4XmXAbBqeeHTBX6mBEphIAWWq\
r8T0fUXVVSnlkuTqkqme5tHw6kkX/JUapGIUokgaffHFF/zlL3/hhBNOSFb1VR+1XaewtlGMGbUN\
u69PDLrgr1RDCUykQDLV1+WXXx53KPmX640oM1ekh8ZVT7rgr1RDTYgiBTBv3rzV1ddPfvKTuMPJ\
v1z7pTLV06bobsuSd0pgIgWQ6L6vfKhPv1QpsCVKXpJ3akIUybNo9dWtW7e4wykM9UtJAiiBieTZ\
DTfcwPLly5tu9ZWhfimJmZoQRfJo/vz53HzzzfTu3Tt91ZeudiEpowQmkkeprb50tQtJISUwkTyZ\
P38+Y8eO5fjjj+enP/1p3OHUj652ISmkBCaSJzfccAPffPNNOs/70tUuJIU0iEMkD6J9X6mrvqD+\
owpzuQqHSIEpgYnkwY033siyZcvS1/cVleuowlyvwiFSYGpCFGmkL7/8cnXf1w477BB3OIWn/jJJ\
CCUwkUbKVF+p7PtqCPWXSUKoCVGkEb788kvGjBnDb3/72+ZRfYGuwiGJoQQm0gjNrvrK0FU4JAFS\
04RoZgeZ2QdmNsPMLoo7HpGqqirGjh3Lcccdx4477hh3OCLNTioSmJm1ILhGwG+AHYHeZqZfDInV\
ww8/zNdff938qi+RhEhFAgP2BGa4+yx3/w4YBxwec0zSjC1YsIAnnniC4447ju7du8cdTrLomopS\
JGlJYF2B2ZHXleE0kVjcdNNNLF++XNVXNl1TUYrI3D3uGOpkZscAB7n7GeHr3wO/cPf+Wcv1AfoA\
dO7cuWTcuHGr5y1dupS2bdsWL+gcJDEmSGZcSYqpqqqK3r17U1JSwtChQ+MOZy2x7qvZwLzI602B\
LZN1/DKSGBPEH1fPnj2nuPvusQVQH+6e+AfwP8CzkdcXAxfXtk5JSYlHlZeXe9IkMSb3ZMaVpJgu\
ueQSNzO/66674g6lWrHuq/Hu3saDf4VtwteerOOXkcSY3OOPC5jsCfjdz+WRlmH0rwM/NrNtgDnA\
8cAJ8YYkzdGCBQsYM2YMxx57LNtss03c4SSPzhGTIkpFAnP3lWbWH3gWaAHc5e7TYg5LmqERI0aw\
dOlSLr/8cr788su4w0kmnSMmRZKWQRy4+9Pu/hN3387dh8UdjzQ/X331FaNHj+bYY49lp512ijsc\
kWYvNQlMJG4jRoxgyZIlGnkokhBKYCI5+Oqrrxg1apSqL5EEUQITycHIkSNVfYkkjBKYSB0y1dcx\
xxzDzjvvHHc4IhJSAhOpw8iRI1m8eDF//vOf4w5FRCKUwERqsXDhQkaNGsXRRx+t6kskYZTARGqh\
6kskuZTARGqwcOFCRo4cydFHH80uu+wSdzgikkUJTKQGo0aNUvUlkmCpuJRUvqxYsYLKykqWL18e\
dygAtG/fnvfeey/uMNZS7Lhat27NFltswbrrrlu096zLokWLGDlyJEcddZSqL5GEalYJrLKyknbt\
2rH11ltjZnGHw5IlS2jXrl3cYaylmHG5OwsWLKCysjJRF8cdNWoUVVVVqr5EEqxZNSEuX76cjTba\
KBHJSwJmxkYbbZSYqhiC6mvEiBEceeSR/OxnP4s7HBGpQbNKYICSVwIl7Zio+hJJh2aXwOKWfafV\
e+65h/79+9ewdOFtvfXWBb0tyHPPPUdJSQk777wzJSUlvPDCCwV7r3zI9H0deeSR7LrrrnGHIyK1\
aFZ9YM3RypUradkyvsO88cYb8+STT9KlSxfeffddDjzwQObMmRNbPHUZPXo0ixYtUvUlkgKqwBJi\
yZIlbLPNNqxYsQKAxYsXr37do0cPBgwYwK677spOO+3Ea6+9BsDXX3/Naaedxp577sluu+3G+PHj\
gaCqKy0tZb/99mP//fenoqKCfffdl0MOOYRu3brRr18/Vq1atVYMRxxxBCUlJey5557cdtttq6e3\
bduWSy+9lJ/97GfstddefPHFFwDMnz+fo48+mj322IM99tiD//znP2ttc7fddqNLly4AdO/enW++\
+YZvv/02vzsvT6qqqhgxYgRHHHGEqi+RFFACK7JvvvmGXXfdlV133ZW999579f/027VrR48ePXjq\
qacAGDduHEcdddTqoeXLli1j6tSp/OUvf+G0004DYNiwYey333689tprlJeXc8EFF/D1118D8MYb\
b/Doo48yceJEAF577TXGjBnD9OnTmTlzJo8//vhasd11111MmTKFiRMnMnr0aBYsWAAEiXKvvfbi\
rbfeYt999+X2228HYMCAAfzxj3/k9ddf57HHHuOMM86o9bM/9thj/PznP6dVq1aN3Y0FoepLJF2a\
bRPiwIEDmTp1al63ueuuuzJy5Mhal1l//fVXv++SJUt47LHHmDx5MgBnnHEG1113HUcccQR33333\
6kQB0Lt3bwD23XdfFi9ezKJFi5gwYQJlZWXccMMNQDDK8tNPPwXg17/+NZ06dVq9/p577sm22267\
elsvv/wyxxxzzA9iGz16NE888QSrVq1i9uzZfPjhh2y00Uast956HHrooQCUlJTw3HPPAfD8888z\
ffr01esvXryYpUuXrtXPBzBt2jQGDRrEhAkTat+JMamqquKmm27i8MMPZ7fddos7HBHJQbNNYEm0\
99578/HHH1NRUcH333//gxsnZo/UMzPcnccee4xu3br9YN6rr77KBhtssNbytb2uqKjg+eef55VX\
XuH777/nsMMOWz20fd111129fIsWLVi5ciUAq1atYtKkSbRu3brWz1VZWcmRRx7Jfffdx3bbbVfX\
bohFpvq64oor4g5FRHLUbBNYXZVSXE466SROOOGEtW6c+NBDD9GzZ09efvll2rdvT/v27TnwwAMZ\
M2YMY8aMwcx48803a6weXnvtNT766CO22morHnroIfr06fOD+VVVVXTs2JE2bdowZcoUJk2aVGes\
vXr1YsyYMVxwwQUATJ06da2+o0WLFnHIIYdw7bXXsvfee9djTxRPpu+rtLRU1ZdIiqgPLGF+97vf\
sXDhwtVNhhmtW7dmt912o1+/ftx5550AXH755axYsYJddtmF7t2713q34D322IP+/fuzww47sM02\
23DkkUf+YP5BBx3EypUr2WGHHbjiiivYa6+96ox19OjRTJ48mV122YUdd9yRW265Za1lxo4dy4wZ\
MxgyZMjqvr958+blsiuKZsyYMSxcuFDVl0jauHuiH8D1wPvA28ATQIdc1ispKfGo8vJynz59uifJ\
4sWL15r2yCOP+IknnviDab/61a/89ddfb/D7lJeX+yGHHNKouAqtrmNTXl5ekPetqqryjh07emlp\
ab3XLVRMjZXEuBRT7uKOC5jsCfjtz+WRhibE54CL3X2lmQ0HLgYGxRxTQZx77rk888wzPP3003GH\
0mxkqi+NPBRJn8QnMHePDlubBBxT07JpN2bMmGqnV1RUNGq7PXr0oEePHo3aRlO0ePFibrzxRg47\
7DBKSkriDkdE6iltfWCnAc/EHYQ0Der7Ekk3C5o8Yw7C7Hlgs2pmXeru48NlLgV2B47yGoI2sz5A\
H4DOnTuXjBs3bvW8pUuX0rVrV7bbbrvEXDz2+++/p0WLFnGHsZZix+XuzJw5k6qqqhqXqen8sob6\
+uuvOeGEE+jevTtXX311g7aR75jyJYlxKabcxR1Xz549p7j77rEFUB9xd8Ll8gBOAV4B2uS6TnWD\
OGbNmuXz58/3VatWrd1zGYM4BkvkophxrVq1yufPn++zZs2qdbl8d2wPGzbMgUYPjkmiJMalmHIX\
d1xoEEf+mNlBwIXAr9x9WWO2tcUWW1BZWcn8+fPzE1wjLV++vM6TgONQ7Lgyd2QuliVLlnDjjTdy\
6KGHsvvu6fiPpoisLfEJDBgLtAKeC5v+Jrl7v4ZsaN11103UXX8rKioSeeJsUuPKl7Fjx/LVV1+p\
70sk5RKfwNx9+7hjkKZjyZIl3HDDDRxyyCGqvkRSLm2jEEUa5eabb1b1JdJEKIFJs5Gpvn7zm9+w\
xx57xB2OiDRSIobRF4KZzQc+iUzaGPgypnBqksSYIJlxKabcJTEuxZS7uOPayt03ifH9c9ZkE1g2\
M5vsCTu3IYkxQTLjUky5S2Jciil3SY0ridSEKCIiqaQEJiIiqdScEthtcQdQjSTGBMmMSzHlLolx\
KabcJTWuxGk2fWAiItK0NKcKTEREmpAmlcDM7Fgzm2Zmq8xs96x5F5vZDDP7wMwOrGH9bczs1XC5\
h8xsvTzH95CZTQ0fH5vZ1BqW+9jM3gmXm5zPGGp4vyvNbE4ktoNrWO6gcP/NMLOLChzT9Wb2vpm9\
bWZPmFmHGpYr+L6q63ObWavw2M4Ivz9bFyKOyPttaWblZjY9/L4PqGaZHmZWFTmmRbljZ13HwwKj\
w331tpn9vMDxdIvsg6lmttjMBmYtU5R9ZWZ3mdk8M3s3Mq2TmT1nZh+GfzvWsO7J4TIfmtnJhYgv\
leK+mnA+H8AOQDegAtg9Mn1H4C2CaypuA8wEWlSz/sPA8eHzW4CzChjrjcCfa5j3MbBxEffblcD5\
dSzTItxv2wLrhftzxwLG1AtoGT4fDgyPY1/l8rmBs4FbwufHAw8V+HhtDvw8fN4O+G81MfUA/lms\
71CuxwM4mOCefgbsBbxaxNhaAHMJznMq+r4C9gV+DrwbmXYdcFH4/KLqvudAJ2BW+Ldj+LxjsY9t\
Eh9NqgJz9/fc/YNqZh0OjHP3b939I2AGsGd0AQuuFLwf8Gg46V7giELEGb7XccCDhdh+gewJzHD3\
We7+HTCOYL8WhLtPcPeV4ctJQPEuV/9DuXzuwwm+LxB8f/a3At50zt0/d/c3wudLgPeAroV6vzw7\
HLjPA5OADma2eZHee39gprt/UueSBeDuLwJfZU2Ofndq+s05EHjO3b9y94XAc8BBhYozTZpUAqtF\
V2B25HUla/+D3whYFPnRrG6ZfPkl8IW7f1jDfAcmmNmU8CadxdA/bNK5q4ZmjFz2YaHUdifuQu+r\
XD736mXC708Vwfep4MLmyt2AV6uZ/T9m9paZPWNm3YsRD3Ufjzi/R8dT838a49hXAJ3d/fPw+Vyg\
czXLxLnPEi3xV6PPZjncvTlOOcbXm9qrr33cfY6ZbUpwG5n3w/+9FSQu4K/AUIIfn6EEzZunNeb9\
GhuT//BO3CuBv9Wwmbzvq7Qws7bAY8BAd1+cNfsNgqaypWGf5j+AHxchrEQej7A/uxS4uJrZce2r\
H3B3NzMNC6+H1CUwdz+gAavNAbaMvN4inBa1gKA5o2X4v+jqlml0fGbWEjgKKKllG3PCv/PM7AmC\
ZqxG/Qjkut/M7Hbgn9XMymUf5jUmMzsFOBTY38POgGq2kfd9lSWXz51ZpjI8vu0Jvk8FY2brEiSv\
v7n749nzownN3Z82s7+Y2cbuXtBr7OVwPPL+PcrRb4A33P2L7Blx7avQF2a2ubt/HjalzqtmmTkE\
/XQZWxD08zd7zaUJsQw4Phwttg3B/65eiy4Q/kCWA8eEk04GClHRHQC87+6V1c00sw3MrF3mOcFg\
hnerWzZfsvogjqzh/V4HfmzBSM31CJpjygoYU+ZO3KVew524i7SvcvncZQTfFwi+Py/UlHDzIexf\
uxN4z91vqmGZzTL9cGa2J8G/9UIn1VyORxlwUjgacS+gKtKEVkg1tnrEsa8iot+dmn5zngV6mVnH\
sHm/VzhN4h5Fks8HwY9vJfAt8AXwbGTepQSjyT4AfhOZ/jTQJXy+LUFimwE8ArQqQIz3AP2ypnUB\
no7E8Fb4mEbQnFbo/XY/8A7wNsE/qM2z4wpfH0ww4m1moeMKj8FsYGr4uCU7pmLtq+o+NzCEILkC\
tA6/LzPC78+2Bd43+xA0974d2T8HA/0y3y2gf7hP3iIYBPO/RfgeVXs8suIy4OZwX75DZLRwAePa\
gCAhtY9MK/q+IkignwMrwt+p0wn6Sv8NfAg8D3QKl90duCOy7mnh92sGcGqh91laHroSh4iIpFJz\
aUIUEZEmRglMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlM\
RERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERSSQlMRERS\
SQlMRERS6f8BmlN57epT9x4AAAAASUVORK5CYII=\
"
  frames[2] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAA4+ElEQVR4nO3dd3gU5drH8e8j0kIJWEAEBDx6kCKgEfU9R4GEEJpEQFBREUQO\
oKCCDRXkeOyKBQFFRQFFJXQJRUqSRcUjIKB4kKI0BRtIj7RA7vePmeAQUjZkNzOzuT/XtRe70/a3\
s5u5mWeemTEiglJKKeU3Z7gdQCmllDodWsCUUkr5khYwpZRSvqQFTCmllC9pAVNKKeVLWsCUUkr5\
khYwpZRSvqQFTCmllC9pAVNKKeVLWsCUUkr5khYwpZRSvqQFTCmllC9pAVNKKeVLWsCUUkr5khYw\
pZRSvqQFTCmllC9pAVNKKeVLWsCUUkr5khYwHzPGiDHmojAt+1pjzIYQLWurMSY+RMsK22eORMaY\
74wxLcKw3J7GmCWhXm446G8mcmkB8whjzKPGmE+yDfshl2E3h+D98iwqIvK5iNQ9jeVOMMY8Xbh0\
RcuPmXOS0+cQkQYistilSMWGMaaxMWaRMWa3MWabMaan25mKAy1g3vEZ8A9jTAkAY0w1oCRwWbZh\
F9nTqmLEGHOm2xm8KOtvwwNqAq8D1YCbgLeMMee7G6kYEBF9eOABlAIOAjH26xuB8cCn2YZtdMwj\
QD/gB2Av1h+Qscf9DUgDdgF/AB8ClexxE4FM4BCQDjycQ54WwHbH68HAz8ABYAPQMod5+gAZwFF7\
ubPt4VuBB4FvgX3AZKCMY77rgG/sz/BfoFEe60mAe4HN9ucaDpzhGN8LWAfsARYAtezhBngV2AHs\
B/4HNMwtc7b3zHFee1xp4CXgJ+B34E2grHMdAo/ZWbcCtzqW2x742l7mNuAJx7ja9me90172Z/bw\
qcBv9nr8DGgQxLqPt58/AUwB3re/x++AKxzvebmd54D9PpOBp3P5HnoCS+zPvgfYArS1x3UFVmab\
/n5glv18gr2eFtnv9WnW92SPv8Qetxvrt3ajY9wEYAwwD/gTiA9ieQJcVIB13sNe538AQxzjzwAe\
ATZh/V1NAc7KYd2caX8Hl7q9XYn0h+sB9OH4MiAADLKfj8baGD+Tbdg4x/QCzAEqARcAO4E29riL\
gFZYG9hz7Y3dCMe8JzZsuWRpgV3AgLr2H/v59uvawN9ymW9C9o2e/V7LgfOBs7AKTD973GVYheEq\
oIS98dgKlM5l+WKvp7Psz/w90Nsedz2wEahnb0SGAv+1x7UGVtrrytjTVMstc7b3zGveV4FkO08F\
YDbwnGMdHgNesb+H5lgb3bqO8ZfaG8ZGWAWwo2MdC1axKcdfRbGX/T6lgRHAN0Gse2cBOwy0s9f1\
c8BSe1wp4EfgPqw9/85YxTCvApYB/Mte1l3AL/b6KY1VfOo5pv8auMGR8wDQzJ72NWCJPa4c1m/t\
Dvs7vAyrkNR3zLsP+Ke93srktTzHb+aiAqzzsUBZoDFwJOtz2OtmKVDDfp+3gEk5rJuRWL/3M3Ja\
d/oI4TbT7QD6cHwZ1gZmpv18NXAx0CbbsB6O6QW4xvF6CvBILsvuCHzteH1iw5bL9C34q4BdhFVk\
4oGS+XyGCdk3evZ73eZ4/SLwpv18DPBUtuk3AM1zWb5gF2n79d1Aqv38E+BOx7gzsPZqawFxWMXu\
6uwblpwyZxuf47xYG+s/cRRz4P+ALY51eAwol+07ejyX9xkBvGo/z9qYXphHrkr2NNH5rHtnAUtx\
jKsPHLKfN8PawzaO8UtyWy9YBczZGhBlZznP8b0+Yz9vgLWXVtqRM8kxb3ngOFYz3E3A59ne6y3g\
345538/h+8txeY7fzEUFWOc1HOOXAzfbz9fhaHnAai7MAM50DHsY6/d7Xl5/J/oIzUOPgXnLZ8A1\
xpizgHNF5AesJrV/2MMacurxr98czw9i/fFijKlqjEkyxvxsjNkPfACcczqhRGQjMBBrA7jDXm5B\
2/dzzIlVXB4wxuzNemBtyPJa/jbH8x8d09YCXnMsZzdWkakuImlYe7Cv25/hbWNMxWCC5zHvuVgb\
7pWO95xvD8+yR0T+zCmvMeYqY0zAGLPTGLMPqzk4+3d04rMaY0oYY543xmyyv9Ot9qiCfK/Zv4cy\
9vG184Gfxd4KZ3/v/JYlIgftp1nf63vALcYYA3QHpojIkZyWLSLpWN/V+Vjf4VXZfg+3Auflkyu3\
5Z0kyHWe1291piPXOqxCWdUx/UDgXyLiXIYKEy1g3vIlEI3VLPMFgIjsx2qa+Rfwi4hsCXJZz2L9\
b/JSEakI3Ia1Mc8iOc6VCxH5SESuwfojFuCF3CYtyHKxNjzPiEglxyNKRCblMU9Nx/MLsNZP1rL6\
ZltWWRH5r/0ZRopIDNaex9+Bh4LNnMu8f2AdR2zgeL9oESnvmLWyMaZcLnk/wmp+rCki0VjHcZzf\
UfZst2A1k8Zj/U5q28NNDtMW1K9AdbvgZKmZ28T5EZGlWE2Q12LlnphtkhPLNsaUx2qC/QXrO/w0\
23dYXkTuci4+h7fMbXnZBbPOc7MN6zifM1sZEfnZMU21XN5XhYEWMA8RkUPACqwD3p87Ri2xhxWk\
92EFrAPJ+4wx1flrY53ld+DCYBZkjKlrjIkzxpTGOoZyCKsTSE6CXq5tLNDP/p+xMcaUM8a0N8ZU\
yGOeh4wxlY0xNbGOS0y2h78JPGqMaWDnjjbGdLWfN7XfoyRWs99hx2fIM3Nu84pIpp3/VWNMFXva\
6saY1tkW8R9jTCljzLVYHVam2sMrALtF5LAx5kqsDX1eKmAdk9mFtef3bLbxBV33Tl9i7U0MMMac\
aYy5HrjyNJeV5X2sPdcMEcl+zlg7Y8w1xphSwFNYx+K2YR3T/bsxprsxpqT9aGqMqZfPe+W2vOwK\
us6d3gSeMcbUAjDGnGuvJ6dqWB1aVBHQAuY9nwJVsIpWls/tYQUpYP/B6lW2D5gLzMg2/jlgqN0c\
8mA+yyoNPI+1x/GbneXRXKZ9F6hvL/fj/EKKyAqsvcvRWMdJNmIdX8nLLKxOFd9gfbZ37WXNxNoz\
TLKb2NYAbe15KmIVmz1YzXi7sHowBpM5r3kH25mX2u+ZgtXpJctv9ny/YPUE7Sci6+1xdwNPGmMO\
AMOwjo/l5X37/X8G1mJ1KHAq0Lp3EpGjWB037sTqDXobVjE5ksds+ZmI1ez9QQ7jPgL+jdXUF2O/\
HyJyAEgAbsZaZ79hfael83mvHJeXg4Kuc6fXsPbeFtrzL8XqfOS0EauVQhWBrC7XSqkQM9YVMD4Q\
kRouRzktxphlWJ1txp/m/GWxOv9cbh/PzRo+AauD0NAQ5Qzp8pR/6B6YUgoAY0xzY8x5dhNiD6xu\
5vMLsci7gK+cxUupUPLU2f3GmHFYxwh2iEhDe9hwoAPWAeFNwB0iste1kEpFrrpYTWrlsE4U7yIi\
v57OgowxW7E6R3QMVTilsvNUE6IxphlWx4P3HQUsAUgTkWPGmBcARGSwizGVUkp5gKeaEEXkM6yD\
sM5hC0XkmP0y6yx4pZRSxZynClgQemFdbUEppVQx56ljYHkxxgzBuizPh3lM0wfroqaULVs2pmbN\
v87DzMzM5IwzvFWvvZgJvJlLMwXPi7mKItOePXvYuXMnFSpUoFq1ap7IdDrczvX999//ISLn5j+l\
B7h9LavsD6yrC6zJNqwn1omWUcEuJyYmRpwCgYB4jRcziXgzl2YKnhdzhTvTa6+9JoB07dpVMjIy\
PJHpdLmdC1ghHqgFwTw8vwdmjGmDdYHM5vLX9daUUgqAN954g/vuu49OnTrx4YcfcuaZnt+sqRDx\
1P6zMWYS1p5WXWPMdmPMnVhXaKgALDLGfGOMedPVkEopz3j77bfp378/HTp0ICkpiZIlS7odSRUh\
T/1XRUS65TD43SIPopTyvHHjxtG3b1/atWvH1KlTKVWqlNuRVBHz1B6YUkoF47333qN3794kJCQw\
ffp0SpfO71KJKhJpAVNK+cqHH37IHXfcQVxcHB9//DFlypRxO5JyiRYwpZRvTJ48mdtvv53mzZuT\
nJxM2bJl3Y6kXKQFTCnlC9OmTePWW2/ln//8J3PmzCEqKsrtSMplWsCUUp738ccf061bN6666irm\
zp1LuXLl8p9JRTwtYEopT5s9ezY33ngjMTExfPLJJ1SokNfNulVxogVMKeVZ8+bNo0uXLjRp0oQF\
CxZQsWJFtyMpD9ECppTypIULF9K5c2caNmzIggULiI6OdjuS8hgtYEopz0lNTeX666/nkksuYeHC\
hVSuXNntSMqDtIAppTxl8eLFdOjQgYsvvpiUlBTOPvtstyMpj9ICplyVnAwDBlj/KvX5559z3XXX\
UadOHVJSUjjnnHPcjqQ8TAuYck1yMnTrBq+/bv2rRax4++9//0u7du2oUaMGqampVKlSxe1IyuO0\
gCnXLFwIB+0b5Bw8aL1WxdOyZcto06YN1apVIy0tjfPOO8/tSMoHtIAp1yQkQNbFFKKirNeq+Fmx\
YgWtW7emSpUqBAIBzj//fLcjKZ/w1O1UVPGSmAiTJll7XgkJ1mtVvKxatYpWrVpRuXJl0tLSqF69\
utuRlI9oAVOuSkzUwlVcrV69mlatWlGxYkUCgQAXXHCB25GUz2gTolKqyK1Zs4b4+HiioqIIBALU\
rl3b7UjKh7SAKaWK1Nq1a4mLi6NUqVIEAgEuvPBCtyMpn9ICppQqMuvXrycuLo4SJUqQlpbGRRdd\
5HYk5WN6DEwpVSR++OEH4uLiEBECgQB169Z1O5LyOd0DU0qF3c8//0xsbCwZGRmkpaVRr149tyOp\
COC5AmaMGWeM2WGMWeMYdpYxZpEx5gf7X72yZ4TRS0pFri1btnD//fdz+PBhUlNTadCggduRVITw\
XAEDJgBtsg17BEgVkYuBVPu1ihB6SanI9dNPPxEXF8ehQ4dISUmhUaNGbkdSEcRzBUxEPgN2Zxt8\
PfCe/fw9oGNRZlLhpZeUikzbt28nNjaWPXv2MHz4cJo0aeJ2JBVhPFfAclFVRH61n/8GVHUzjAot\
vaRU5Pnll1+IjY3ljz/+YOHChdphQ4WFERG3M5zCGFMbmCMiDe3Xe0WkkmP8HhE55TiYMaYP0Aeg\
atWqMUlJSSfGpaenU758+TAnLxgvZgJ3cu3bB/v3Q8WKkNONd724rryYCdzPtXv3bgYOHMgff/zB\
8OHDadCggeuZcuLFTOB+rtjY2JUicoVrAQpCRDz3AGoDaxyvNwDV7OfVgA35LSMmJkacAoGAeI0X\
M4l4M1e4Ms2aJdK/v/VvQXlxPYm4m+u3336TevXqSbly5eTzzz/3RKbceDGTiPu5gBXigToQzMMv\
TYjJQA/7eQ9glotZVITQziOhtXPnTuLj49m6dStz587lmmuucTuSinCeK2DGmEnAl0BdY8x2Y8yd\
wPNAK2PMD0C8/Vr5nNtd57XzSOjs2rWL+Ph4Nm7cyJw5c2jevLnbkVQx4LkrcYhIt1xGtSzSICqs\
svZ+Dh6E8eNh4EDrOFhR3lYlIcF674MHtfNIYezZs4dWrVqxYcMGZs+eTVxcnNuRVDHhuQKmiofs\
ez8vvgjHjlkFZdKkoiliej+ywtu7dy8JCQl89913zJo1i1atWrkdSRUjnmtCVMWDs+t8iRJW8YKi\
b8pLTITRo7V4nY59+/bRunVrVq9ezYwZM2jTJvv1B5QKLy1gKqxyO86VtffTvz8MHqzngfnNgQMH\
aNeuHatWrWLatGm0b9/e7UiqGNImRBU2zuNc77wDLVtC375/7e0478Z81VXalOcX6enptGvXjmXL\
ljFlyhQS9QtTLtECpkIuOdkqRlu2/HWc68gRmDcPFi/O+RiXs5gp7zp48CAdOnTgyy+/ZNKkSXTu\
3NntSKoY0wKmQsq511WqFJQubRWvLFnHuMJVrLKKp+7Jhd6hQ4dITEzks88+44MPPqBr165uR1LF\
nB4DUyHl7F149KjVbNiunVXMIOdjXKE6H0xPTA6fw4cP07FjR9LS0pgwYQLduuV2totSRUcLmAqp\
7Bfm7dsX5s6FqVOtDhvZmw9DWXT0xOTwOHLkCJ07d2bRokWMGzeO7t27ux1JKUALmAoxZ+9CZ7HK\
rbt6KIuOXtU+9I4ePUrXrl355JNPePvtt+nZs6fbkZQ6QY+BqZArSIeMUF4NQ09MDq2MjAxuuukm\
Zs+ezZgxY+jdu7fbkZQ6iRYw5apQFx3tzRgaGRkZdOvWjY8//pjRo0fTr18/tyMpdQotYMp12YtO\
UfUk1B6LOTt27Bjdu3dn+vTpvPrqq/Tv39/tSErlSAuYclX2IpLbyc8VK4b+fZ0XEy6q6y963fHj\
x+nZsyeTJ09m+PDhDBw40O1ISuVKO3Eo1+TUA9HZqSPr5Odu3awr1WeftzBd77XH4qmOHz9Or169\
+PDDD3nuued48MEH3Y6kVJ60gCnXZC8iQ4ZAdPRfPQmzHDwI+/f/9ToUXe+1x+LJMjMz6dOnD++/\
/z5PPfUUjzzyiNuRlMqXFjDlGmcRAVizBkaMsO4Nlv3kZ2cTYij2nnLr7l8cZWZm0q9fP8aNG8ew\
YcMYOnSo25GUCooWMHXaCtuMl1VEGjb8a9jBg1ZzYfaTn6Oj/5omVHtPeisVEBEGDBjA2LFjeeyx\
x3jiiSfcjqRU0LQThzotoeoEkTVP1rKcBcnZO3Hx4pPn0fO9Ck9EGDhwIGPGjOHhhx/m6aefxhjj\
diylgqYFTJ2W3JrxTqeonE5B0vO9CkdEeOCBBxg5ciSDBg3i+eef1+KlfEcLmDot2a+gER1duD0y\
LUhFR0QYPHgwr776Kvfeey8vv/yyFi/lS3oMTJ2W7J0g9u3Tbul+ICIMHTqU4cOHc9dddzFixAgt\
Xsq3fLMHZowZBPQGBPgfcIeIHHY3VfGWfa8pVNc0VOHzxBNP8Oyzz9KnTx9Gjx6txUv5mi/2wIwx\
1YF7gStEpCFQArjZ3VTKSbule9+TTz7Jk08+Sa9evRgzZgxnnOGLP3+lcuWbPTCsrGWNMRlAFPCL\
y3lUNuE6jpWcbDVRJidrYTxdzz33HP/+97+5/fbbGTt2rBYvFRF88SsWkZ+Bl4CfgF+BfSKiR1mK\
gazu+jt26F2WT9fw4cN57LHHuOWWWxg3bpwWLxUxjIi4nSFfxpjKwHTgJmAvMBWYJiIfZJuuD9AH\
oGrVqjFJSUknxqWnp1O+fPmiihwUL2YCb+Xats0qXjVqpLN9e3mqVIGaNd1OZfHSenJy5po6dSpv\
vPEGLVq0YOjQoZQoUcL1TF7hxUzgfq7Y2NiVInKFawEKQkQ8/wC6Au86Xt8OvJHXPDExMeIUCATE\
a7yYScRbuWbNEomKEnnppYBERVmvg5mnf//gpi0ML60np6xco0aNEkBuuOEGOXr0qCcyeYkXM4m4\
nwtYIR7Y7gfz8Etbwk/A1caYKGN1m2oJrHM5kyoCWZ1DqlQJrnNIKC70GwnefPNN7rnnHjp27Mik\
SZMoWbKk25GUCjlfFDARWQZMA1ZhdaE/A3jb1VCqyCQmWs2GwXTg0NukwJw5c7jrrru47rrrmDx5\
shYvFbF8UcAAROTfInKJiDQUke4icsTtTKpgCnPx3337gpu3uN8mZfz48bz88su0bduWadOmUSrr\
kv5KRSDfFDDlb4Vp2ktOhs2bg5u3OJ+PNnHiRO68806uuOIKZsyYQenSpd2OpFRYaQFTRaIwTXsL\
F0JmZvDzFsfbpHz00Uf07NmT2NhYnn76acqUKeN2JKXCTguYKhKFadpLSICsU5eKY7NgfqZMmUL3\
7t259tprSU5O1j0vVWz46UocyscKcw+vxESYNctqFtT7f51sxowZ3HLLLfzjH/9gzpw5lCtXzu1I\
ShUZLWCqyBTmUlPR0VazYJbkZL2h5axZs7jpppu48sormTdvnidPylUqnLQJUfmOnutldZXv2rUr\
l19+OfPnz6dChQpuR1KqyGkBU76SnAxDhhTvc73mz5/PDTfcQKNGjViwYAEVK1Z0O5JSrtAmROUb\
ycnQtSscPfrXsOLWqWPRokV07NiR+vXrs3DhQipVquR2JKVco3tgyjfeeuvk4nXBBcXrXK+0tDQS\
ExOpW7cuKSkpnHXWWW5HUspVWsCUbzVsWHyK12effUaHDh3429/+RkpKCmeffbbbkZRynRYw5Rt9\
+0LWKU6lS1uvi4MlS5bQrl07atWqRWpqKueee67bkZTyBD0GpnwjMRGmTCle3ee//PJL2rZtS/Xq\
1UlNTaVq1apuR1LKM7SAKV8pzLlkfrN8+XJat27NeeedR1paGtWqVXM7klKeok2ISnnQypUrSUhI\
4JxzziEtLY3q1au7HUkpz9ECppTHfP3117Rq1YpKlSoRCASoWbOm25GU8iQtYEp5yLfffkurVq0o\
X748gUCAWrVquR1JKc/SAqaUR6xZs4aWLVtSpkwZAoEAderUcTuSUp6mBUwpD1i3bh0tW7akZMmS\
BAIB/va3v7kdSSnP0wKmlMu+//574uLiMMYQCAS4+OKL3Y6klC9oN3qlXLRx40ZiY2PJzMwkEAhQ\
t25dtyMp5RtawJRyyebNm4mNjeXo0aMEAgHq16/vdiSlfMU3TYjGmErGmGnGmPXGmHXGmP9zO5NS\
p2vr1q3ExsZy8OBBUlJSaNiwoduRlPIdP+2BvQbMF5EuxphSQJTbgZQ6HT/99BNxcXHs37+f1NRU\
Gjdu7HYkpXzJFwXMGBMNNAN6AojIUeBoXvMo5UXbt28nLi6O3bt3s2jRIi6//HK3IynlW35pQqwD\
7ATGG2O+Nsa8Y4wp53YopQril19+IS4ujh07drBgwQKaNm3qdiSlfM2IiNsZ8mWMuQJYCvxTRJYZ\
Y14D9ovI49mm6wP0AahatWpMUlLSiXHp6emUL1++CFPnz4uZwJu5/J5p9+7dDBo0iJ07d/Liiy+G\
9ZiX39dVUfFiJnA/V2xs7EoRucK1AAUhIp5/AOcBWx2vrwXm5jVPTEyMOAUCAfEaL2YS8WYuP2f6\
/fffpX79+hIVFSWfffZZeEOJv9dVUfJiJhH3cwErxAPb/WAevmhCFJHfgG3GmKyTZFoCa12MpFRQ\
/vjjD+Lj49myZQtz587l2muvdTuSUhHDF504bPcAH9o9EDcDd7icR6k87d69m1atWvHDDz8wZ84c\
WrRo4XYkpSKKbwqYiHwD+KNdVhV7e/bsoVWrVqxbt45Zs2bRsmVLtyMpFXF8U8CU8ou9e/eSkJDA\
mjVrmDlzJq1bt3Y7klIRSQuYUiG0f/9+2rZty+rVq5k+fTrt2rVzO5JSEUsLmFIhcuDAAdq2bcuK\
FSuYOnUqHTp0cDuSUhFNC5hSIfDnn3/Svn17li1bRlJSEh07dnQ7klIRTwuYUoV08OBBOnTowBdf\
fMFHH31Ely5d3I6kVLGgBUypQjh06BDXX389ixcvZuLEidx0001uR1Kq2PDFicxKedHRo0fp1KkT\
qampjB8/nltvvdXtSEoVK7oHptRpOHLkCMOGDWPZsmW8++679OjRw+1IShU7ugemVAEdPXqUG2+8\
kWXLlvHWW2/Rq1cvtyMpVSxpAVOqADIyMrj55ptJTk7mvvvuo0+fPm5HUqrY0iZEpYJ07Ngxbr31\
VmbOnMlrr71Go0aN3I6kVLGme2BKBeHYsWN0796dqVOn8vLLL3Pvvfe6HUmpYk8LmFL5OH78OHfc\
cQdJSUm88MIL3H///W5HUkqhBUypPGVmZtK7d28++OADnnnmGR5++GG3IymlbFrAlMpFZmYmffv2\
ZcKECfznP//hscceczuSUspBC5hSORAR7r77bt555x2GDh3KsGHD3I6klMpGC5hS2YgI99xzD2+9\
9RaPPPIITz75pNuRlFI50AKmlIOIcP/99/P666/z4IMP8uyzz2KMcTuWUioHWsCUsokIDz30ECNG\
jGDgwIG8+OKLWryU8jAtYEphFa9HH32Ul19+mQEDBvDKK69o8VLK47SAKQUMGzaMF154gbvuuouR\
I0dq8VLKB3xVwIwxJYwxXxtj5ridRUWOJ598kqeffprevXszevRoLV5K+YSvChhwH7DO7RAqcjz7\
7LP8+9//pmfPnrz11luccYbf/iSUKr5889dqjKkBtAfecTuLigzDhw9nyJAhdO/enXfeeUeLl1I+\
46e/2BHAw0CmyzlUBHjllVd4+OGHufnmmxk/fjwlSpRwO5JSqoCMiLidIV/GmOuAdiJytzGmBfCg\
iFyXw3R9gD4AVatWjUlKSjoxLj09nfLlyxdN4CB5MRN4M1coM02fPp3Ro0fTvHlzHn/88dMuXl5c\
T+DNXJopeG7nio2NXSkiV7gWoCBExPMP4DlgO7AV+A04CHyQ1zwxMTHiFAgExGu8mEnEm7lClen1\
118XQDp37ixHjx71RKZQ82IuzRQ8t3MBK8QD2/1gHr5oQhSRR0WkhojUBm4G0kTkNpdjKZ95++23\
6d+/P4mJiUyaNImSJUu6HUkpVQi+KGBKFda4cePo27cv7du3Z8qUKZQqVcrtSEqpQvJdARORxZLD\
8S+lcvPee+/Ru3dv2rRpw7Rp0yhdurTbkSJTMjDA/lepIuC7AqZUQXzwwQfccccdtGzZkhkzZlCm\
TBm3I0WmZKAb8Lr9b3K2cdvQwqZCTguYilhJSUn06NGDFi1aMGvWLMqWLet2pMi1EKtrFfa/C+3n\
WYVtB6cWNqUKSQuYikhTp07ltttu45prrmH27NlERUW5HSmyJQBZqzjKfg25FzalQkALmIo4M2fO\
pFu3blx99dXMnTuXcuXKuR0p8iUCk4D+9r+J9vDcClsWPW6mCuFMtwMoFUrJycnceOONNG3alE8+\
+cSTJ6pGrET+KlzOYZOAfZxc2OCv5sWDwPgcxiuVD90DUxFj3rx5dOnShcsuu4z58+dToUIFtyMp\
sIpSTU4tTtq8qApJC5iKCAsWLKBTp05ceumlLFy4kOjoaLcjqfzk17yoVD60CVH5XkpKCtdffz31\
69dn0aJFVKpUye1IKhhZzYsLsYqXNh+qAtICpnwtEAiQmJjI3//+dxYtWsRZZ53ldqTiKxmrGEVj\
HfPKKkrJ9utkcj5GpoVLnSYtYMq3Pv/8c6677jouvPBCUlNTOeecc9yOVHw5O2RkGQ8MxLoR0pNA\
P/LuqJFVAHVvTAVJj4EpX/riiy9o27YtF1xwAampqZx77rluRyrenB0yshzEKkrBdNTI60oeSuVC\
C5jynaVLl9K2bVuqV69OWloaVatWdTtS0fLiuVPODhlZorD2pILpqKE9EtVp0CZE5StfffUVrVu3\
pkqVKqSlpVGtWjW3IxUtr5475eyQkf0Y2FXkfB6YUwLW5zmI9khUQdMCpnxj1apVJCQkcPbZZ5OW\
lkb16tXdjlT0ctpT8UIBg9w7ZCQCi4EW+cyrPRJVAWkTovKFjRs3Eh8fT3R0NIFAgAsuuMDtSO6I\
5HOnEoHRaPFSQdM9MOV5//vf/3jggQeIjo4mLS2NWrVquR3JPW7vqWhPQeUhWsCUp61du5aWLVtS\
qlQp0tLSuPDCC92O5D63zp3y6vE3VWxpE6LyrPXr1xMXF0eJEiV45ZVXuOiii9yOVLxpT0HlMVrA\
lCd9//33xMXFAdbVNmrWrOlyIpXj8bdQdOn34mkByhe0gCnP2bRpE3FxcWRkZJCamsoll1zidiQF\
p97zC/I/+TirOO3LZZl6ArMqBC1gylO2bNlCbGwshw8fJjU1lQYNGrgdSTk5ewrm16ToLE6bybk4\
abOkKgRfFDBjTE1jTMAYs9YY850x5j63M6nQ+/HHH4mNjSU9PZ2UlBQaNWrkdiSVl/y69DuLUyY5\
F6dIPi1AhZ1feiEeAx4QkVXGmArASmPMIhFZ63YwFRrbtm0jNjaWffv2kZKSQpMmTdyOpPKTX5d+\
59U1ziDn4uT2aQHK13xRwETkV+BX+/kBY8w6oDqgBSwC/Pzzz8TFxbFr1y5SUlKIiYlxO5IKVl5d\
+p3F6cJ8ptPCpU6DL5oQnYwxtYHLgGUuR1Eh8OuvvxIXF8fvv//OggULaNq0qduRVChlHTPTG2Sr\
MDAi4naGoBljygOfAs+IyIwcxvcB+gBUrVo1Jikp6cS49PR0ypcvX1RRg+LFTFB0uXbv3s2gQYPY\
sWMHL774IpdeeqnrmQrCi5nAm7k0U/DczhUbG7tSRK5wLUBBiIgvHkBJYAFwfzDTx8TEiFMgEBCv\
8WImkaLJtWPHDmnQoIFERUXJp59+6olMBeXFTCJFkGuWiPS3/w1ynBfXlRczibifC1ghHtjmB/Pw\
RROiMcYA7wLrROQVt/Oowtm1axfx8fFs3ryZOXPm0KxZM7cjqWDldd6Wc1xnYEiRp1PFjC86cQD/\
BLoD/zPGfGMPe0xE5rkXSZ2O3bt3Ex8fz4YNG5gzZw6xsbFuR1K5ybpwr/P+XnndzsU57jjwIta9\
wLSDhgoTXxQwEVkCGLdzqMLZu3cvCQkJrF27luTkZOLj492OVLzldWV554V7s4wHBmKdr5XTjScT\
gDexihdYJ7946X5lKuL4oglR+d++ffto3bo13377LTNnzqR169ZuRyre8ruEk3NvKstB/rqzctbl\
pJzFKREYzF//LdYTk1WYaQFTYbd//37atGnDqlWrmDZtGu3atXM7ksrvEk4JQKlsw0rx195abjee\
fAaYTs4FrjD0gr8qB1rAVFilp6fTrl07vvrqK6ZMmUJiorYneUJ+l3BKBLK38MYTXEEK9Z2V9YK/\
KhdawFTY/Pnnn7Rv356lS5cyadIkOnXq5HYklSX7leVzKjZ9ObnI9T3N90oGtnH6hUcv+KtyoQVM\
hcXBgwdJTExkyZIlTJw4ka5du7odSWWX355SMEUuP1l7Tzs4/b0nveCvyoUveiEqfzl8+DAdO3Yk\
EAjw/vvv061bN7cjqdOV13UK8+rFmCWvbvcFyaAX/FU50AKmQurIkSN06tSJlJQUxo8fz2233eZ2\
JBUOzm7248l9Dy3rivRQuL0nveCvyoE2IaqQOXLkCF26dGH+/PmMHTuWHj16uB1JhUuwx6Wy9p6q\
ENpeiUqhBUyFSEZGBjfddBNz5sxhzJgx3HnnnW5HUuFUkONSiUBNtHipkNMmRFVoGRkZdOvWjVmz\
ZjF69Gj69evndiQVbnpcSnmAFjBVKMeOHeO2225j+vTpvPrqq/Tv39/tSKqo6HEp5TJtQlSn7fjx\
4/To0YMpU6YwfPhwBg4c6HYkVRh6tQvlM1rA1Gk5fvw4d9xxBx999BHPP/88Dz74oNuRVGHo1S6U\
D2kBUwWWmZnJv/71LyZOnMhTTz3F4MGD3Y6kCkuvdqF8SAuYKpDMzEz69evH+PHjGTZsGEOHDnU7\
kgoFvdqF8iHtxKGCJiIMGDCAsWPHMmTIEJ544gm3I6lQKWivwmCuwqFUmGkBU0EREe677z7GjBnD\
ww8/zFNPPYUxeo/RiBJsr8Jgr8KhVJhpE6LKl4jwwAMPMGrUKAYNGsTzzz+vxas40+NlyiO0gKk8\
iQiDBw/m1Vdf5d577+Xll1/W4lXc6fEy5RHahKhyJSIMHTqU4cOHc/fddzNixAgtXkqvwqE8QwuY\
ytUTTzzBs88+S58+fRg1apQWL/UXvQqH8gDfNCEaY9oYYzYYYzYaYx5xO0+ke+qpp3jyySfp1asX\
Y8aM4YwzfPNTUUoVE77YAzPGlMC6RkArYDvwlTEmWUTWupssMn300UeMHTuW22+/nbFjx2rxUkp5\
kl+2TFcCG0Vks4gcBZKA613OFJFeeuklxo4dyy233MK4ceO0eKmC02sqqiLil61TdWCb4/V2e5gK\
oREjRvDQQw8RGxvLe++9R4kSJdyOpPxGr6moipAREbcz5MsY0wVoIyK97dfdgatEZEC26foAfQCq\
Vq0ak5SUdGJceno65cuXL7rQQfBSppkzZzJy5EiaN2/OoEGDiI6OdjvSSby0rrJ4MRO4nGsbsMPx\
ugpQ05vryouZwP1csbGxK0XkCtcCFISIeP4B/B+wwPH6UeDRvOaJiYkRp0AgIF7jlUxvvPGGANKx\
Y0c5evSoZ3I5aabguZprlohEifVXGGW/Fm+uKy9mEnE/F7BCPLDdD+bhi04cwFfAxcaYOsDPwM3A\
Le5Gigxjx47l7rvvpkOHDkyePJmSJUu6HUn5mZ4jpoqQLwqYiBwzxgwAFgAlgHEi8p3LsXxv/Pjx\
9O3bl7Zt2zJ16lRKlSrldiQVCfQcMVVEfFHAAERkHjDP7RyRYuLEidx5553Ex8czY8YMSpcu7XYk\
pZQqEL/0QlQh9NFHH9GzZ09iY2OZNWsWZcqUcTuSUkoVmBawYmbKlCl0796dZs2aMXv2bMqWLet2\
JKWUOi1awIqRGTNmcMstt/CPf/yD2bNnExUVlf9MSinlUVrAiolZs2Zx0003ceWVVzJv3jxPnv+i\
lFIFoQWsGJgzZw5du3YlJiaG+fPnU6FCBbcjKaVUoWkBi3Dz58/nhhtuoHHjxsyfP5+KFSu6HUkp\
pUJCC1gEW7RoER07dqRBgwYsXLiQSpUquR1JKaVCRgtYhEpLSyMxMZFLLrmERYsWUblyZbcjKaVU\
SGkBi0CffvopHTp04KKLLiIlJYWzzz7b7UhKKRVyvrkShwrOkiVLaN++PbVq1SI1NZVzzjnH7Uiq\
mMvIyKB8+fKsW7fO7SgniY6O9lwmKLpcZcqUoUaNGr6+/qkWsAjy5Zdf0rZtW2rUqEFaWhpVqlRx\
O5JSbN++napVq1KjRg2MMW7HOeHAgQOe7JFbFLlEhF27drF9+3bq1KkT1vcKJ21CjBDLly+ndevW\
VKtWjbS0NM477zy3IykFwOHDh4mOjvZU8SrujDGcffbZHD582O0ohaIFLAKsXLmShIQEzj33XNLS\
0jj//PPdjqTUSbR4eU8kfCdawHzu66+/plWrVlSuXJlAIECNGjXcjqSU52S/8syECRN44IEHXEoD\
tWvX5o8//gjb8pcvX06TJk1o0qQJjRs3ZubMmWF7LzfpMTAf+/bbb4mPj6dChQoEAgEuuOACtyMp\
pYBjx45x5pnubV4bNmzIihUrOPPMM/n1119p3LgxHTp0cDVTOOgemE+tWbOGli1bEhUVRSAQoHbt\
2m5HUsp3Dhw4QJ06dcjIyABg//79J163aNGC++67jyZNmtCwYUOWL18OwJ9//kmvXr248sorueyy\
y5g1axZg7dUlJiYSFxdHy5YtWbx4Mc2aNaN9+/bUrVuXfv36kZmZeUqGjh07EhMTQ4MGDXj77bdP\
DC9fvjxDhgyhcePGXH311fz+++8A7Ny5kxtuuIGmTZvStGlTvvjii1OWGRUVdaJYHT58OCKaC3Oi\
BcyH1q5dS8uWLSlVqhRpaWlceOGFbkdSytMOHTp0okmtSZMmDBs2DIAKFSrQokUL5s6dC0BSUhKd\
O3c+0bX84MGDfPPNN7zxxhv06tULgGeeeYa4uDiWL19OIBDgoYce4s8//wRg1apVTJs2jU8//RSw\
mvJGjRrF2rVr2bRpEzNmzDgl27hx41i5ciUrVqxg5MiR7Nq1C7AK5dVXX83q1atp1qwZY8eOBeC+\
++5j0KBBfPXVV0yfPp3evXvn+JmXLVtGgwYNuPTSS3nzzTcjbu8LtAnRdzZs2EBcXBxnnHEGaWlp\
XHzxxW5HUipoAwcO5JtvvgnpMps0acKIESPynKZs2bInve+ECRP473//C0Dv3r158cUX6dixI+PH\
jz9RKAC6desGQLNmzdi/fz979+5l4cKFJCcn89JLLwHWHs5PP/0EQKtWrTjrrLNOzH/llVee+A9m\
t27dWLJkCV26dDkp28iRI08co9q2bRubNm2idu3alCpViuuuuw6AmJgYFi1aBEBKSgpr1649Mf/+\
/ftJT08/5TjfVVddxXfffce6devo0aMHbdu2jbib12oB85EffviB2NhYRIS0tDTq1q3rdiSlfO+f\
//wnW7duZfHixRw/fpyGDRueGJe96c0Yg4gwffr0U/7+li1bRrly5U6ZPq/XixcvJiUlhS+//JKo\
qChatGjBkSNHAChZsuSJ6UuUKMGxY8cAyMzMZOnSpUEXo3r16lG+fHnWrFnDFVdcEdQ8fqEFzCc2\
bdpEbGwsGRkZBAIB6tev73YkpQosvz0lt9x+++3ccsstPP744ycNnzx5MrGxsSxZsoTo6Giio6Np\
3bo1o0aNYtSoURhj+Prrr7nssstyXO7y5cvZsmULtWrVYvLkyfTp0+ek8fv27aNy5cpERUWxfv16\
li5dmm/WhIQERo0axUMPPQTAN998Q5MmTU6aZsuWLdSsWZMzzzyTH3/8kfXr10fkcXI9BuYDW7du\
JS4ujkOHDpGamnrS/xCVUoV36623smfPnhNNhlnKlCnDZZddRr9+/Xj33XcBePzxx8nIyKBRo0Y0\
aNDglKLn1LRpUwYMGEC9evWoU6cOnTp1Oml8mzZtOHbsGPXq1eORRx7h6quvzjfryJEjWbFiBY0a\
NaJ+/fq8+eabp0yzZMkSGjduTJMmTejUqRNvvPFGZF5WTkQ8/QCGA+uBb4GZQKVg5ouJiRGnQCAg\
XhNMph9//FFq164tlSpVklWrVoU/lPh3XRU1L2YS8V6utWvXyv79+92OcQpnpqlTp8ptt9120vjm\
zZvLV199ddrLDwQC0r59+0LlCre1a9eeMgxYIR7Y9gfz8EMT4iLgURE5Zox5AXgUGOxypiKxfft2\
YmNj2bNnDykpKbk2UyilTt8999zDJ598wrx589yOogrI8wVMRBY6Xi4FuuQ2bST55ZdfiI2NZefO\
nSxatCjiDr4q5RWjRo3KcfjixYsLtdwWLVrQokWLQi1D5c1vx8B6AZ+4HSLcfvvtN+Li4vjtt9+Y\
P38+V111lduRlFLKc4zV5OlyCGNSgJwunz5ERGbZ0wwBrgA6Sy6hjTF9gD4AVatWjUlKSjoxLqfz\
JNyWU6Y9e/YwaNAgfv/9d1544QUaNWrkiVxu00zB81qu6Ohoateu7bkTaY8fP06JEiXcjnGKosol\
ImzatIl9+/adNDw2NnaliPijycftg3DBPICewJdAVLDz+LETx86dO6Vhw4ZStmxZWbx4sTuhxB/r\
ygu8mEnEe7k2b94sP/30k2RmZrod5SRe7FgiUjS5MjMzZefOnbJ58+ZTxqGdOELHGNMGeBhoLiIH\
3c4TLrt27SI+Pp6NGzcyd+5cmjdv7nYkpUKiRo0arF69mvT0dLejnOTw4cOevDJFUeXKuiOzn3m+\
gAGjgdLAIvus9KUi0s/dSKG1Z88eWrVqxfr160lOTiYuLs7tSEqFTMmSJUlPT/dcR6TFixd7smev\
V3N5kecLmIhc5HaGcNq7dy8JCQl89913fPzxxyQkJLgdSSmlfMHzBSyS/fnnn7Rp04bVq1czY8YM\
2rZt63YkpZTyDS1gLjlw4ACDBw9mw4YNTJ069cRVp5VSSgXHE93ow8EYsxP40THoHCB89/A+PV7M\
BN7MpZmC58Vcmil4bueqJSLnuvj+QYvYApadMWaFeOzcBi9mAm/m0kzB82IuzRQ8r+byIr9diUMp\
pZQCtIAppZTyqeJUwN52O0AOvJgJvJlLMwXPi7k0U/C8mstzis0xMKWUUpGlOO2BKaWUiiARVcCM\
MV2NMd8ZYzKNMVdkG/eoMWajMWaDMaZ1LvPXMcYss6ebbIwpFeJ8k40x39iPrcaYb3KZbqsx5n/2\
dCtCmSGX93vCGPOzI1u7XKZrY6+/jcaYR8KcabgxZr0x5ltjzExjTKVcpgv7usrvcxtjStvf7Ub7\
91M7HDkc71fTGBMwxqy1f+/35TBNC2PMPsd3OiycmRzvm+f3YSwj7XX1rTHm8jDnqetYB98YY/Yb\
YwZmm6ZI1pUxZpwxZocxZo1j2FnGmEXGmB/sfyvnMm8Pe5ofjDE9wpHPl9y+mnAoH0A9oC6wGLjC\
Mbw+sBrrmop1gE1AiRzmnwLcbD9/E7grjFlfBoblMm4rcE4RrrcngAfzmaaEvd4uBErZ67N+GDMl\
AGfaz18AXnBjXQXzuYG7gTft5zcDk8P8fVUDLrefVwC+zyFTC2BOUf2Ggv0+gHZY9/QzwNXAsiLM\
VgL4Des8pyJfV0Az4HJgjWPYi8Aj9vNHcvqdA2cBm+1/K9vPKxf1d+vFR0TtgYnIOhHZkMOo64Ek\
ETkiIluAjcCVzgmMdaXgOGCaPeg9oGM4ctrvdSMwKRzLD5MrgY0isllEjgJJWOs1LERkoYgcs18u\
Bdy6bHYwn/t6rN8LWL+flvZ3HBYi8quIrLKfHwDWAdXD9X4hdj3wvliWApWMMdWK6L1bAptE5Md8\
pwwDEfkM2J1tsPO3k9s2pzWwSER2i8geYBHQJlw5/SSiClgeqgPbHK+3c+of/NnAXsdGM6dpQuVa\
4HcR+SGX8QIsNMastG/SWRQG2E0643JpxghmHYZLXnfiDve6CuZzn5jG/v3sw/o9hZ3dXHkZsCyH\
0f9njFltjPnEGNOgKPKQ//fh5u/oZnL/T6Mb6wqgqoj8aj//DaiawzRurjNP8921EE0Qd292U5D5\
upH33tc1IvKzMaYK1m1k1tv/ewtLLmAM8BTWxucprObNXoV5v8JmkpPvxH0M+DCXxYR8XfmFMaY8\
MB0YKCL7s41ehdVUlm4f0/wYuLgIYnny+7CPZycCj+Yw2q11dRIREWOMdgsvAN8VMBGJP43ZfgZq\
Ol7XsIc57cJqzjjT/l90TtMUOp8x5kygMxCTxzJ+tv/dYYyZidWMVaiNQLDrzRgzFpiTw6hg1mFI\
MxljegLXAS3FPhiQwzJCvq6yCeZzZ02z3f5+o7F+T2FjjCmJVbw+FJEZ2cc7C5qIzDPGvGGMOUdE\
wnqNvSC+j5D/joLUFlglIr9nH+HWurL9boypJiK/2k2pO3KY5mes43RZamAd5y/2iksTYjJws91b\
rA7W/66WOyewN5ABoIs9qAcQjj26eGC9iGzPaaQxppwxpkLWc6zODGtymjZUsh2D6JTL+30FXGys\
npqlsJpjksOYKetO3ImSy524i2hdBfO5k7F+L2D9ftJyK7ihYB9fexdYJyKv5DLNeVnH4YwxV2L9\
rYe7qAbzfSQDt9u9Ea8G9jma0MIp11YPN9aVg/O3k9s2ZwGQYIypbDfvJ9jDlNu9SEL5wNr4bgeO\
AL8DCxzjhmD1JtsAtHUMnwecbz+/EKuwbQSmAqXDkHEC0C/bsPOBeY4Mq+3Hd1jNaeFebxOB/wHf\
Yv1BVcuey37dDqvH26Zw57K/g23AN/bjzeyZimpd5fS5gSexiitAGfv3stH+/VwY5nVzDVZz77eO\
9dMO6Jf12wIG2OtkNVYnmH8Uwe8ox+8jWy4DvG6vy//h6C0cxlzlsApStGNYka8rrAL6K5Bhb6fu\
xDpWmgr8AKQAZ9nTXgG845i3l/372gjcEe515peHXolDKaWULxWXJkSllFIRRguYUkopX9ICppRS\
ype0gCmllPIlLWBKKaV8SQuYUkopX9ICppRSype0gCmllPIlLWBKKaV8SQuYUkopX9ICppRSype0\
gCmllPIlLWBKKaV8SQuYUkopX9ICppRSype0gCmllPIlLWBKKaV8SQuYUkopX9ICppRSype0gCml\
lPKl/wcOsDl8cmOWEAAAAABJRU5ErkJggg==\
"
  frames[3] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAArq0lEQVR4nO3deZwU9Z3/8dcHBAYYGEF0mAEiGBNUiIoDSBSVEUU8fmgS3dVc\
osmiiWbVJGs0xl13s24SN7+NMckuxnjkdMxlmE10IyaMxkRlweCtEZUIcig3w4189o+qHmp6unt6\
mO6pqpn38/Hox3TX1Z+ubupNfetbVebuiIiIpE2vuAsQERHZHwowERFJJQWYiIikkgJMRERSSQEm\
IiKppAATEZFUUoCJiEgqKcBERCSVFGAiIpJKCjAREUklBZiIiKSSAkxERFJJASYiIqmkABMRkVRS\
gImISCopwEREJJUUYCIikkoKMBERSSUFWIqZmZvZ4WVa9klm9nKJlrXMzE4r0bLK9pm7IzN73sym\
lWG5s83ssVIvtxz0m+m+FGAJYWbXm9mDWcNeyTPswhK8X8FQcfc/uPvY/VjuPWb2r52rrmulseZc\
cn0Odx/n7k0xldRjmNkxZjbfzNab2XIzmx13TT2BAiw5HgVOMLPeAGZWA/QBJmQNOzycVnoQMzsg\
7hqSKPNvIwFGAd8BaoC/BW43s9p4S+oB3F2PBDyAvsA2oC58/TfA3cAjWcOWRuZx4HLgFWAjwT8g\
C8e9G/g9sA5YC/wYODAc90NgL7AdaAauzVHPNGBF5PUXgDeBLcDLwPQc88wBdgO7wuX+dzh8GfB5\
4BlgE3AfUBGZ7xxgSfgZ/gQcXWA9OfD3wGvh5/p3oFdk/KXAi8AG4LfAoeFwA74BvAVsBp4Fxuer\
Oes9c84bjusHfB14A1gDzAX6R9ch8MWw1mXARyLLPRv4c7jM5cBNkXGjw8/6iXDZj4bDfwasDtfj\
o8C4Itb9aeHzm4CfAj8Iv8fngYmR9zwurGdL+D73Af+a53uYDTwWfvYNwOvAmeG4C4DFWdN/FpgX\
Pr8nXE/zw/d6JPM9heOPCMetJ/it/U1k3D3AfwEPAFuB04pYngOHd2CdXxyu87XADZHxvYDrgFcJ\
/l39FBiaY90cEH4H74t7u9LdH7EXoEfky4AFwDXh828TbIxvzhp2V2R6B34NHAi8C3gbmBmOOxw4\
nWADe3C4sbs1Mm/Lhi1PLdMIAwwYG/5jrw1fjwbenWe+e7I3euF7LQRqgaEEAXN5OG4CQTAcD/QO\
Nx7LgH55lu/hehoafua/AJ8Mx50LLAWODDciXwL+FI47A1gcrisLp6nJV3PWexaa9xtAY1jPIOC/\
ga9E1uEe4D/C7+EUgo3u2Mj494UbxqMJAvC8yDp2grAZyL5QvDR8n37ArcCSItZ9NMB2AGeF6/or\
wBPhuL7AX4GrCPb8P0gQhoUCbDfwd+GyPgWsDNdPP4LwOTIy/Z+BD0Xq3AKcHE77TeCxcNxAgt/a\
JeF3OIEgSI6KzLsJODFcbxWFlhf5zRzegXV+B9AfOAbYmfkc4bp5AhgZvs/twL051s1tBL/3XrnW\
nR4l3GbGXYAekS8j2MDcHz5/GngPMDNr2MWR6R2YGnn9U+C6PMs+D/hz5HXLhi3P9NPYF2CHE4TM\
aUCfdj7DPdkbvfC9Php5fQswN3z+X8CXs6Z/GTglz/KdMKTD158Gfhc+fxD4RGRcL4K92kOBUwnC\
bkr2hiVXzVnjc85LsLHeSiTMgfcDr0fW4R5gYNZ3dGOe97kV+Eb4PLMxPaxAXQeG01S1s+6jAfZw\
ZNxRwPbw+ckEe9gWGf9YvvVCEGDR1oABYS3DI9/rzeHzcQR7af0idTZE5q0E3iFohvtb4A9Z73U7\
8E+ReX+Q4/vLubzIb+bwDqzzkZHxC4ELw+cvEml5IGgu3A0cEBl2LcHvd3ihfyd6lOahY2DJ8igw\
1cyGAge7+ysETWonhMPG0/b41+rI820E/3gxs2ozazCzN81sM/AjYNj+FOXuS4GrCTaAb4XL7Wj7\
fs46CcLlc2a2MfMg2JAVWv7yyPO/RqY9FPhmZDnrCUJmhLv/nmAP9jvhZ/iumQ0upvAC8x5MsOFe\
HHnP/wmHZ2xw96256jWz481sgZm9bWabCJqDs7+jls9qZr3N7Ktm9mr4nS4LR3Xke83+HirC42u1\
wJseboWz37u9Zbn7tvBp5nv9PvBhMzPgY8BP3X1nrmW7ezPBd1VL8B0en/V7+AgwvJ268i2vlSLX\
eaHf6v2Rul4kCMrqyPRXA3/n7tFlSJkowJLlcaCKoFnmjwDuvpmgaebvgJXu/nqRy/o3gv9Nvs/d\
BwMfJdiYZ3jOufJw95+4+1SCf8QOfC3fpB1ZLsGG52Z3PzDyGODu9xaYZ1Tk+bsI1k9mWZdlLau/\
u/8p/Ay3uXsdwZ7He4F/KLbmPPOuJTiOOC7yflXuXhmZdYiZDcxT708Imh9HuXsVwXGc6HeUXduH\
CZpJTyP4nYwOh1uOaTtqFTAiDJyMUfkmbo+7P0HQBHkSQd0/zJqkZdlmVknQBLuS4Dt8JOs7rHT3\
T0UXn+Mt8y0vWzHrPJ/lBMf5orVVuPubkWlq8ryvlIECLEHcfTuwiOCA9x8iox4Lh3Wk9+EgggPJ\
m8xsBPs21hlrgMOKWZCZjTWzU82sH8ExlO0EnUByKXq5oTuAy8P/GZuZDTSzs81sUIF5/sHMhpjZ\
KILjEveFw+cC15vZuLDuKjO7IHw+KXyPPgTNfjsin6Fgzfnmdfe9Yf3fMLNDwmlHmNkZWYv4ZzPr\
a2YnEXRY+Vk4fBCw3t13mNlkgg19IYMIjsmsI9jz+7es8R1d91GPE+xNXGlmB5jZucDk/VxWxg8I\
9lx3u3v2OWNnmdlUM+sLfJngWNxygmO67zWzj5lZn/AxycyObOe98i0vW0fXedRc4GYzOxTAzA4O\
11NUDUGHFukCCrDkeQQ4hCC0Mv4QDutIgP0zQa+yTcBvgF9mjf8K8KWwOeTz7SyrH/BVgj2O1WEt\
1+eZ9k7gqHC5v2qvSHdfRLB3+W2C4yRLCY6vFDKPoFPFEoLPdme4rPsJ9gwbwia254Azw3kGE4TN\
BoJmvHUEPRiLqbnQvF8Ia34ifM+HCTq9ZKwO51tJ0BP0cnd/KRz3aeBfzGwL8I8Ex8cK+UH4/m8C\
LxB0KIjq0LqPcvddBB03PkHQG/SjBGGys8Bs7fkhQbP3j3KM+wnwTwRNfXXh++HuW4AZwIUE62w1\
wXfar533yrm8HDq6zqO+SbD39lA4/xMEnY+ilhK0UkgXyHS5FpESs+AKGD9y95Exl7JfzOxJgs42\
d+/n/P0JOv8cFx7PzQy/h6CD0JdKVGdJlyfpoT0wEQHAzE4xs+FhE+LFBN3M/6cTi/wU8L/R8BIp\
pUSd3W9mdxEcI3jL3ceHw/4d+H8EB4RfBS5x942xFSnSfY0laFIbSHCi+Pnuvmp/FmRmywg6R5xX\
quJEsiWqCdHMTiboePCDSIDNAH7v7nvM7GsA7v6FGMsUEZEESFQTors/SnAQNjrsIXffE77MnAUv\
IiI9XKICrAiXElxtQUREerhEHQMrxMxuILgsz48LTDOH4KKm9O/fv27UqH3nYe7du5devZKV10ms\
CZJZl2oqXhLrUk3Fi7uuv/zlL2vd/eD2p0yAuK9llf0guLrAc1nDZhOcaDmg2OXU1dV51IIFCzxp\
kliTezLrUk3FS2Jdqql4cdcFLPIEZEExj8TvgZnZTIILZJ7i+663JiIiPVyi9p/N7F6CPa2xZrbC\
zD5BcIWGQcB8M1tiZnNjLVJERBIhUXtg7n5RjsF3dnkhIiKSeIkKMBHpfnbv3k1lZSUvvvhi3KW0\
UlVVlbiaoOvqqqioYOTIkfTp06fs71UuCjARKasVK1ZQXV3NyJEjaX23lnht2bKFQYMK3fQgHl1R\
l7uzbt06VqxYwZgxY8r6XuWUqGNgItL97Nixg6qqqkSFV09nZhx00EHs2LEj7lI6RQEmImWn8Eqe\
7vCdKMBEpNurrKxs9fqee+7hc5/7XEzVwOjRo1m7dm3Z3+eNN96gsrKSr3/962V/rzgowERESmzP\
nj3tT9QFPvvZz3LmmWe2P2FKKcBEpMfasmULY8aMYffu3QBs3ry55fW0adO46qqrOPbYYxk/fjwL\
Fy4EYOvWrVx66aVMnjyZCRMmMG/ePCDYq5s1axannnoq06dPp6mpiZNPPpmzzz6bsWPHcvnll7N3\
7942NZx33nnU1dUxbtw4vvvd77YMr6ys5IYbbuCYY45hypQprFmzBoC3336bD33oQ0yaNIlJkybx\
xz/+Medn+9WvfsWYMWMYN25cSddZkqgXooh0mauvvpolS5aUdJnHHnsst956a8Fptm/fzrHHHtvy\
ev369cycOZNBgwYxbdo0fvOb33DeeefR0NDABz/4wZau5du2bWPJkiU8+uijXHrppTz33HPcfPPN\
nHrqqdx1111s3LiRyZMnc9pppwHw1FNP8cwzzzB06FCamppYuHAhL7zwAoceeigzZ87kl7/8Jeef\
f36r2u666y6GDh3K9u3bmTRpEjNmzGDQoEFs3bqVKVOmcPPNN3Pttddyxx138KUvfYmrrrqKa665\
hqlTp/LGG29wxhlntOl239zczNe+9jXmz5/fbZsPQQEmIj1A//79WwXnPffcw5/+9CcAPvnJT3LL\
Lbdw3nnncffdd3PHHXe0THfRRcG1FU4++WQ2b97Mxo0beeihh2hsbGwJhh07dvDGG28AcPrppzN0\
6NCW+SdPnsxhhx3WsqzHHnusTYDddttt3H///QAsX76cV199ldGjR9O3b1/OOeccAOrq6pg/fz4A\
Dz/8MC+88ELL/Js3b6a5ubnVcb6bbrqJa665ps2xv+5GASYiXaa9PaU4nHjiiSxbtoympibeeecd\
xo8f3zIuu6eemeHu/OIXv2Ds2LGtxj355JMMHDiwzfSFXjc1NfHwww/z+OOPM2DAAKZNm8bOnTsB\
6NOnT8v0vXv3bjmutnfvXp544gkqKiryfqYnn3ySn//851x77bVs3LiRXr16UVFRwZVXXlnMKkkN\
HQOTWDU2wpVXBn9F4vLxj3+cD3/4w1xyySWtht93330APPbYY1RVVVFVVcUZZ5zBt771rcydMvjz\
n/+cd7kLFy7k9ddfZ+/evdx3331MnTq11fhNmzYxZMgQBgwYwEsvvcQTTzzRbq0zZszgW9/6Vsvr\
XE2yf/jDH1i2bBnLli3j6quv5otf/GK3Cy9QgEmMGhvhoovgO98J/irEJC4f+chH2LBhQ0uTYUZF\
RQUTJkzg8ssv5847g8uy3njjjezevZujjz6acePGceONN+Zd7qRJk7jyyis58sgjGTNmDB/4wAda\
jZ85cyZ79uzhyCOP5LrrrmPKlCnt1nrbbbexaNEijj76aI466ijmzu251zdXE6LE5qGHYFt4g5xt\
24LXs2bFW5N0T83Nza1ez549mw996EMtrzPHpg488MBW0330ox9t0+zZv39/br/99jbvMXv2bGbP\
nt1q2ODBg/n1r3/dZtply5a1PH/wwdY3md+yZUubms8///yWY2fDhg1r2TMsxk033VT0tGmjAJPY\
zJgBd98dhNeAAcFrka72mc98hgcffJAHHngg7lKkgxRgEptZs+Dee4M9rxkztPcl8YgeT4pqamrq\
1HKnTZvGtGnTOrUMKUwBJrGaNUvBJSL7R504RKTsMj32JDm6w3eiABORsqqoqGDTpk3dYoPZXWTu\
B1boXLI0UBOiiJTVyJEjefrpp9v0BIzbjh07ErkB76q6MndkTjMFmIiUVZ8+fWhubmbixIlxl9JK\
U1MTEyZMiLuMNpJaVxKpCVFERFIpcQFmZneZ2Vtm9lxk2FAzm29mr4R/h8RZo5SeLiklIh2VuAAD\
7gFmZg27Dvidu78H+F34WroJXVJKRPZH4gLM3R8F1mcNPhf4fvj8+8B5XVmTlFeuS0qJiLQncQGW\
R7W7rwqfrwaq4yxGSmvGjOBSUqBLSolI8SyJ52aY2Wjg1+4+Pny90d0PjIzf4O5tjoOZ2RxgDkB1\
dXVdQ0NDy7jsG74lQRJrgnjq2rQJNm+GwYOhqioZNbUniTVBMutSTcWLu676+vrF7p6sLqP5uHvi\
HsBo4LnI65eBmvB5DfBye8uoq6vzqAULFnjSJLEm92TWVa6a5s1zv+KK4G9HJXE9uSezLtVUvLjr\
AhZ5AnKgmEdamhAbgYvD5xcD82KsRboJdR4RSbfEBZiZ3Qs8Dow1sxVm9gngq8DpZvYKcFr4WlIu\
7q7z6jwikm6JuxKHu1+UZ9T0Li1Eyiqz97NtW3BPsKuvDo6DdeVtVXQ/MpF0S1yASc+Qvfdzyy2w\
Z08QKPfe2zUhpvuRiaRb4poQpWeIdp3v3TsIL+j6prxZs+Db31Z4iaSRAkzKKt9xrszezxVXwBe+\
oPPARKTj1IQoZRM9zvW978H06XDZZfv2dqJ3Yz7+eDXliUjHKMCk5BobgzB6/fV9x7l27oQHHoCm\
ptzHuKJhJiJSDAWYlFR0r6tvX+jXLwivjMwxrnKFVSY8tScn0v3pGJiUVLR34a5dQbPhWWcFYQa5\
j3GV6nwwnZgs0rMowKSksi/Me9ll8JvfwM9+FnTYyG4+LGXo6MRkkZ5FASYlFe1dGA2rfN3VSxk6\
uqq9SM+iY2BSch3pkFHKq2HoxGSRnkUBJrEqdeioN6NIz6EAk9hlh05X9SRUj0WRdFOASayyQyTf\
yc+DB5f+faMXE+6q6y+KSOmoE4fEJlcPxGinjszJzxddFFypPnveznS9V49FkfRTgElsskPkhhug\
qmpfT8KMbdtg8+Z9r0vR9V49FkXSTwEmsYmGCMBzz8Gttwb3Bss++TnahFiKvad83f1FJD0UYLLf\
OtuMlwmR8eP3Ddu2LWguzD75uapq3zSl2nvSrVRE0k2dOGS/lKoTRGaezLKigRTtndjU1Hoene8l\
Igow2S/5mvH2J1T2J5B0vpeIKMBkv2RfQaOqqnN7ZAokEekoHQOT/ZLdCWLTJnVLF5GulZo9MDO7\
Bvgk4MCzwCXuviPeqnq27L2mUl3TUESkGKnYAzOzEcDfAxPdfTzQG7gw3qokSt3SRaSrpWYPjKDW\
/ma2GxgArIy5HslSruNYjY1BE2Vjo4JRRPZJxR6Yu78JfB14A1gFbHJ3HWXpATLd9d96S3dZFpHW\
zN3jrqFdZjYE+AXwt8BG4GfAz939R1nTzQHmAFRXV9c1NDS0jGtubqaysrKrSi5KEmuCZNW1fHkQ\
XiNHNrNiRSWHHAKjRsVdVSBJ6ykqiXWppuLFXVd9ff1id58YWwEd4e6JfwAXAHdGXn8c+M9C89TV\
1XnUggULPGmSWJN7suqaN899wAD3r399gQ8YELwuZp4rrihu2s5I0nqKSmJdqql4cdcFLPIEbPeL\
eaSiCZGg6XCKmQ0wMwOmAy/GXJN0gUznkEMOKa5zSCku9Csi6ZCKAHP3J4GfA08RdKHvBXw31qKk\
y8yaFTQbFtOBQ7dJEek5UhFgAO7+T+5+hLuPd/ePufvOuGuSjunMxX83bSpuXt0mRaTnSE2ASbp1\
pmmvsRFee624eXU+mkjPoQCTLtGZpr2HHoK9e4ufV7dJEekZFGDSJTrTtDdjBvTqtX/zikj3laYr\
cUiKdeYeXrNmwbx5QbOg7v8lIhkKMOkynbnUVFVV0CyY0dioG1qK9HRqQpTU0bleIgIKMEmZxka4\
4Qad6yUiakKUFGlshAsugF279g1Tpw6Rnkt7YJIat9/eOrze9S6d6yXSkynAJLXGj1d4ifRkCjBJ\
jcsug379guf9+gWvRaTn0jEwSY1Zs+CnP1X3eREJKMAkVTpzLpmIdC9qQhQRkVRSgImISCopwERE\
JJUUYCIikkoKMBERSSUFmIiIpJICTEREUkkBJiIiqZSaADOzA83s52b2kpm9aGbvj7smERGJT5qu\
xPFN4H/c/Xwz6wsMiLsgERGJTyoCzMyqgJOB2QDuvgvYVWgeERHp3tLShDgGeBu428z+bGbfM7OB\
cRclIiLxMXePu4Z2mdlE4AngRHd/0sy+CWx29xuzppsDzAGorq6ua2hoaBnX3NxMZWVlF1bdviTW\
BMmsSzUVL4l1qabixV1XfX39YnefGFsBHeHuiX8Aw4FlkdcnAb8pNE9dXZ1HLViwwJMmiTW5J7Mu\
1VS8JNalmooXd13AIk/Adr+YRyqaEN19NbDczMaGg6YDL8RYkoiIxCwVnThCnwF+HPZAfA24JOZ6\
REQkRqkJMHdfAqSjXVZEpIvs3buXt99+m5UrV7Jy5UpWrVrF9OnTGTNmTNyllV1qAkxEpCfZu3cv\
a9eubRVMmefR16tXr+add95pNe9PfvITBZiIiJRWJpiigRR9/vLLL9Pc3Mzq1avZs2dPm/mHDRtG\
TU0NNTU1jB8/ntraWmpqaqitrW15XlNTE8Mn63oKMBGREti7dy/r1q1rFUi5QmrVqlU5g2no0KHU\
1tZSWVnJ8ccfz4gRI1rCKPN8+PDh9OvXL4ZPl0wKMBGRAjLBlG+Pqb1gGjJkSMve0RFHHNFmb6m2\
tpbhw4dTUVEBQFNTE9OmTeviT5lOCjAR6ZHcvWBTXjSYdu/e3Wb+oUOHtgTQ2LFjcwZTTU1NSzBJ\
6SnARKRbcXfWr1/fbjCtXLkyZzBl9phqamqYNm1a3mNMCqb4KcBEJBXyBVOu1zt37mwzf1VVVcux\
pJNOOol33nmH448/vuUYU6Ypr3///jF8OtkfCjARiZW7s2HDhrx7ScUEU2YPaerUqW32ljKhlR1M\
OtaUfgowESkLd2fjxo2sWrWKxYsX88Ybb+Q9l2l/ginzd8AA3Rqwp1KAiUiHuDubNm0qeGwp83zH\
jh1t5h88eHBL+Jxwwgk5jzHV1tYqmKRdCjARAdoPpujf7du3t5m/srKypbkucx5TJpDWrFnD2Wef\
TU1NDQMH6lZ+UhoKMJFuzt3ZvHlzUXtM+YIps1cU7fSQfZJtoXtYNTU1cfjhh5fzY0oPpAATSSl3\
Z8uWLW3CaOHChcydO7dVOG3btq3N/AMHDqS2tpYRI0YwefLkNsGU2XsaNGhQDJ9OpH0KMJEEyhVM\
uZ5v3bq1zbwVFRWMGjWKmpoaJk2alPcYk4JJ0k4BJtKFCgVT9G9zc3ObeQcMGNASQscddxznnHNO\
znBavHgx9fX1MXw6ka6lABMpgebm5oJ7TIWCqX///i3NdxMmTOCss85qc4wps8dkZu3WUsw0It2B\
AkykgK1bt+YNphdeeIFt27axatUqtmzZ0mbefMEUvRxRbW0tgwcPVuiI7AcFmPRIW7duzXkZouw9\
ps2bN7eZt6KiouU8pWOOOYYzzzwzZ888BZNIeSnApFvJFUy59qByBVO/fv1ajiW9733v44wzzsh5\
jOnAAw/EzHQpIpGYKcAkFXbs2MGrr77abjBt2rSpzbz9+vVrCaHx48dz+umnt2rKy4RTJphEJB0U\
YBKr7du3F7x7beb5xo0b28ybCabMrdVPP/30Nre8qK2tZciQIQomkW4oVQFmZr2BRcCb7n5O3PVI\
ftnBlC+kNmzY0Gbevn37tgTQUUcdxfTp09m+fTsnnXRSq2NMCiaRni1VAQZcBbwIDI67kJ5qx44d\
Re0x5QqmPn36tLqDbX19fc5jTAcddFCbYNLxJhHJlpoAM7ORwNnAzcBnYy6n29m5c2dLCD3yyCM8\
++yzOXvmtRdMRxxxBPX19a0uRZQJp6FDh9KrV68YPp2IdEepCTDgVuBaQNe/6YCdO3eyevXqViGU\
6+oP69evbzNvnz59GD58OLW1tbz3ve/llFNOydldXMEkInEwd4+7hnaZ2TnAWe7+aTObBnw+1zEw\
M5sDzAGorq6ua2hoaBnX3Nxc8GrZcehMTbt27WL9+vWsW7eu5bF27VrWr1/f6m+u7uK9e/dm6NCh\
HHTQQQwbNqzV82HDhtG/f39GjRrF4MGDExNM3e37K6ck1qWaihd3XfX19YvdfWJsBXRAWgLsK8DH\
gD1ABcExsF+6+0fzzTNx4kRftGhRy+skHkPJVdOuXbta9pgKnWS7bt26Nsvr3bt3yx5Tvtuq19TU\
cPDBBxcMprSsq7glsSZIZl2qqXhx12VmqQmwVDQhuvv1wPUAkT2wvOGVRLt3724TTI8//jg//OEP\
WwXT2rVr28zbu3dvqqurGTFiBIcddhgnnnhim67itbW1DBs2jN69e8fw6UREul4qAizJMsFU6Ori\
K1eu5O23324zb69evVpCaPTo0Zxwwgk5b3uhYBIRaSt1AebuTUBTud9n9+7drFmzpuAtLzLBlN0M\
26tXL4YPH05NTQ3vete7mDJlSqtgyoTT888/z/Tp08v9UUS6RiPwEDADmBVzLdIjpC7Ayu2rX/0q\
3/jGN/IGU3V1dcsxpcmTJ+c83nTIIYcUtcf00ksvletjiHStRuAiYBtwN3Av+0KsEdgU/lWwSQkp\
wLK8+93v5txzz23TjJcJpgMO0CoTaeMhgvAi/PsQQVhlgu1fgMtpHWwinaStcZYLLriACy64IO4y\
RNJlBsGe1zZgQPga8gebSAkk4yQfEUm3WQR7V1fQei9rBkGgQetgy2gErgz/inSQ9sBEpDRm0Xbv\
KhNsm2jbfFjouJlIEbQHJiLlNQsYRdtwytW8KNIBCjARiUd7zYsi7VAToojEI9O8qHPHZD8pwESk\
NDInMlcRHPPKhFKh88ByHTcTKZICTEQ6L9ohI+Nu4GqCGyEVcx6YruQhHaRjYCLSedEOGRnbCEKp\
mI4amQD8TvhX3eqlCAowkbRJ4rlT0Q4ZGQMI9qSK6aihHomyH9SEKJImST13KtohI/sY2PHkPg8s\
Kt+VPEQKUICJpEmSL82Ur0PGLIL7R0xrZ171SJQOUoCJpEl33lNRj0TpIAWYSJrEvaeinoKSIAow\
kbSJa08lqcffpMdSL0QRKY56CkrCKMBEpDi5rl1Yii79STwtQFJBTYgiUpzs42/QfpNi5pjZ6XmW\
qWZJ6QTtgYlI8WYB3w7/ttekGL26xmvk3sNSs6R0QioCzMxGmdkCM3vBzJ43s6virkmkx2vvdijR\
cNpL7nDSLVWkE9LShLgH+Jy7P2Vmg4DFZjbf3V+IuzCRHqu9Lv3Rc9Z6kTuc4j4tQFItFQHm7quA\
VeHzLWb2IjACUICJxKlQl/5oOB3WznQKLtkPqWhCjDKz0cAE4MmYSxGR9mSOmVXFXYh0R+bucddQ\
NDOrBB4Bbnb3X+YYPweYA1BdXV3X0NDQMq65uZnKysquKrUoSawJklmXaipeEutSTcWLu676+vrF\
7j4xtgI6wt1T8QD6AL8FPlvM9HV1dR61YMECT5ok1uSezLpUU/HKXtc8d78i/FvkuCSuqyTW5B5/\
XcAiT8A2v5hHKpoQzcyAO4EX3f0/4q5HpMcqdOPJ6LgPAjd0eXXSw6SiEwdwIvAx4FkzWxIO+6K7\
PxBfSSLdXOYk5Oj9vQrdziU67h3gFoJ7gamDhpRJKgLM3R8DLO46RLqVQleWj14hI+Nu4GqC87Vy\
3c5lBjCXILwgOPklSfcrk24nFU2IIlJihZoCofXeVMY29t1Z+QraXvZpFvAF9v23WCcmS5kpwER6\
ovYu4TQD6Js1rC/79tYyl5PKdjPwC3IHXGfogr+SgwJMpCdq7xJOs4DTsoadRnGBVCjg9kd7e4vS\
YynARHqizFUyCu0pXUbrkLtsP9+rEVjO/gePLvgreSjARHqq9vaUigm59mT2nt5i//eedMFfySMV\
vRBFJCaFrlNYqBdjRqFu9x2pQRf8lRwUYCLSccXeiDJzRXro3N6TLvgrOagJUUQ6rtjjUpm9p0PQ\
3Zal5BRgItJxHTkuNQsYhcJLSk5NiCLScTouJQmgABOR/aPjUhIzNSGKSEBXu5CUUYCJiK52Iamk\
ABMRXe1CUkkBJiK62oWkkjpxiEjHexUWcxUOkTJTgIlIoNhehcVehUOkzNSEKCIdo+NlkhAKMBHp\
GB0vk4RQE6KIdIyuwiEJoQATkY7TVTgkAVLThGhmM83sZTNbambXxV2PiIjEKxUBZma9Ca4RcCZw\
FHCRmR0Vb1UiIhKnVAQYMBlY6u6vufsuoAE4N+aaRCQXXVNRukhaAmwEsDzyekU4TESSRNdUlC5k\
7h53De0ys/OBme7+yfD1x4Dj3f3KrOnmAHMAqqur6xoaGlrGNTc3U1lZ2XVFFyGJNUEy61JNxYu1\
ruXAW5HXhwCjkrmuklgTxF9XfX39YnefGFsBHeHuiX8A7wd+G3l9PXB9oXnq6uo8asGCBZ40SazJ\
PZl1qabixVrXPHcf4MG/wgHha0/mukpiTe7x1wUs8gRs94t5pKUb/f8C7zGzMcCbwIXAh+MtSUTa\
0Dli0oVSEWDuvsfMrgR+C/QG7nL352MuS0Ry0Tli0kVSEWAA7v4A8EDcdYiISDKkpReiiIhIKwow\
ERFJJQWYiIikkgJMRERSSQEmIiKppAATEZFUUoCJiEgqKcBERCSVFGAiIpJKCjAREUklBZiIiKSS\
AkxERFJJASYiIqmkABMRkVRSgImISCopwEREJJUUYCIikkoKMBERSSUFmIiIpJICTEREUkkBJiIi\
qZT4ADOzfzezl8zsGTO738wOjLsmERGJX+IDDJgPjHf3o4G/ANfHXI+IiCRA4gPM3R9y9z3hyyeA\
kXHWIyIiyZD4AMtyKfBg3EWIiEj8zN3jrgEzexgYnmPUDe4+L5zmBmAi8EHPU7SZzQHmAFRXV9c1\
NDS0jGtubqaysrLUpXdKEmuCZNalmoqXxLpUU/Hirqu+vn6xu0+MrYCOcPfEP4DZwOPAgGLnqaur\
86gFCxZ40iSxJvdk1qWaipfEulRT8eKuC1jkCdjuF/M4IM7wLIaZzQSuBU5x921x1yMiIsmQhmNg\
3wYGAfPNbImZzY27IBERiV/i98Dc/fC4axARkeRJwx6YiIhIGwowERFJpUR0oy8HM3sb+Gtk0DBg\
bUzl5JPEmiCZdamm4iWxLtVUvLjrOtTdD47x/YvWbQMsm5kt8oSd25DEmiCZdamm4iWxLtVUvKTW\
lURqQhQRkVRSgImISCr1pAD7btwF5JDEmiCZdamm4iWxLtVUvKTWlTg95hiYiIh0Lz1pD0xERLqR\
bhVgZnaBmT1vZnvNbGLWuOvNbKmZvWxmZ+SZf4yZPRlOd5+Z9S1xffeFl8NaYmbLzGxJnumWmdmz\
4XSLSllDnve7yczejNR2Vp7pZobrb6mZXVfmmoq6E3dXrKv2PreZ9Qu/26Xh72d0OeqIvN8oM1tg\
Zi+Ev/erckwzzcw2Rb7TfyxnTZH3Lfh9WOC2cF09Y2bHlbmesZF1sMTMNpvZ1VnTdMm6MrO7zOwt\
M3suMmyomc03s1fCv0PyzHtxOM0rZnZxOepLpbivJlzKB3AkMBZoAiZGhh8FPA30A8YArwK9c8z/\
U+DC8Plc4FNlrPX/A/+YZ9wyYFgXrrebgM+3M03vcL0dBvQN1+dRZaxpBnBA+PxrwNfiWFfFfG7g\
08Dc8PmFwH1l/r5qgOPC54MI7lSeXdM04Ndd9Rsq9vsAziK4p58BU4Anu7C23sBqgvOcunxdAScD\
xwHPRYbdAlwXPr8u1+8cGAq8Fv4dEj4f0tXfbRIf3WoPzN1fdPeXc4w6F2hw953u/jqwFJgcncDM\
DDgV+Hk46PvAeeWoM3yvvwHuLcfyy2QysNTdX3P3XUADwXotC0/OnbiL+dznEvxeIPj9TA+/47Jw\
91Xu/lT4fAvwIjCiXO9XYucCP/DAE8CBZlbTRe89HXjV3f/a7pRl4O6PAuuzBkd/O/m2OWcA8919\
vbtvAOYDM8tVZ5p0qwArYASwPPJ6BW3/wR8EbIxsNHNNUyonAWvc/ZU84x14yMwWhzfp7ApXhk06\
d+VpxihmHZZLoTtxl3tdFfO5W6YJfz+bCH5PZRc2V04Answx+v1m9rSZPWhm47qiHtr/PuL8HV1I\
/v80xrGuAKrdfVX4fDVQnWOaONdZoiX+avTZrIi7N8epyPouovDe11R3f9PMDiG4jcxL4f/eylIX\
8F/Alwk2Pl8maN68tDPv19mavPWduPcAP86zmJKvq7Qws0rgF8DV7r45a/RTBE1lzeExzV8B7+mC\
shL5fYTHs2cB1+cYHde6asXd3czULbwDUhdg7n7afsz2JjAq8npkOCxqHUFzxgHh/6JzTdPp+szs\
AOCDQF2BZbwZ/n3LzO4naMbq1Eag2PVmZncAv84xqph1WNKazGw2cA4w3cODATmWUfJ1laWYz52Z\
ZkX4/VYR/J7Kxsz6EITXj939l9njo4Hm7g+Y2X+a2TB3L+s19or4Pkr+OyrSmcBT7r4me0Rc6yq0\
xsxq3H1V2JT6Vo5p3iQ4TpcxkuA4f4/XU5oQG4ELw95iYwj+d7UwOkG4gVwAnB8Ouhgoxx7dacBL\
7r4i10gzG2hmgzLPCTozPJdr2lLJOgbxgTzv97/AeyzoqdmXoDmmsYw1Ze7EPcvz3Im7i9ZVMZ+7\
keD3AsHv5/f5ArcUwuNrdwIvuvt/5JlmeOY4nJlNJvi3Xu5QLeb7aAQ+HvZGnAJsijShlVPeVo84\
1lVE9LeTb5vzW2CGmQ0Jm/dnhMMk7l4kpXwQbHxXADuBNcBvI+NuIOhN9jJwZmT4A0Bt+PwwgmBb\
CvwM6FeGGu8BLs8aVgs8EKnh6fDxPEFzWrnX2w+BZ4FnCP5B1WTXFb4+i6DH26vlriv8DpYDS8LH\
3Oyaumpd5frcwL8QhCtARfh7WRr+fg4r87qZStDc+0xk/ZwFXJ75bQFXhuvkaYJOMCd0we8o5/eR\
VZcB3wnX5bNEeguXsa6BBIFUFRnW5euKIEBXAbvD7dQnCI6V/g54BXgYGBpOOxH4XmTeS8Pf11Lg\
knKvs7Q8dCUOERFJpZ7ShCgiIt2MAkxERFJJASYiIqmkABMRkVRSgImISCopwEREJJUUYCIikkoK\
MBERSSUFmIiIpJICTEREUkkBJiIiqaQAExGRVFKAiYhIKinAREQklRRgIiKSSgowERFJJQWYiIik\
kgJMRERSSQEmIiKppAATEZFU+j9FMoXMOz9vUQAAAABJRU5ErkJggg==\
"
  frames[4] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAAsdklEQVR4nO3de5gU5Zn38e8tZxgYxcNwFFGzrMoayCDRjRpAg0TzIkazKzGJ\
xGSRRBPN5qQvcTe7CZuN67sxJtnFGJUcXMacXCYeNooZYkyCLCgaPKP2yBmNCgxn5H7/qGqoabpn\
ema6u6p6fp/r6ovuqurqu6t7+kc99TxV5u6IiIikzWFxFyAiItIZCjAREUklBZiIiKSSAkxERFJJ\
ASYiIqmkABMRkVRSgImISCopwEREJJUUYCIikkoKMBERSSUFmIiIpJICTEREUkkBJiIiqaQAExGR\
VFKAiYhIKinAREQklRRgIiKSSgowERFJJQVYipmZm9mJZVr3WWb2fInWlTGzc0u0rrK952pkZk+b\
2aQyrHeWmT1a6vWWg74z1UsBlhBmdr2ZPZAz7cUC0y4tweu1GSru/jt3H9OJ9S4ws693rbrKSmPN\
+eR7H+5+irsviamkbsPM3mlmD5nZG2a2xsxmxV1Td6AAS45HgL82sx4AZjYU6AWMz5l2YrisdCNm\
1jPuGpIo+7eRACOB7wFDgb8FbjWzYfGW1A24u24JuAG9gR1Affj4b4A7gd/mTFsdeY4Dc4AXgbcI\
/oAsnHcC8Bvgz8DrwF3A4eG8HwP7gZ1AC/ClPPVMAtZGHn8ZWAdsA54HzsnznNnAXmBPuN5fhdMz\
wBeAp4AtwN1A38jzPgCsDN/DH4BT29hODnwWeDl8X/8GHBaZfwXwLPAm8GtgVDjdgG8Bm4GtwJ+A\
sYVqznnNvM8N5/UBbgJeBTYB84F+0W0I/N+w1gxwWWS9FwBPhOtcA3w1Mu+48L1+Ilz3I+H0nwEb\
w+34CHBKEdv+3PD+V4GfAj8KP8engQmR13xXWM+28HXuBr5e4HOYBTwavvc3gVeA94fzPgSsyFn+\
74FF4f0F4XZ6KHyt32Y/p3D+X4bz3iD4rv1NZN4C4D+B+4HtwLlFrM+BEzuwzS8Pt/nrwNzI/MOA\
64CXCP6ufgoMzrNteoafwV/F/btS7bfYC9At8mFAE/C58P53CX6M5+VMuyOyvAP3AocDxwKvAdPC\
eScC7yP4gT06/LG7OfLcAz9sBWqZRBhgwJjwj31Y+Pg44IQCz1uQ+6MXvtYyYBgwmCBg5oTzxhME\
w7uBHuGPRwboU2D9Hm6nweF7fgH4ZDjvQmA1cFL4I/IV4A/hvPOAFeG2snCZoYVqznnNtp77LaAx\
rGcg8CvgG5FtuA/49/BzeC/Bj+6YyPy/Cn8YTyUIwBmRbewEYTOAg6F4Rfg6fYCbgZVFbPtogO0C\
zg+39TeApeG83kAzcA3Bnv8HCcKwrQDbC/xduK5PAevD7dOHIHxOiiz/BHBxpM5twNnhst8GHg3n\
DSD4rn08/AzHEwTJyZHnbgHeE263vm2tL/KdObED2/w2oB/wTmB39n2E22YpMCJ8nVuBhXm2zS0E\
3/fD8m073Ur4mxl3AbpFPozgB+ae8P6TwDuAaTnTLo8s78CZkcc/Ba4rsO4ZwBORxwd+2AosP4mD\
AXYiQcicC/Rq5z0syP3RC1/rI5HHNwLzw/v/CXwtZ/nngfcWWL8ThnT4+NPAw+H9B4BPROYdRrBX\
OwqYQhB2p+f+sOSrOWd+3ucS/FhvJxLmwBnAK5FtuA8YkPMZ3VDgdW4GvhXez/6YHt9GXYeHy9S2\
s+2jAbY4Mu9kYGd4/2yCPWyLzH+00HYhCLBoa0D/sJYhkc91Xnj/FIK9tD6ROhsiz60B3iZohvtb\
4Hc5r3Ur8I+R5/4oz+eXd32R78yJHdjmIyLzlwGXhvefJdLyQNBcuBfoGZn2JYLv75C2/k50K81N\
x8CS5RHgTDMbDBzt7i8SNKn9dThtLIce/9oYub+D4I8XM6szswYzW2dmW4GfAEd1pih3Xw1cS/AD\
uDlcb0fb9/PWSRAunzezt7I3gh+ytta/JnK/ObLsKODbkfW8QRAyw939NwR7sN8L38P3zWxQMYW3\
8dyjCX64V0Re83/C6Vlvuvv2fPWa2bvNrMnMXjOzLQTNwbmf0YH3amY9zOxfzeyl8DPNhLM68rnm\
fg59w+Nrw4B1Hv4K5752e+ty9x3h3ezn+kPgw2ZmwEeBn7r77nzrdvcWgs9qGMFn+O6c78NlwJB2\
6iq0vlaK3OZtfVfvidT1LEFQ1kWWvxb4O3ePrkPKRAGWLH8EagmaZX4P4O5bCZpm/g5Y7+6vFLmu\
fyH43+Rfufsg4CMEP+ZZnvdZBbj7f7n7mQR/xA58s9CiHVkvwQ/PPHc/PHLr7+4L23jOyMj9Ywm2\
T3ZdV+asq5+7/yF8D7e4ez3BnsdfAF8stuYCz32d4DjiKZHXq3X3mshTjzCzAQXq/S+C5seR7l5L\
cBwn+hnl1vZhgmbScwm+J8eF0y3Psh21ARgeBk7WyEILt8fdlxI0QZ5FUPePcxY5sG4zqyFogl1P\
8Bn+NuczrHH3T0VXn+clC60vVzHbvJA1BMf5orX1dfd1kWWGFnhdKQMFWIK4+05gOcEB799FZj0a\
TutI78OBBAeSt5jZcA7+WGdtAo4vZkVmNsbMpphZH4JjKDsJOoHkU/R6Q7cBc8L/GZuZDTCzC8xs\
YBvP+aKZHWFmIwmOS9wdTp8PXG9mp4R115rZh8L7p4Wv0Yug2W9X5D20WXOh57r7/rD+b5nZMeGy\
w83svJxV/JOZ9Tazswg6rPwsnD4QeMPdd5nZRIIf+rYMJDgm82eCPb9/yZnf0W0f9UeCvYmrzayn\
mV0ITOzkurJ+RLDnutfdc8eMnW9mZ5pZb+BrBMfi1hAc0/0LM/uomfUKb6eZ2UntvFah9eXq6DaP\
mg/MM7NRAGZ2dLidooYSdGiRClCAJc9vgWMIQivrd+G0jgTYPxH0KtsC3Af8Mmf+N4CvhM0hX2hn\
XX2AfyXY49gY1nJ9gWVvB04O1/vf7RXp7ssJ9i6/S3CcZDXB8ZW2LCLoVLGS4L3dHq7rHoI9w4aw\
iW0V8P7wOYMIwuZNgma8PxP0YCym5rae++Ww5qXhay4m6PSStTF83nqCnqBz3P25cN6ngX82s23A\
PxAcH2vLj8LXXwc8Q9ChIKpD2z7K3fcQdNz4BEFv0I8QhMnuNp7Wnh8TNHv/JM+8/wL+kaCprz58\
Pdx9GzAVuJRgm20k+Ez7tPNaedeXR0e3edS3CfbeHgyfv5Sg81HUaoJWCqmAbJdrESkxC86A8RN3\
HxFzKZ1iZo8RdLa5s5PP70fQ+edd4fHc7PQFBB2EvlKiOku6PkkP7YGJCABm9l4zGxI2IV5O0M38\
f7qwyk8B/xsNL5FSStTofjO7g+AYwWZ3HxtO+zfg/xAcEH4J+Li7vxVbkSLVawxBk9oAgoHil7j7\
hs6syMwyBJ0jZpSqOJFciWpCNLOzCToe/CgSYFOB37j7PjP7JoC7fznGMkVEJAES1YTo7o8QHISN\
TnvQ3feFD7Oj4EVEpJtLVIAV4QqCsy2IiEg3l6hjYG0xs7kEp+W5q41lZhOc1JR+/frVjxx5cBzm\
/v37OeywZOV1EmuCZNalmvJ76aWXqKmpoa7u4MkgklBXLtVUvLjreuGFF15396PbXzIB4j6XVe6N\
4OwCq3KmzSIYaNm/2PXU19d7VFNTkydNEmtyT2ZdqulQ27dvd8DnzZvXanrcdeWjmooXd13Ack9A\
FhRzS/wemJlNIzhB5nv94PnWRLq95uZmAI477rh4CxGJSaL2n81sIcGe1hgzW2tmnyA4Q8NA4CEz\
W2lm82MtUiQhMpkMoACT7itRe2DuPjPP5NsrXohICijApLtLVICJSPEymQy9e/dmyJAh7S8co717\
91JTU8Ozzz4bdymt1NbWJq4mqFxdffv2ZcSIEfTq1avsr1UuCjCRlMpkMowaNSqRPemi1q5dS11d\
HSNGjKD11VritW3bNgYObOuiB/GoRF3uzp///GfWrl3L6NGjy/pa5ZTsb76IFJTJZFLRfLhr1y5q\
a2sTFV7dnZlx5JFHsmvXrrhL6RIFmEhKZffA0kDhlTzV8JkowERSaOfOnWzevDkVe2BJUFNT0+rx\
ggUL+PznPx9TNUHHm9dff71s689kMvTr149x48Yxbtw45syZU7bXipOOgYmkkMaAJdu+ffvo2TPe\
n9cTTjiBlStXxlpDuWkPTCSF1IW+NLZt28bo0aPZu3cvAFu3bj3weNKkSVxzzTWMGzeOsWPHsmzZ\
MgC2b9/OFVdcwcSJExk/fjyLFi0Cgr266dOnM2XKFM455xyWLFnC2WefzQUXXMCYMWOYM2cO+/fv\
P6SGGTNmUF9fzymnnML3v//9A9NramqYO3cu73znOzn99NPZtGkTAK+99hoXX3wxp512Gqeddhq/\
//3vy72ZEkt7YCIplNYAu/baa0u+VzBu3DhuvvnmNpfZuXMn48aNO/D4jTfeYNq0aQwcOJBJkyZx\
3333MWPGDBoaGvjgBz94oGv5jh07WLlyJY888ghXXHEFq1atYt68eUyZMoU77riDt956i4kTJ3Lu\
uecC8Pjjj/PUU08xePBglixZwrJly3jmmWcYNWoU06ZN45e//CWXXHJJq9ruuOMOBg8ezM6dOznt\
tNOYOnUqAwcOZPv27Zx++unMmzePL33pS9x222185Stf4ZprruFzn/scZ555Jq+++irnnXde3m73\
r7zyCuPHj2fQoEF8/etf56yzzurahk4gBZhICmUyGXr16sXQoUPjLiUV+vXr1yo4FyxYwB/+8AcA\
PvnJT3LjjTcyY8YM7rzzTm677bYDy82cGZxb4eyzz2br1q289dZbPPjggzQ2NnLTTTcBQS/LV199\
FYD3ve99DB48+MDzJ06cyPHHH39gXY8++ughAXbLLbdwzz33ALBmzRpeeukljjvuOHr37s0HPvAB\
AOrr63nooYcAWLx4Mc8888yB52/dupWWlpZWx/mGDh3Kq6++ypFHHsmKFSuYMWMGTz/9NIMGDerC\
VkweBZhICqVlDFiu9vaU4vCe97yHTCbDkiVLePvttxk7duyBebk99cwMd+cXv/gFY8aMaTXvscce\
Y8CAAYcs39bjJUuWsHjxYv74xz/Sv39/Jk2axO7duwHo1avXgeV79OjBvn3BZRH379/P0qVL6du3\
b8H31KdPH/r06QME4XfCCSfwwgsvMGHChHa3R5qk69svVaexEa6+OvhXipeWMWBp8bGPfYwPf/jD\
fPzjH281/e677wbg0Ucfpba2ltraWs477zy+853vZK+UwRNPPFFwvcuWLeOVV15h//793H333Zx5\
5pmt5m/ZsoUjjjiC/v3789xzz7F06dJ2a506dSrf+c53DjzO1yT72muv8fbbbwPw8ssv8+KLLx7Y\
E6wmCjCJTWMjzJwJ3/te8K9CrHgKsNK67LLLePPNNw80GWb17duX8ePHM2fOHG6/PTgt6w033MDe\
vXs59dRTOeWUU7jhhhsKrve0007j6quv5qSTTmL06NFcdNFFreZPmzaNffv2cdJJJ3Hddddx+umn\
t1vrLbfcwvLlyzn11FM5+eSTmT//0PObP/LII5x66qmMGzeOSy65hPnz57dq2qwWakKU2Dz4IOwI\
L5CzY0fwePr0eGtKg507d7Jp06bUDGJOgpaWllaPZ82axcUXX3zgcfbY1OGHH95quY985COHNHv2\
69ePW2+99ZDXmDVrFrNmzWo1bdCgQdx7772HLJvthAPwwAOtLzK/bdu2Q2q+5JJLDhw7O+qoow7s\
GRZy8cUXt3p/1UoBJrGZOhXuvDMIr/79g8fSvmyHAe2BlcZnPvMZHnjgAe6///64S5EOUoBJbKZP\
h4ULgz2vqVO191WstHahT6ro8aSoJUuWdGm9kyZNYtKkSV1ah7RNASaxmj5dwdVR2QBTE6J0d+rE\
IZIy2TFgw4YNi7uUomV77ElyVMNnogATSZlMJsOxxx5Ljx494i6lKH379mXLli1V8YNZLbLXA2tr\
LFkaqAlRJGXS1oV+xIgRPPnkk4f0BIzbrl27EvkDXqm6sldkTjMFmEjKZDIZLrjggrjLKFqvXr1o\
aWlJ3FkglixZwvjx4+Mu4xBJrSuJ1IQokiI7d+5k48aNqdoDEymXxAWYmd1hZpvNbFVk2mAze8jM\
Xgz/PSLOGqX0dEqp4mgMmMhBiQswYAEwLWfadcDD7v4O4OHwsVQJnVKqeBoDJnJQ4gLM3R8B3siZ\
fCHww/D+D4EZlaxJyivfKaUkv+yVmDUGTCSBAVZAnbtvCO9vBOriLEZKa+rU4FRSoFNKtSeTydCz\
Z89UjQETKRdL4tgMMzsOuNfdx4aP33L3wyPz33T3Q46DmdlsYDZAXV1dfUNDw4F5uRd8S4Ik1gTx\
1LVlC2zdCoMGQW1tMmpqTxw1fe1rX+O5557jrrvuKriMtlVxklgTxF/X5MmTV7h7srqMFuLuibsB\
xwGrIo+fB4aG94cCz7e3jvr6eo9qamrypEliTe7JrKtcNS1a5H7VVcG/HRXHdjrjjDN8ypQpbS7T\
nT6/rkhiTe7x1wUs9wTkQDG3tDQhNgKXh/cvBxbFWItUiTR2HknbIGaRckpcgJnZQuCPwBgzW2tm\
nwD+FXifmb0InBs+lpSLu+t82jqP7Nq1iw0bNijAREKJOxOHu88sMOucihYiZZXd+9mxI7gm2LXX\
BsfBKnlZlbRdj0xjwERaS1yASfeQu/dz442wb18QKAsXVibE0nY9Mo0BE2ktcU2I0j1Eu8736BGE\
F1S+KW/6dPjud5MfXqAAE8mlAJOyKnScK7v3c9VV8OUvaxxYMTQGTKQ1NSFK2USPc/3gB3DOOXDl\
lQf3dqJXY373u9PTlBeXtF0HTKTcFGBSco2NQRi98srB41y7d8P998OSJfmPcUXDTPJrbm7WKaRE\
ItSEKCUVHVu1eDH06dN6frmPccXdNb+cNAZMpDUFmJRUtHfhnj1Bs+H550Pv3sG0fMe4ShU6aRyY\
XKzdu3ezfv16BZhIhAJMSir3xLxXXgn33Qc/+1nQYSO3+bCUoZO2gckdoTFgIodSgElJRXsXRsOq\
UHf1UoZONZ/VXl3oRQ6lThxSch3pkFHKs2GkbWByRyjARA6lAJNYlTp0qrU3YyaToUePHhoDJhKh\
AJPY5YZOtht+ufeiKvU6pZAdA9azp/5kRbL01yCxyg2RQoOfBw0q/etGTyZcqfMvdpa60IscSp04\
JDb5eiBGO3VkBz/PnBmcqT73uV3pep+2HosKMJFDKcAkNrkhMncu1NYe7EmYtWMHbN168HEput6n\
qceixoCJ5KcAk9hEQwRg1Sq4+ebg2mC5g5+jTYil2Hsq1N0/idasWQOoB6JILgWYdFpXm/GyITJ2\
7MFpO3YEzYW5g59raw8uU6q9p7RcSiXbhV7nQRRpTZ04pFNK1Qki+5zsuqKBFO2duGRJ6+dU63iv\
fDQGTCQ/BZh0SqFmvM6ESmcCqVrHe+WTHQM2fPjwuEsRSRQFmHRK7hk0amu7tkfWnQKpozKZDCNH\
jtQYMJEcOgYmnZLbCWLLlnR1S08TdaEXyS81AWZmnzOzp81slZktNLO+cdfU3UU7QaSpW3raKMBE\
8ktFgJnZcOCzwAR3Hwv0AC6NtyqJSlO39DTRGDCRwtLUqN4T6Gdme4H+wPqY65Ec5TqO1dgYNFE2\
Nna/YFyzZg3urgATySMVe2Duvg64CXgV2ABscXcdZekGst31N2+uvqssF0Nd6EUKM3ePu4Z2mdkR\
wC+AvwXeAn4G/Nzdf5Kz3GxgNkBdXV19Q0PDgXktLS3U1NRUquSiJLEmSFZda9YE4TViRAtr19Zw\
zDEwcmTcVQUqsZ3uu+8+brrpJhYuXMiQIUMSU1dHqabixV3X5MmTV7j7hNgK6Ah3T/wN+BBwe+Tx\
x4D/aOs59fX1HtXU1ORJk8Sa3JNV16JF7v37u990U5P37x88LuY5V11V3LJdUYntNHfuXO/Ro4fv\
3bu36Ock6fPLUk3Fi7suYLkn4He/mFsqmhAJmg5PN7P+ZmbAOcCzMdckFZDtHHLMMcV1DinFiX6T\
pLm5mREjRmgMmEgeqQgwd38M+DnwOPAngrq/H2tRUjHTpwfNhsV04EjbZVLaoy70IoWlIsAA3P0f\
3f0v3X2su3/U3XfHXZN0TFdO/rtlS3HPrbbxaAowkcJSE2CSbl1p2mtshJdfLu651TQebc+ePaxb\
t04BJlKAAkwqoitNew8+CPv3F//ctFwmpT0aAybSNgWYVERXmvamToXDDuvcc9NMY8BE2qauTVIR\
XbmG1/TpsGhR0CzYHa7/laUAE2mbAkwqpiunmqqtDZoFsxobq/+CltnrgI0YMSLuUkQSSU2IkjrV\
NtarkEwmw/DhwzUGTKQABZikSmMjzJ1bXWO9CslkMowePTruMkQSSwEmqdHYCB/6EKxadXBaNXfq\
0BgwkbYpwCQ1br0V9uw5+PjYY9M/1quQ7BiwUaNGxV2KSGIpwCS1xo6tzvACjQETKYYCTFLjyiuh\
T5/gfp8+weNqpS70Iu1T9yZJjenT4ac/rf7u8xCchR4UYCJtUYBJqnRlLFmaZDIZDjvsMI0BE2mD\
mhBFEiiTyTBixAh69eoVdykiiaUAE0kgdaEXaZ8CTCSBFGAi7VOAiSSMrgMmUhwFmEjCrF27lv37\
9yvARNqhABNJGI0BEymOAkwkYbIBptNIibRNASaSMBoDJlKc1ASYmR1uZj83s+fM7FkzOyPumkTK\
IXsdsN69e8ddikiipelMHN8G/sfdLzGz3kD/uAsSKQd1oRcpTir2wMysFjgbuB3A3fe4+1uxFiVS\
JgowkeKkIsCA0cBrwJ1m9oSZ/cDMBsRdlEipaQyYSPHM3eOuoV1mNgFYCrzH3R8zs28DW939hpzl\
ZgOzAerq6uobGhoOzGtpaaGmpqaCVbcviTVBMuvqLjWtX7+eyy67jC9+8Yucf/75iamrq1RT8eKu\
a/LkySvcfUJsBXSEuyf+BgwBMpHHZwH3tfWc+vp6j2pqavKkSWJN7smsq7vU9PDDDzvgDz/8cKfX\
0V22VVclsSb3+OsClnsCfveLuaWiCdHdNwJrzGxMOOkc4JkYSxIpCw1iFilemnohfga4K+yB+DLw\
8ZjrESk5jQETKV5qAszdVwLpaJcV6aRMJsOwYcM0BkykCKloQhTpLpqbmxk9enTcZYikggJMJEE0\
BkykeAowkYTYu3cva9eu1Ul8RYqkABNJCF0HTKRjFGAiCaEu9CIdowATSQgFmEjHKMBEEiKTyWBm\
jBw5Mu5SRFJBASaSELoOmEjHKMBEEkJd6EU6RgEmkhAKMJGOUYCJJEB2DJgCTKR4CjCRBNAYMJGO\
U4CJJEBzczOgLvQiHaEAE0kAjQET6TgFmEgCaAyYSMcpwEQSQGPARDpOASaSAOpCL9JxCjCRBFCA\
iXScAkwkZvv27dMYMJFOUICJxGzt2rW8/fbbCjCRDlKAicRMXehFOidVAWZmPczsCTO7N+5aREpF\
ASbSOakKMOAa4Nm4ixApJY0BE+mc1ASYmY0ALgB+EHctIqXU3NzMsGHDNAZMpINSE2DAzcCXgP0x\
1yFSUplMhlGjRsVdhkjqmLvHXUO7zOwDwPnu/mkzmwR8wd0/kGe52cBsgLq6uvqGhoYD81paWqip\
qalMwUVKYk2QzLqquaaZM2cyduxY5s6dW4KqqntblVISa4L465o8efIKd58QWwEd4e6JvwHfANYC\
GWAjsAP4SVvPqa+v96impiZPmiTW5J7Muqq1pr1793qPHj187ty5XS8oVK3bqtSSWJN7/HUByz0B\
v/vF3FLRhOju17v7CHc/DrgU+I27fyTmskS6bN26dRoDJtJJqQgwkWqlLvQinZe6AHP3JZ7n+JdI\
GlVVgDUCV4f/ilRA6gJMpJpUzRiwRmAm8L3w38aceWtQsEnJKcBEYpTJZBg2bBh9+vSJu5SueZCg\
axXhvw+G97PBtplDg02kixRgIjGqmsuoTAX6h/f7h4+hcLCJlIACTCRGVRNg04GFwFXhv9PD6YWC\
LUvHzaQLesZdgEh3tW/fPtasWVM9Z+GYzsHgik5bCGyhdbDBwebFHcCdeeaLtEN7YCIxWb9+ffcY\
AzYdGMmh4aTmRekiBZhITKqqC31ntNe8KNIONSGKxKTbB1i2efFBgvBS86F0kAJMJCbZADv22GPj\
LaRUGgnCqJbgmFc2lBrDx43kP0am4JJOUoCJxKRqxoBB6w4ZWXcC1xJcCOmfgTm03VEjG4DaG5Mi\
6RiYSEyqpgs9tO6QkbWDIJSK6ajR1pk8RApQgInEpNMBlsSxU9EOGVn9CfakiumooR6J0glqQhSJ\
QXYM2MyZMzv2xKSOnYp2yMg9BvZu8o8Di5pK8H52oB6JUjQFmEgM1q9fz759+zq+B5ZvTyUJAQaF\
O2RMB5YAk9p5rnokSgcpwERi0Oku9NW8p6IeidJBCjCRGDQ3NwOdCLC491TUU1ASRAEmEoPsHlin\
rgMW155KUo+/SbelXogiMchkMgwdOpS+ffvGXUrx1FNQEkYBJhKDVI4By3fuwlJ06U/isABJBQWY\
SAwymUz6LqOSe80vaH/wcTacthRYpwYwSxcowEQq7O233+bVV19l9OjRcZfScdOB74b/ttekGA2n\
l8kfTmqWlC5IRYCZ2UgzazKzZ8zsaTO7Ju6aRDqr02PAkqa9y6FEw2k/+cNJl1SRLkhLL8R9wOfd\
/XEzGwisMLOH3P2ZuAsT6aiquYxKe136o2PWDiN/OMU9LEBSLRUB5u4bgA3h/W1m9iwwHFCASepU\
TYBB2136o+F0fDvLKbikE1LRhBhlZscB44HHYi5FpFOq7jpgbckeM6uNuxCpRubucddQNDOrAX4L\
zHP3X+aZPxuYDVBXV1ff0NBwYF5LSws1NTWVKrUoSawJkllXNdV04403smzZMn7+85+Xoarq2lbl\
lMSaIP66Jk+evMLdJ8RWQEe4eypuQC/g18DfF7N8fX29RzU1NXnSJLEm92TWVU01TZkyxc8444zS\
FhNR9m21yN2vCv8tcl41fX7lFnddwHJPwG9+MbdUNCGamQG3A8+6+7/HXY9IVzQ3N6dvDFhWW+O2\
ovM+CMyteHXSzaQiwID3AB8FppjZyvB2ftxFiXRUdgxYKjpwZAchz+XgmTLaGrcVnfc2cCMamCxl\
lZZeiI8CFncdIl21YcMG9u7dm4wAa+vM8tET92bdCVxLMF4r3+VcpgLzCcILgsEvSbpemVSdtOyB\
iVSFxHShb+8UTtG9qawdHLyycvZ0UtFwmg58mYP/LdbAZCkzBZhIBSUmwNo7hdNUoHfOtN4c3FvL\
nk4q1zzgF+QPuK7QCX8lDwWYSAUlZgxYe6dwmg6cmzPtXIoLpLYCrjN0wl8pQAEmUkGZTIYhQ4bQ\
r1+/eAvJPbN8vrC5ktYhd2UnX6sRWEPng0cn/JUCFGAiFZSo64C1t6dUTMi1J7v3tJnO7z3phL9S\
gAJMpIISFWDFaCvkijkuVYq9p1IEqVQlBZhIhaRqDFh7ij0uVaq9p1IfV5OqoAATqZBEjQHrqmL3\
rLJ7T8egvScpOQWYSIU0NzcDpPc0UlEd2bOaDoxE4SUll4ozcYhUg8SMASsFXYhSEkABJlIh2QCr\
ij0w0IUoJXZqQhSpkEwmQ11dXfxjwArR2S4kZRRgIhWS6C70OtuFpJACTKRCMplMcpsPdbYLSSEF\
mEgF7N+/n+bmZkaPHh13KfnpbBeSQurEIVIBiR8D1tFehW1dS0ykQhRgIhWQii70xfYqjF7s8k40\
QFlioyZEkQpIRYAVS8fLJCEUYCIVUFVjwHS8TBJCTYgiFZDJZDjmmGOSOwasI3QWDkkIBZhIBTQ3\
N1dH82GWzsIhCZCaJkQzm2Zmz5vZajO7Lu56RDoi0YOYRVIqFQFmZj0IzhHwfuBkYKaZnRxvVSLF\
yY4BU4CJlFYqAgyYCKx295fdfQ/QAFwYc00iRdm4cSN79uzpPgGmcypKhaQlwIYDayKP14bTRBKv\
qrrQt0fnVJQKMnePu4Z2mdklwDR3/2T4+KPAu9396pzlZgOzAerq6uobGhoOzGtpaaGmpqZyRRch\
iTVBMutKc02LFy9m3rx5LFiwoCLd6GPdVmuAzZHHxwAj0/35VVrcdU2ePHmFu0+IrYCOcPfE34Az\
gF9HHl8PXN/Wc+rr6z2qqanJkyaJNbkns6401zRv3jwHfPv27eUtKBTrtlrk7v09+CvsHz72dH9+\
lRZ3XcByT8DvfjG3tHSj/1/gHWY2GlgHXAp8ON6SRIqTHQPWv3//9hdOO40RkwpKRYC5+z4zuxr4\
NdADuMPdn465LJGidLsu9BojJhWSigADcPf7gfvjrkOkozKZDOPHj4+7DJGqk5ZeiCKppDFgIuWj\
ABMpo02bNrFnz57qOImvSMIowETKqFuNAROpMAWYSBkpwETKRwEmUkZVdR0wkYRRgImUUSaT4eij\
j2bAgAFxlyJSdRRgImXU7caAiVSQAkykjBRgIuWjABMpk+wYMB3/EikPBZhImWzatIndu3czevTo\
uEsRqUoKMJEyURd6kfJSgImUiQJMpLwUYCJl0tzcDGgMmEi5KMBEyiSTyXDUUUdpDJhImSjARMpE\
XehFyksBJlImCjCR8lKAiZSBu+s6YCJlpgATKYNNmzaxa9cuBZhIGSnARMpAXehFyk8BJlIGCjCR\
8lOAiZSBrgMmUn6JDzAz+zcze87MnjKze8zs8LhrEmlPdgxYTU1N3KWIVK3EBxjwEDDW3U8FXgCu\
j7kekXbpLPQi5Zf4AHP3B919X/hwKTAiznpEiqExYCLll/gAy3EF8EDcRYi0xd0VYCIVYO4edw2Y\
2WJgSJ5Zc919UbjMXGAC8EEvULSZzQZmA9TV1dU3NDQcmNfS0pK44xFJrAmSWVeaanrjjTe4+OKL\
+exnP8tFF12UmLripJqKF3ddkydPXuHuE2IroCPcPfE3YBbwR6B/sc+pr6/3qKamJk+aJNbknsy6\
0lTT0qVLHfBf/epXlS0olKZtFack1uQef13Ack/A734xt54xZmdRzGwa8CXgve6+I+56RNqjMWAi\
lZGGY2DfBQYCD5nZSjObH3dBIm3RGDCRykj8Hpi7nxh3DSIdkclkOPLIIxk4cGDcpYhUtTTsgYmk\
inogilSGAkykxDKZjJoPRSogEd3oy8HMXgOaI5OOAl6PqZxCklgTJLMu1VS8JNalmooXd12j3P3o\
GF+/aFUbYLnMbLknbGxDEmuCZNalmoqXxLpUU/GSWlcSqQlRRERSSQEmIiKp1J0C7PtxF5BHEmuC\
ZNalmoqXxLpUU/GSWlfidJtjYCIiUl260x6YiIhUkaoKMDP7kJk9bWb7zWxCzrzrzWy1mT1vZucV\
eP5oM3ssXO5uM+td4vruDk+HtdLMMma2ssByGTP7U7jc8lLWUOD1vmpm6yK1nV9guWnh9lttZteV\
uaairsRdiW3V3vs2sz7hZ7s6/P4cV446Iq830syazOyZ8Pt+TZ5lJpnZlshn+g/lrCnyum1+Hha4\
JdxWT5nZu8pcz5jINlhpZlvN7NqcZSqyrczsDjPbbGarItMGm9lDZvZi+O8RBZ57ebjMi2Z2eTnq\
S6W4zyZcyhtwEjAGWAJMiEw/GXgS6AOMBl4CeuR5/k+BS8P784FPlbHW/wf8Q4F5GeCoCm63rwJf\
aGeZHuF2Ox7oHW7Pk8tY01SgZ3j/m8A349hWxbxv4NPA/PD+pcDdZf68hgLvCu8PJLhSeW5Nk4B7\
K/UdKvbzAM4nuKafAacDj1Wwth7ARoJxThXfVsDZwLuAVZFpNwLXhfevy/c9BwYDL4f/HhHeP6LS\
n20Sb1W1B+buz7r783lmXQg0uPtud38FWA1MjC5gZgZMAX4eTvohMKMcdYav9TfAwnKsv0wmAqvd\
/WV33wM0EGzXsvDkXIm7mPd9IcH3BYLvzznhZ1wW7r7B3R8P728DngWGl+v1SuxC4EceWAocbmZD\
K/Ta5wAvuXtzu0uWgbs/AryRMzn63Sn0m3Me8JC7v+HubwIPAdPKVWeaVFWAtWE4sCbyeC2H/sEf\
CbwV+dHMt0ypnAVscvcXC8x34EEzWxFepLMSrg6bdO4o0IxRzDYsl7auxF3ubVXM+z6wTPj92ULw\
fSq7sLlyPPBYntlnmNmTZvaAmZ1SiXpo//OI83t0KYX/0xjHtgKoc/cN4f2NQF2eZeLcZomW+LPR\
57Iirt4cpyLrm0nbe19nuvs6MzuG4DIyz4X/eytLXcB/Al8j+PH5GkHz5hVdeb2u1uStr8S9D7ir\
wGpKvq3SwsxqgF8A17r71pzZjxM0lbWExzT/G3hHBcpK5OcRHs+eDlyfZ3Zc26oVd3czU7fwDkhd\
gLn7uZ142jpgZOTxiHBa1J8JmjN6hv+LzrdMl+szs57AB4H6NtaxLvx3s5ndQ9CM1aUfgWK3m5nd\
BtybZ1Yx27CkNZnZLOADwDkeHgzIs46Sb6scxbzv7DJrw8+3luD7VDZm1osgvO5y91/mzo8Gmrvf\
b2b/YWZHuXtZz7FXxOdR8u9Rkd4PPO7um3JnxLWtQpvMbKi7bwibUjfnWWYdwXG6rBEEx/m7ve7S\
hNgIXBr2FhtN8L+rZdEFwh/IJuCScNLlQDn26M4FnnP3tflmmtkAMxuYvU/QmWFVvmVLJecYxEUF\
Xu9/gXdY0FOzN0FzTGMZa8peiXu6F7gSd4W2VTHvu5Hg+wLB9+c3hQK3FMLja7cDz7r7vxdYZkj2\
OJyZTST4Wy93qBbzeTQCHwt7I54ObIk0oZVTwVaPOLZVRPS7U+g359fAVDM7ImzenxpOk7h7kZTy\
RvDjuxbYDWwCfh2ZN5egN9nzwPsj0+8HhoX3jycIttXAz4A+ZahxATAnZ9ow4P5IDU+Gt6cJmtPK\
vd1+DPwJeIrgD2pobl3h4/MJery9VO66ws9gDbAyvM3PralS2yrf+wb+mSBcAfqG35fV4ffn+DJv\
mzMJmnufimyf84E52e8WcHW4TZ4k6ATz1xX4HuX9PHLqMuB74bb8E5HewmWsawBBINVGplV8WxEE\
6AZgb/g79QmCY6UPAy8Ci4HB4bITgB9EnntF+P1aDXy83NssLTediUNERFKpuzQhiohIlVGAiYhI\
KinAREQklRRgIiKSSgowERFJJQWYiIikkgJMRERSSQEmIiKppAATEZFUUoCJiEgqKcBERCSVFGAi\
IpJKCjAREUklBZiIiKSSAkxERFJJASYiIqmkABMRkVRSgImISCopwEREJJUUYCIikkr/H88RKG/5\
AGNzAAAAAElFTkSuQmCC\
"


    /* set a timeout to make sure all the above elements are created before
       the object is initialized. */
    setTimeout(function() {
        anim5e1aa03096434066a570480411c28485 = new Animation(frames, img_id, slider_id, 200.0,
                                 loop_select_id);
    }, 0);
  })()
</script>
</div></div>
</div>
<p>What do you think is the best hyperplane? Think about what happens if we consider test data points of the two blobs. The test data might reach a bit outwards from the training data blobs. Hence, we donâ€™t want that the hyperplane is too close to one of the blobs. The best hyperplane is the one that is equidistant to both blobs, Hyperplane 3. Mathematically, this is expressed by the idea to maximize the margin,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">D</span><span class="nd">@w</span><span class="o">+</span><span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>24
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># 1. Create toy data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 2. Train linear SVM</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># 3. Extract weights and bias</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 4. Define decision boundary and margin functions</span>
<span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">plot_margin</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Plot decision boundary and margins</span>
    <span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">decision_function</span><span class="p">(</span><span class="n">x_vals</span><span class="p">),</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;decision boundary&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">decision_function</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)),</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;margin -1&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">decision_function</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)),</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;margin +1&quot;</span><span class="p">)</span>

<span class="c1"># 5. Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot points</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">)</span>

<span class="c1"># Plot decision boundary and margins</span>
<span class="n">plot_margin</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Highlight support vectors</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
           <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Support Vectors&#39;</span><span class="p">)</span>

<span class="c1"># Optional: draw arrows (distances) to hyperplane</span>
<span class="k">def</span> <span class="nf">signed_distance</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">:</span>
    <span class="n">proj</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">signed_distance</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">proj</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;SVM Decision Boundary and Margins&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/classification_svms_4_0.png" src="_images/classification_svms_4_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># Generate 2D data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train SVM</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Decision boundary function</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Margin offset</span>
<span class="n">margin</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># X range</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Create margin boundaries</span>
<span class="n">lower_margin</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span> <span class="o">-</span> <span class="n">margin</span>
<span class="n">upper_margin</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span> <span class="o">+</span> <span class="n">margin</span>

<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">)</span>

<span class="c1"># Fill margin area</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">lower_margin</span><span class="p">,</span> <span class="n">upper_margin</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Decision boundary and margins</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x_vals</span><span class="p">),</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Boundary&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">upper_margin</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Margin +1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">lower_margin</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Margin -1&#39;</span><span class="p">)</span>

<span class="c1"># Support vectors</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
           <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Support Vectors&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;SVM with Margin Highlighted&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/classification_svms_5_0.png" src="_images/classification_svms_5_0.png" />
</div>
</div>
<section id="defining-hyperplanes-mathematically">
<h2>Defining Hyperplanes Mathematically<a class="headerlink" href="#defining-hyperplanes-mathematically" title="Permalink to this headline">#</a></h2>
<p>A hyperplane is a generalization of a line (in 2D) or a plane (in 3D) to any number of dimensions.
In <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, a hyperplane is a flat, <span class="math notranslate nohighlight">\((dâˆ’1)\)</span>-dimensional subspace that divides the space into two halves.</p>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 24 </span></p>
<section class="definition-content" id="proof-content">
<p>Given a vector <span class="math notranslate nohighlight">\(\vvec{w}\in\mathbb{R}^d\)</span> and value <span class="math notranslate nohighlight">\(b\in\mathbb{R}\)</span>, then
a hyperplane is defined as the set of all points satisfying the equation
<div class="math notranslate nohighlight">
\[\mathcal{H}_{\vvec{w},b} = \{\vvec{x}\in\mathbb{R}^d\mid\vvec{w}^\top \vvec{x}+b=0\}.\]</div>

The vector <span class="math notranslate nohighlight">\(\vvec{w}\)</span> is called the <strong>normal vector</strong> and <span class="math notranslate nohighlight">\(b\)</span> is called <strong>bias</strong>.</p>
</section>
</div><p>Why are hyperplanes defined like that? We first have a look at a hyperplane with no bias (<span class="math notranslate nohighlight">\(b=0\)</span>). The plot below shows a hyperplane defined as <span class="math notranslate nohighlight">\(\vvec{w}^\top \vvec{x}=0\)</span> and the normal vector <span class="math notranslate nohighlight">\(\vvec{w}\)</span>. Recall from <span class="xref std std-ref">lina_projection</span> that a vector product with a norm one vector <span class="math notranslate nohighlight">\(\tilde{\vvec{w}}=\vvec{w}/\lVert \vvec{w}\rVert\)</span> has a geometric interpretation as the length of the projection of <span class="math notranslate nohighlight">\(\vvec{x}\)</span> onto <span class="math notranslate nohighlight">\(\tilde{\vvec{w}}\)</span>. Hence, all vectors satisfying
<div class="math notranslate nohighlight">
\[\vvec{w}^\top\vvec{x}=0\Leftrightarrow \frac{\vvec{w}^\top}{\lVert \vvec{w}\rVert}\vvec{x}=0\Leftrightarrow \tilde{\vvec{w}}^\top\vvec{x}=0\]</div>

land on the origin when being projected onto <span class="math notranslate nohighlight">\(\vvec{w}\)</span>.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-dc9ced9e6e92676c991f20d7f998f1b40b98ae21.svg" alt="Figure made with TikZ" /></p>
</div><p>Now, we can define all kinds of lengths that the projection onto <span class="math notranslate nohighlight">\(\tilde{\vvec{w}}\)</span> shall have. For example, we can define the hyperplane thatâ€™s orthogonal to the normal vector <span class="math notranslate nohighlight">\(\vvec{w}\)</span> from the plot above, and that has distance two to the origin as the hyperplane
<div class="math notranslate nohighlight">
\[\frac{\vvec{w}^\top}{\lVert\vvec{w}\rVert} \vvec{x}=2\Leftrightarrow \tilde{\vvec{w}}^\top\vvec{x}=2.\]</div>

This hyperplane is plotted below.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-4194c7e574a0e96a5c285b12947e20bc48c788f3.svg" alt="Figure made with TikZ" /></p>
</div><p>The hyperplane from the plot above is likewise defined as the points <span class="math notranslate nohighlight">\(\vvec{x}\)</span> satisfying
<div class="math notranslate nohighlight">
\[\vvec{w}^\top \vvec{x} - 2\lVert\vvec{w}\rVert=0,\]</div>

which adheres to the general definition of a hyperplane with <span class="math notranslate nohighlight">\(b=-2\lVert\vvec{w}\rVert\)</span>. Keep in mind, that we can interpret this hyperplane equation when dividing by the norm of the vector <span class="math notranslate nohighlight">\(\vvec{w}\)</span>.
As a result, geometrically we can say that <span class="math notranslate nohighlight">\(\vvec{w}\)</span> controls the orientation of the hyperplane, and <span class="math notranslate nohighlight">\(b\)</span> controls the offset from the origin.</p>
</section>
<section id="inference-of-a-linear-svm-for-binary-classification">
<h2>Inference of a Linear SVM for Binary Classification<a class="headerlink" href="#inference-of-a-linear-svm-for-binary-classification" title="Permalink to this headline">#</a></h2>
<p>A hyperplane divides a space into two halves. Assuming now that we have only two classes (a binary classification problem). The hyperplane should be positioned such that one class is on one side and the other class is on the other. We can call the sides <em>positive</em> and <em>negative</em>, the positive side is the one the normal vector points to and the negative is the other side.</p>
<div class="amsmath math notranslate nohighlight" id="equation-5ed57659-04e9-44b8-8b59-ccaff4299d9a">
<span class="eqno">(67)<a class="headerlink" href="#equation-5ed57659-04e9-44b8-8b59-ccaff4299d9a" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf w}^{T} {\bf x} + b &amp;&lt;&amp; 0\,\, \mbox{(negative side)} \nonumber \\
{\bf w}^{T} {\bf x} + b &amp;\geq&amp; 0\,\, \mbox{(positive side)}. \nonumber
\end{eqnarray}\]</div>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-b7674db6f749dd49cedc4a5b782fee44c85b3ecb.svg" alt="Figure made with TikZ" /></p>
</div><p>This way, we can define the inference of the SVM for the binary classification problem</p>
<div class="proof definition admonition" id="definition-1">
<p class="admonition-title"><span class="caption-number">Definition 25 </span> (SVM for binary Classification)</p>
<section class="definition-content" id="proof-content">
<p>An SVM classifier for a binary classification problem (<span class="math notranslate nohighlight">\(y\in\{-1,1\}\)</span>) reflects the distance to the decision boundary <span class="math notranslate nohighlight">\(\vvec{w}^\top \vvec{x}+b=0\)</span>
<div class="math notranslate nohighlight">
\[f_{svm}(\vvec{x}) = \vvec{w}^\top \vvec{x}+b.\]</div>

If <span class="math notranslate nohighlight">\(f_{svm}(\vvec{x})\)</span> is positive, then we predict class 1, and otherwise class -1. Using the sign function
<div class="math notranslate nohighlight">
\[\begin{split} \sign(a) = \begin{cases} +1 &amp; \text{ if } a \geq 0 \\ -1 &amp; \text{ otherwise}  \end{cases}, \end{split}\]</div>

we define the class prediction as
<div class="math notranslate nohighlight">
\[\hat{y} = \sign(\vvec{w}^\top \vvec{x}+b).\]</div>
</p>
</section>
</div></section>
<section id="training-of-an-svm-when-classes-are-separable">
<h2>Training of an SVM when Classes are Separable<a class="headerlink" href="#training-of-an-svm-when-classes-are-separable" title="Permalink to this headline">#</a></h2>
<p>We assume for now that the classes are linearly separable, as in our initial example with the two blobs. That means that we can find a hyperplane (defined now as a set)
<span class="math notranslate nohighlight">\(\mathcal{H}_{\vvec{w},b}\)</span>
such that all training data points of the positive class are on the positive side, and the training data points of the negative class are on the negative side. That is, we can find <span class="math notranslate nohighlight">\(\vvec{w}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> such that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\vvec{w}^\top \vvec{x}_i+b &lt;0 &amp; \text{ for all } i \text{ with }y_i=-1\\
\vvec{w}^\top \vvec{x}_i+b &gt;0 &amp; \text{ for all } i \text{ with }y_i=1.
\end{align*}\]</div>
<p>The equations above are satisfied if we have for all training data points
<div class="math notranslate nohighlight">
\[
(\vvec{w}^\top \vvec{x}_i+b)y_i &gt;0.
\]</div>

The goal of the SVM is to find the hyperplane with maximum margin among all the separable hyperplanes. Hence, the SVM hyperplane maximizes the distance to the closest training data point. This way, we can define our SVM task.</p>
<div class="tip admonition">
<p class="admonition-title">Task (hard-margin SVM)</p>
<p><strong>Given</strong> a binary classification training data set <span class="math notranslate nohighlight">\(\mathcal{D}=\{(\vvec{x}_i,y_i)\mid 1\leq i\leq n, y_i\in\{-1,1\}\}\)</span>.</p>
<p><strong>Find</strong> the hyperplane defined as all points in the set <span class="math notranslate nohighlight">\(\mathcal{H}_{\vvec{w},b}\{\vvec{x}\mid\vvec{w}^\top\vvec{x}+b=0\}\)</span> separating the classes and having maximum margin (maximizing the distance of the hyperplane to its closest training data point indicated by <span class="math notranslate nohighlight">\(dist\)</span>).
<div class="math notranslate nohighlight">
\[\max_{\vvec{w},b}\min_{1\leq i\leq n}dist(\mathcal{H}_{\vvec{w},b},\vvec{x}_i)\text{ s.t. } y_i(\vvec{w}^\top\vvec{x}_i+b)\geq 0\]</div>

<strong>Return</strong> the hyperplane defining parameters <span class="math notranslate nohighlight">\(\vvec{w},b\)</span></p>
</div>
<p>To formalize the SVM task, we need to know how we compute the distance of a point to a hyperplane. Geometrically, this works by projecting the point on the normal vector <span class="math notranslate nohighlight">\(\vvec{w}\)</span>, giving the distance from the origin to the projection onto <span class="math notranslate nohighlight">\(\vvec{w}\)</span>. From this distance, we subtract the offset <span class="math notranslate nohighlight">\(b\)</span> and we have the distance to the hyperplane.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-34fdf1ea80090deeb5df0cd777f99ba157aa8c31.svg" alt="Figure made with TikZ" /></p>
</div><div class="proof theorem admonition" id="theorem-2">
<p class="admonition-title"><span class="caption-number">Theorem 16 </span></p>
<section class="theorem-content" id="proof-content">
<p>The following objective is equivalent to the objective
<div class="math notranslate nohighlight">
\[\min_{\vvec{w},b}\lVert \vvec{w}\rVert^2\text{ s.t. } y_i(\vvec{w}^\top\vvec{x}_i+b)\geq 1\]</div>
</p>
</section>
</div><p>Let us consider first the following two parallel hyperplanes</p>
<div class="amsmath math notranslate nohighlight" id="equation-78b965c5-4de7-4028-9cc3-4662cb3440db">
<span class="eqno">(68)<a class="headerlink" href="#equation-78b965c5-4de7-4028-9cc3-4662cb3440db" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf w}^{T} {\bf x} - b &amp;=&amp; -1\,\, \mbox{(negative boundary)} \nonumber \\
{\bf w}^{T} {\bf x} - b &amp;=&amp; +1\,\, \mbox{(positive boundary)} \nonumber
\end{eqnarray}\]</div>
<p>defining <em>negative</em> and <em>positive</em> boundaries such that the maximum-margin hyperplane <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0 \)</span> lies halfway between them. We call the region between these <em>negative</em> and <em>positive</em> boundaries as the <em>margin</em>. Lastly, the distance between the <em>negative</em> and <em>positive</em> boundaries â€“ or equivalently, the margin thickness â€“ is given by <span class="math notranslate nohighlight">\( \frac{2}{||{\bf w}||} \)</span>.</p>
<figure class="align-left" id="margin-planes-03-fig">
<a class="reference internal image-reference" href="_images/margin_planes_03.png"><img alt="_images/margin_planes_03.png" src="_images/margin_planes_03.png" style="height: 480px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">The <em>margin</em> between the <em>negative</em> and <em>positive</em> boundaries with the decision boundary lies half way between them. The <em>margin</em> thickness <span class="math notranslate nohighlight">\( \frac{2}{||{\bf w}||} \)</span> is in turn regulated by the reciprocal of <span class="math notranslate nohighlight">\( || {\bf w} ||\)</span>.</span><a class="headerlink" href="#margin-planes-03-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>As the training samples are not allowed to fall in the margin region, we must select <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> such that</p>
<div class="amsmath math notranslate nohighlight" id="equation-3070fccd-27d7-4b02-8300-8efe21765b67">
<span class="eqno">(69)<a class="headerlink" href="#equation-3070fccd-27d7-4b02-8300-8efe21765b67" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf w}^{T} {\bf x}_{i} - b &amp;\leq&amp; -1\,\, \mbox{if } y_{i} = -1 \nonumber \\
{\bf w}^{T} {\bf x}_{i} - b &amp;\geq&amp; +1\,\, \mbox{if } y_{i} = +1 \nonumber
\end{eqnarray}\]</div>
<p>for all training samples <span class="math notranslate nohighlight">\( \left( {\bf x}_{i}, y_{i} \right) \in {\cal D} \)</span>. Alternatively, we can write</p>
<div class="amsmath math notranslate nohighlight" id="equation-895e0893-61e1-417f-be9a-1836fb922506">
<span class="eqno">(70)<a class="headerlink" href="#equation-895e0893-61e1-417f-be9a-1836fb922506" title="Permalink to this equation">#</a></span>\[\begin{equation}
y_{i} \left( {\bf w}^{T} {\bf x} - b \right) \geq +1,  \nonumber
\end{equation}\]</div>
<p>since the class label <span class="math notranslate nohighlight">\( y_{i} \)</span> and the function <span class="math notranslate nohighlight">\( h({\bf x}_{i}; {\bf w}, b) = {\bf w}^{T} {\bf x}_{i} - b \)</span> have always<a class="footnote-reference brackets" href="#footnote2" id="id1">1</a> the same sign <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
<p>Thus, we can rewrite the optimization problem in <code class="xref eq docutils literal notranslate"><span class="pre">svm_form_1</span></code> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form-2">
<span class="eqno">(71)<a class="headerlink" href="#equation-svm-form-2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast} \right) &amp;=&amp; \argmax_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} \frac{2}{||{\bf w}||} \\
&amp;\equiv&amp; \argmax_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} \frac{1}{||{\bf w}||} \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D},
\end{eqnarray}\end{split}\]</div>
<p>i.e. we seek to find the parameters <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> and <span class="math notranslate nohighlight">\( b^{\ast} \)</span> that maximize the margin thickness <span class="math notranslate nohighlight">\( \frac{2}{||{\bf w}||} \)</span> subjected to the linear constrains <span class="math notranslate nohighlight">\( y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1 \)</span>, <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. However, this is a non-convex optimization problem as the denominator <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> introduces a singularity in the objective function <span class="math notranslate nohighlight">\( \frac{1}{||{\bf w}||} \)</span> in <a class="reference internal" href="#equation-svm-form-2">(71)</a>. Fortunately, we can reformulate the problem of maximizing the reciprocal of the norm <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> into the problem o minimizing the norm <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> itself</p>
<div class="math notranslate nohighlight" id="equation-svm-form-3">
<span class="eqno">(72)<a class="headerlink" href="#equation-svm-form-3" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast} \right) &amp;=&amp; \argmin_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} ||{\bf w}|| \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D},
\end{eqnarray}\end{split}\]</div>
<p>which is in turn a convex optimization problem. For convenience, we replace the objective function <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> in <a class="reference internal" href="#equation-svm-form-3">(72)</a> by <span class="math notranslate nohighlight">\( \frac{1}{2}||{\bf w}||^{2} \)</span> â€“ which is a quadratic function differentiable everywhere â€“ and write</p>
<div class="math notranslate nohighlight" id="equation-svm-form-4">
<span class="eqno">(73)<a class="headerlink" href="#equation-svm-form-4" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast} \right) &amp;=&amp; \argmin_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} \frac{1}{2}||{\bf w}||^{2} \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D},
\end{eqnarray}\end{split}\]</div>
<p>so that we can use e.g. gradient descent solvers to optimize the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>.</p>
<p>In summary, we transformed the non-convex optimization problem of maximizing the margin in <code class="xref eq docutils literal notranslate"><span class="pre">svm_form_1</span></code> into a convex optimization problem of minimizing the squared norm <span class="math notranslate nohighlight">\( \frac{1}{2}||{\bf w}||^{2} \)</span> in <a class="reference internal" href="#equation-svm-form-4">(73)</a> which is subjected in turn to several margin constraints <div class="math notranslate nohighlight">
\[ y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \]</div>
 for <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
<section id="svm-for-two-non-separable-classes">
<h3>SVM for Two Non-Separable Classes<a class="headerlink" href="#svm-for-two-non-separable-classes" title="Permalink to this headline">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Task (soft-margin SVM)</p>
<p><strong>Given</strong> a binary classification training data set <span class="math notranslate nohighlight">\(\mathcal{D}=\{(\vvec{x}_i,y_i)\mid 1\leq i\leq n, y_i\in\{-1,1\}\}\)</span> and parameter <span class="math notranslate nohighlight">\(s&gt;0\)</span>.</p>
<p><strong>Find</strong> the hyperplane defined as all points in the set <span class="math notranslate nohighlight">\(\{\vvec{x}\mid\vvec{w}^\top\vvec{x}+b=0\}\)</span> separating the classes and having maximum margin.
<div class="math notranslate nohighlight">
\[\min_{\vvec{w},b}\lVert \vvec{w}\rVert^2+s\sum_{i=1}^n\xi_i\text{ s.t. } y_i(\vvec{w}^\top\vvec{x}_i+b)\geq 1 -\xi_i\]</div>

<strong>Return</strong> the hyperplane defining parameters <span class="math notranslate nohighlight">\(\vvec{w},b\)</span></p>
</div>
<p>The parameter <span class="math notranslate nohighlight">\(s\)</span> balances margin size vs classification error.
Unfortunately, the hard-margin formulation still requires a linearly separable training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span> so that the linear constraints <div class="math notranslate nohighlight">
\[ y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \]</div>
 <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>, can be satisfied. To overcome this limitation, we can relax the linear constraints by discounting a fixed amount <span class="math notranslate nohighlight">\( \xi_{i} \geq 0 \)</span> from each constraint as <div class="math notranslate nohighlight">
\[ y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1 - \xi_{i} \]</div>
 such that the original constraint is fully imposed for <span class="math notranslate nohighlight">\( \xi_{i} = 0 \)</span> and it is incrementally relaxed as <span class="math notranslate nohighlight">\( \xi_{i} \)</span> grows for <span class="math notranslate nohighlight">\( \xi_{i} &gt; 0 \)</span>. Let the vector <span class="math notranslate nohighlight">\( \boldsymbol{\xi} = \begin{bmatrix} \xi_{1} &amp; \xi_{2} &amp; \ldots &amp; \xi_{N} \end{bmatrix}^{T} \)</span> collect all <span class="math notranslate nohighlight">\( N \)</span> <em>slack</em> variables. Thus, we can rewrite the optimization problem in <a class="reference internal" href="#equation-svm-form-4">(73)</a> assuming relaxed linear constraints as</p>
<div class="math notranslate nohighlight" id="equation-svm-form-5">
<span class="eqno">(74)<a class="headerlink" href="#equation-svm-form-5" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast}, \boldsymbol{\xi}^{\ast} \right) &amp;=&amp; \argmin_{\left( {\bf w}, b, \boldsymbol{\xi} \right) \in \mathbb{R}^{D+N+1}} \left\lbrace \frac{1}{2}||{\bf w}||^{2} + C \sum_{i=1}^{N} \xi_{i} \right\rbrace \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1 - \xi_{i} \\
&amp;&amp; \xi_{i} \geq 0, \,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace,
\end{eqnarray}\end{split}\]</div>
<p>where the hyperparameter <span class="math notranslate nohighlight">\( C &gt; 0 \)</span> regulates the trade off between the original objective function <span class="math notranslate nohighlight">\( \frac{1}{2}||{\bf w}||^{2} \)</span> and the slacks sum <span class="math notranslate nohighlight">\( \sum_{i=1}^{N} \xi_{i} \)</span> such that <span class="math notranslate nohighlight">\( C \rightarrow \infty \)</span> leads to the original optimization problem with hard margin. Conversely, as <span class="math notranslate nohighlight">\( C \rightarrow 0^{+} \)</span>, the constraints are increasingly relaxed. Note that this formulation allows the <span class="math notranslate nohighlight">\(i\)</span>-th training example to violate the original constraint by a fixed amount <span class="math notranslate nohighlight">\( \xi_{i} \)</span> (slack). However, the optimization problem in <a class="reference internal" href="#equation-svm-form-5">(74)</a> is set up such that the sum of these violations <span class="math notranslate nohighlight">\( \sum_{i=1}^{N} \xi_{i} \)</span> is also minimized.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The slack variables <span class="math notranslate nohighlight">\( \lbrace \xi_{1}, \ldots, \xi_{N} \rbrace \)</span> allow therefore to train a SVM classifier using linearly non-separable datasets.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We refer to <a class="reference internal" href="#equation-svm-form-5">(74)</a> as the <strong>primal</strong> SVM formulation which allows us to build a SVM classifier using either linearly separable or linearly non-separable training datasets.</p>
</div>
</aside>
<div class="proof remark admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 4 </span></p>
<section class="remark-content" id="proof-content">
<p>Equivalently, the soft-margin problem in <a class="reference internal" href="#equation-svm-form-5">(74)</a> can be reformulated as a convex optimization problem without constraints. In particular, we can write</p>
<div class="math notranslate nohighlight" id="equation-svm-form-6">
<span class="eqno">(75)<a class="headerlink" href="#equation-svm-form-6" title="Permalink to this equation">#</a></span>\[\begin{equation}
\left( {\bf w}^{\ast}, b^{\ast} \right) = \argmin_{\left( {\bf w}, b, \boldsymbol{\xi} \right) \in \mathbb{R}^{D+1}} \left\lbrace \frac{1}{2}||{\bf w}||^{2} + C \sum_{i=1}^{N} \underbrace{ \max \left( 1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right), 0 \right)}_{\mbox{hinge loss}} \right\rbrace,
\end{equation}\]</div>
<p>in which the hinge loss <div class="math notranslate nohighlight">
\[ \epsilon_{i} \triangleq \max \left( 1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right), 0 \right) \]</div>
 associated with the <span class="math notranslate nohighlight">\( i \)</span>-th training data item is such that</p>
<div class="amsmath math notranslate nohighlight" id="equation-f1d7ff52-2639-44d2-833f-14784d38855a">
<span class="eqno">(76)<a class="headerlink" href="#equation-f1d7ff52-2639-44d2-833f-14784d38855a" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq 1 &amp;\rightarrow&amp; \epsilon_{i} = 0\,\, \mbox{(no penalty)} \nonumber \\
0 \leq y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) &lt; 1 &amp;\rightarrow&amp; 0 &lt; \epsilon_{i} \leq 1\,\, \mbox{(small penalty)} \nonumber \\
y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) &lt; 0 &amp;\rightarrow&amp; \epsilon_{i} &gt; 1\,\, \mbox{(unbounded penalty)}. \nonumber
\end{eqnarray}\]</div>
<p>Note that there is no penalty in the first case, since the training sample <span class="math notranslate nohighlight">\( \left( {\bf x}_{i}, y_{i} \right) \)</span> is not violating the constraint. On the other hand, there is a small penalty in the second case, as the data item <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> falls within the margin, but still in the right side of the hyperplane defined by <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>. The hinge loss grows unbounded in the last case since the data item <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> falls on the wrong side of the hyperplane in this case. The objective function in <a class="reference internal" href="#equation-svm-form-6">(75)</a> penalizes thus the sum of the hinge losses committed by all data items in <span class="math notranslate nohighlight">\( {\cal D} \)</span>. Lastly, as the hinge loss function has a non-linearity around the origin â€“ it is clamped to zero when the constraint <span class="math notranslate nohighlight">\( y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq 1 \)</span> is satisfied â€“, the objective function is neither differentiable everywhere nor quadratic with respect to the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> anymore.</p>
</section>
</div><div class="proof example admonition" id="example-4">
<p class="admonition-title"><span class="caption-number">Example 23 </span></p>
<section class="example-content" id="proof-content">
<p>The effect of the hyperparameter <span class="math notranslate nohighlight">\( C \)</span> on the decision boundary. Note that, as <span class="math notranslate nohighlight">\( C \)</span> grows, the maximum-margin hyperplane becomes more diplomatic in the sense of keeping as much as possible distance to the training examples from different classes. Lastly, note that the final solution becomes influence by more and more data items in <span class="math notranslate nohighlight">\( {\cal D} \)</span> as <span class="math notranslate nohighlight">\( C \)</span> decreases, i.e. more and more support vectors â€“ corresponding to training samples <span class="math notranslate nohighlight">\( \lbrace \left( {\bf x}', y'\right) \in {\cal D} \mid y' \left( {\bf w}^{T} {\bf x}' - b \right) \leq 1 \rbrace \)</span> â€“ will contribute to determine the linear decision boundary <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x}' - b = 0 \)</span>. On the other hand, the training samples <span class="math notranslate nohighlight">\( \lbrace \left( {\bf x}', y'\right) \in {\cal D} \mid y' \left( {\bf w}^{T} {\bf x}' - b \right) &gt; 1 \rbrace \)</span> do not contribute directly to the values of the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> in the sense that any changes of their positions in the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span> without violating the constraints lead to the same solution.</p>
<figure class="align-left" id="effect-softmargin-sep-fig">
<a class="reference internal image-reference" href="_images/effect_softmargin_sep.png"><img alt="_images/effect_softmargin_sep.png" src="_images/effect_softmargin_sep.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Linearly separable dataset.</span><a class="headerlink" href="#effect-softmargin-sep-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="effect-softmargin-nonsep-fig">
<a class="reference internal image-reference" href="_images/effect_softmargin_nonsep.png"><img alt="_images/effect_softmargin_nonsep.png" src="_images/effect_softmargin_nonsep.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">Linearly non-separable dataset.</span><a class="headerlink" href="#effect-softmargin-nonsep-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div></section>
</section>
<section id="dual-svm-formulation">
<h2>Dual SVM formulation<a class="headerlink" href="#dual-svm-formulation" title="Permalink to this headline">#</a></h2>
<p>Let us rewrite the hard-margin SVM problem in <a class="reference internal" href="#equation-svm-form-4">(73)</a> using the standard format as</p>
<div class="math notranslate nohighlight" id="equation-svm-form-7">
<span class="eqno">(77)<a class="headerlink" href="#equation-svm-form-7" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\minimize_{{\bf w} \in \mathbb{R}^{D}, b \in \mathbb{R}} &amp;&amp;  \frac{1}{2}||{\bf w}||^{2} \\
s.t. \,\,\, &amp;&amp;  1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \leq 0, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D}.
\end{eqnarray}\end{split}\]</div>
<p>We can write the Lagrangian of <a class="reference internal" href="#equation-svm-form-7">(77)</a> as</p>
<div class="math notranslate nohighlight" id="equation-lagrandian2">
<span class="eqno">(78)<a class="headerlink" href="#equation-lagrandian2" title="Permalink to this equation">#</a></span>\[{\cal L}({\bf z}, \boldsymbol{\lambda}) = f({\bf z}) + \sum_{i=1}^{N} \lambda_{i} \left( 1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right)  \right),\]</div>
<p>which is convex with respect to the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>. Thus, we can find the solution to <div class="math notranslate nohighlight">
\[ {\cal L}_{dual}(\boldsymbol{\lambda}) = \min_{{\bf z} \in {\cal Z}} {\cal L}({\bf z}, \boldsymbol{\lambda}) \]</div>
 by setting both the gradient <div class="math notranslate nohighlight">
\[ \nabla_{\bf w} {\cal L}({\bf z}, \boldsymbol{\lambda}) = {\bf w} - \sum_{i=1}^{N} \lambda_{i} y_{i} {\bf x}_{i} \]</div>
 and the partial derivative <div class="math notranslate nohighlight">
\[ \frac{\partial {\cal L}({\bf z}, \boldsymbol{\lambda})}{\partial b} = \sum_{i=1}^{N} \lambda_{i} y_{i} \]</div>
 to zero. Hence, we write</p>
<div class="math notranslate nohighlight" id="equation-w-grad-zero">
<span class="eqno">(79)<a class="headerlink" href="#equation-w-grad-zero" title="Permalink to this equation">#</a></span>\[\nabla_{\bf w} {\cal L}({\bf z}, \boldsymbol{\lambda}) = 0 \Leftrightarrow {\bf w} = \sum_{i=1}^{N} \lambda_{i} y_{i} {\bf x}_{i}\]</div>
<div class="math notranslate nohighlight" id="equation-cond-partial-zero">
<span class="eqno">(80)<a class="headerlink" href="#equation-cond-partial-zero" title="Permalink to this equation">#</a></span>\[\frac{\partial {\cal L}({\bf z}, \boldsymbol{\lambda})}{\partial b} = 0 \Leftrightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0.\]</div>
<p>Now, by plugging <a class="reference internal" href="#equation-w-grad-zero">(79)</a> and <a class="reference internal" href="#equation-cond-partial-zero">(80)</a> back into the Lagrangian definition <a class="reference internal" href="#equation-lagrandian2">(78)</a>, we obtain</p>
<div class="amsmath math notranslate nohighlight" id="equation-1bccb474-7b54-4866-88a1-62b16e5c67a0">
<span class="eqno">(81)<a class="headerlink" href="#equation-1bccb474-7b54-4866-88a1-62b16e5c67a0" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\cal L}_{dual}(\boldsymbol{\lambda}) &amp;=&amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i} - \underbrace{\sum_{i=1}^{N} \lambda_{i}y_{i} {\bf x}_{i} \sum_{j=1}^{N} \lambda_{j} y_{j} {\bf x}_{j}}_{= \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j}} + \underbrace{\sum_{i=1}^{N} \lambda_{i} y_{i}}_{= 0} b \nonumber \\
&amp;=&amp; - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i}.
\end{eqnarray}\]</div>
<p>Thus, we can write the dual problem in <code class="xref eq docutils literal notranslate"><span class="pre">dual_prob1</span></code> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form8">
<span class="eqno">(82)<a class="headerlink" href="#equation-svm-form8" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}} &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i}, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace,
\end{eqnarray}\end{split}\]</div>
<p>which is clearly a quadratic program leading therefore to a convex optimization problem. Hence, the optimal <span class="math notranslate nohighlight">\( \boldsymbol{\lambda}^{\ast} \)</span> in <a class="reference internal" href="#equation-svm-form8">(82)</a> also delivers the optimal solution for the primal problem in <a class="reference internal" href="#equation-svm-form-7">(77)</a>. Specifically,</p>
<div class="math notranslate nohighlight" id="equation-w-optimal">
<span class="eqno">(83)<a class="headerlink" href="#equation-w-optimal" title="Permalink to this equation">#</a></span>\[{\bf w}^{\ast} = \sum_{i=1}^{N} \lambda_{i}^{\ast} y_{i} {\bf x}_{i}\]</div>
<div class="math notranslate nohighlight" id="equation-b-optimal">
<span class="eqno">(84)<a class="headerlink" href="#equation-b-optimal" title="Permalink to this equation">#</a></span>\[b^{\ast} = ({\bf w}^{\ast})^{T} {\bf x}_{j} - y_{j},\]</div>
<p>in which we plug any training example <span class="math notranslate nohighlight">\( \left( {\bf x}_{j}, y_{j} \right) \)</span> with index <span class="math notranslate nohighlight">\( j \in \lbrace 1, 2, \ldots, N \rbrace \)</span> such that <span class="math notranslate nohighlight">\( \lambda_{j} &gt; 0 \)</span>.</p>
<p>Finally, we offer without proof that <a class="reference internal" href="#equation-svm-form8">(82)</a> can be rewritten to consider soft-margin constraints simply by plugging in the hyperparameter <span class="math notranslate nohighlight">\( C \)</span> into the linear constraints as follows</p>
<div class="math notranslate nohighlight" id="equation-svm-form9">
<span class="eqno">(85)<a class="headerlink" href="#equation-svm-form9" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace.
\end{eqnarray}\end{split}\]</div>
<p>Alternatively, we can rewrite the dual problem in <a class="reference internal" href="#equation-svm-form9">(85)</a> more concisely in terms of the so-called Gram matrix, a.k.a. pairwise influence matrix, <span class="math notranslate nohighlight">\( {\bf G} \triangleq \left[ G_{i,j} \right] \)</span> with <span class="math notranslate nohighlight">\( G_{i,j} = y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} \)</span> for <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots N \rbrace \)</span> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form10">
<span class="eqno">(86)<a class="headerlink" href="#equation-svm-form10" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \boldsymbol{\lambda}^{T} {\bf G} \boldsymbol{\lambda} + {\bf 1}^{T} \boldsymbol{\lambda} \\
s.t. \,\,\, &amp;&amp;   {\bf y}^{T} \boldsymbol{\lambda} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace,
\end{eqnarray}\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\( {\bf y} \triangleq \begin{bmatrix} y_{1} &amp; y_{2} &amp; \ldots &amp; y_{N} \end{bmatrix}^{T} \)</span> and <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} = \begin{bmatrix} \lambda_{1} &amp; \lambda_{2} &amp; \ldots &amp; \lambda_{N} \end{bmatrix}^{T} \)</span> collecting the labels and the slack variables associated with each training example, respectively, and <span class="math notranslate nohighlight">\( {\bf 1} \triangleq \begin{bmatrix} 1 &amp; 1 &amp; \ldots &amp; 1 \end{bmatrix}^{T} \)</span> representing a vector of <span class="math notranslate nohighlight">\(1\)</span>â€™s with appropriate number of dimensions â€“ in this case <span class="math notranslate nohighlight">\( N \)</span>. Note also that <a class="reference internal" href="#equation-svm-form10">(86)</a> is equivalent to writing</p>
<div class="math notranslate nohighlight" id="equation-svm-form11">
<span class="eqno">(87)<a class="headerlink" href="#equation-svm-form11" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\minimize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  \frac{1}{2} \boldsymbol{\lambda}^{T} {\bf G} \boldsymbol{\lambda} - {\bf 1}^{T} \boldsymbol{\lambda} \\
s.t. \,\,\, &amp;&amp;   {\bf y}^{T} \boldsymbol{\lambda} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace.
\end{eqnarray}\end{split}\]</div>
<div class="proof remark admonition" id="remark-5">
<p class="admonition-title"><span class="caption-number">Remark 5 </span></p>
<section class="remark-content" id="proof-content">
<p>As the objective function <span class="math notranslate nohighlight">\( \frac{1}{2} || {\bf w} ||^{2} \)</span> in <a class="reference internal" href="#equation-svm-form9">(85)</a> depends on the coefficient vector <div class="math notranslate nohighlight">
\[ {\bf w} = \begin{bmatrix} w_{1} &amp; w_{2} &amp; \ldots &amp; w_{D} \end{bmatrix}^{T}, \]</div>
 the primal SVM problem has computational complexity <span class="math notranslate nohighlight">\( {\cal O}(D) \)</span> with <span class="math notranslate nohighlight">\( D \)</span> denoting the number of features in the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span>. On the other hand, the dual SVM problem in <a class="reference internal" href="#equation-svm-form9">(85)</a> is clearly quadratic on the number of samples <span class="math notranslate nohighlight">\( N \)</span>, i.e it has computational complexity <span class="math notranslate nohighlight">\( {\cal O} (N^{2}) \)</span>. Thus, for a large number of features <span class="math notranslate nohighlight">\( D \gg N \)</span>, the dual SVM problem can be cheaper, while the solution to the primal SVM problem has a smaller computational burden for large datasets with <span class="math notranslate nohighlight">\( N \gg D \)</span>.</p>
</section>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The objective function of the dual SVM problem formulations in <a class="reference internal" href="#equation-svm-form8">(82)</a>â€“<a class="reference internal" href="#equation-svm-form11">(87)</a> depends only on the inner products <span class="math notranslate nohighlight">\( \lbrace {\bf x}_{i}^{T} {\bf x}_{j} \rbrace \)</span> with <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. This dependence unlocks the kernel trick that will be discussed in the sequel.</p>
</div>
</aside>
</section>
<section id="a-word-on-kernels">
<h2>A word on kernels<a class="headerlink" href="#a-word-on-kernels" title="Permalink to this headline">#</a></h2>
<p>A kernel function <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> maps pairs of vectors <span class="math notranslate nohighlight">\( {\bf x}_{i}, {\bf x}_{j} \)</span> residing in a <span class="math notranslate nohighlight">\(D\)</span>-dimensional Euclidean space <span class="math notranslate nohighlight">\( \mathbb{R}^{D} \)</span> into real numbers in <span class="math notranslate nohighlight">\( \mathbb{R} \)</span>, i.e. <div class="math notranslate nohighlight">
\[ k: \mathbb{R}^{D} \times \mathbb{R}^{D} \rightarrow \mathbb{R}. \]</div>
 Furthermore, a kernel is positive definite if for any <em>finite</em> collection of vectors <span class="math notranslate nohighlight">\( {\bf x}_{1}, \ldots, {\bf x}_{N} \)</span> and any collection of real numbers <span class="math notranslate nohighlight">\( a_{1}, \ldots, a_{N} \)</span>, the following holds <div class="math notranslate nohighlight">
\[ \sum_{i=1}^{N} \sum_{j=1}^{N} a_{i} a_{j} {\bf x}_{i}^{T} {\bf x}_{j} \geq 0. \]</div>
 Alternatively, we can write in vector notation as</p>
<div class="math notranslate nohighlight" id="equation-positive-definite">
<span class="eqno">(88)<a class="headerlink" href="#equation-positive-definite" title="Permalink to this equation">#</a></span>\[{\bf a}^{T} {\bf K} {\bf a} \geq 0,\]</div>
<p>where <span class="math notranslate nohighlight">\( {\bf K} = \left[ K_{i,j} \right] \)</span> is a <span class="math notranslate nohighlight">\( N \times N \)</span> matrix with <span class="math notranslate nohighlight">\( K_{i,j} = k({\bf x}_{i}, {\bf x}_{j}) \)</span> for all <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span> and <span class="math notranslate nohighlight">\( {\bf a} = \begin{bmatrix} a_{1} &amp; a_{2} &amp; \ldots &amp; a_{N} \end{bmatrix}^{T} \)</span> is an arbitrary vector in <span class="math notranslate nohighlight">\( \mathbb{R}^{N} \)</span>.</p>
</section>
<section id="kernel-trick">
<h2>Kernel trick<a class="headerlink" href="#kernel-trick" title="Permalink to this headline">#</a></h2>
<p>Applying non-linear transformations of the type <div class="math notranslate nohighlight">
\[ \phi : {\cal X} \rightarrow {\cal Z} \]</div>
 mapping feature vectors <span class="math notranslate nohighlight">\( {\bf x} \in {\cal X} \)</span> into a higher-dimensional space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> can significantly boost several Machine Learning (ML) algorithms, for instance SVMs, Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).</p>
<p>Mapping feature vectors into such high-dimensional space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> can be effective but it is often expensive and selecting the proper non-linear transformation <span class="math notranslate nohighlight">\( \phi(\cdot) \)</span> is also a hard task. Fortunately, provided that the ML algorithm works with inner products of feature vectors as in <a class="reference internal" href="#equation-svm-form8">(82)</a>â€“<a class="reference internal" href="#equation-svm-form11">(87)</a>, the kernel trick allows one to work with high-dimensional spaces efficiently since it avoids computing <span class="math notranslate nohighlight">\( \phi(\cdot) \)</span> explicitly.</p>
<p>In most classification problems, decision boundaries are far from linear. In particular, for real-world binary classification problems, chances are that no hyperplane can separate training examples from both classes. On the other hand, high-dimensional data tends to be linearly separable. Intuitively, as the number of features increases, chances are that the data become linearly separable with respect of some of these features.</p>
<p>One approach to increase the number of dimensions is to apply <strong>non-linear feature transforms</strong> to the original features in <span class="math notranslate nohighlight">\( {\cal X} \)</span> transforming thus the learning problem into a higher dimensional feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span>. Note though that the feature transformations must be non-linear, for instance polynomials, exponential (e.g. <span class="math notranslate nohighlight">\(\exp\)</span>), logarithm (e.g. <span class="math notranslate nohighlight">\(\log\)</span>) and trigonometric functions (e.g. <span class="math notranslate nohighlight">\(\cos\)</span>, <span class="math notranslate nohighlight">\(\sin\)</span>, <span class="math notranslate nohighlight">\(\tanh\)</span>). This is a necessary (non-sufficient) condition. Note though that if we apply linear (affine) transformations, the data in <span class="math notranslate nohighlight">\({\cal Z} \)</span> will still linearly non-separable as in the original feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span>.</p>
<p><a class="reference internal" href="#non-linear-transformation-fig"><span class="std std-numref">Fig. 14</span></a> illustrates a particular non-linear transformations of features. Note that, despite being linearly non-separable in the original space <span class="math notranslate nohighlight">\( {\cal X} \)</span>, the non-linear features in <span class="math notranslate nohighlight">\( {\cal Z} \)</span> become linearly separable, i.e. one can design a linear classifier of the type
<div class="math notranslate nohighlight">
\[
h({\bf z}; {\bf w}, b) = \sgn({\bf w}^{T} {\bf z} - b)
\]</div>

residing in the non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> that is able to separate the data from both classes. Equivalently, we can also write
<div class="math notranslate nohighlight">
\[
h({\bf x}; {\bf w}, b) = \sgn({\bf w}^{T} \phi({\bf x}) - b).
\]</div>
</p>
<figure class="align-left" id="non-linear-transformation-fig">
<a class="reference internal image-reference" href="_images/degree2_monomials.png"><img alt="_images/degree2_monomials.png" src="_images/degree2_monomials.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">Non-linear transformation <span class="math notranslate nohighlight">\(\phi: \mathbb{R}^2 \rightarrow \mathbb{R}^3 \)</span> mapping feature vectors <span class="math notranslate nohighlight">\( {\bf x} = \begin{bmatrix} x_1 &amp; x_2 \end{bmatrix}^{T} \)</span> into non-linear feature vectors <span class="math notranslate nohighlight">\( {\bf z} = \begin{bmatrix} z_1 &amp; z_2 &amp; z_3 \end{bmatrix}^{T} \)</span> such that <span class="math notranslate nohighlight">\( {\bf z} = \phi({\bf x}) := \begin{bmatrix} x_1^2 &amp; \sqrt{2} x_1 x_2 &amp; x_2^2 \end{bmatrix}^{T} \)</span>. The original bi-dimensional feature space in the left is unsuitable for a linear classifier. However, the non-linear transformation of features yields to a three-dimensional feature space in the right in which the data is linearly separable for some classifier of the type <span class="math notranslate nohighlight">\( h({\bf z}; {\bf w}, b) = \sgn({\bf w}^{T} {\bf z} - b) \)</span> (borrowed from <span id="id2">[<a class="reference internal" href="bibliography.html#id10">13</a>]</span>).</span><a class="headerlink" href="#non-linear-transformation-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Therefore, a linear classifier operating on a non-linear feature space leads to a non-linear classifier. Schematically, <div class="math notranslate nohighlight">
\[ \mbox{non-linear features} + \mbox{linear classifier} = \mbox{non-linear classifier}. \]</div>
</p>
</div>
</aside>
<p>Unfortunately, there are too many non-linear transforms to the features. In addition to this, selecting a non-linear transform <span class="math notranslate nohighlight">\(\phi: {\cal X} \rightarrow {\cal Z} \)</span> that leads to a non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> in which the transformed data from <span class="math notranslate nohighlight">\( {\cal X} \)</span> is linearly separable is not a straightforward task. Furthermore, for a given class of non-linear transformations, the number of possible features might quickly explode. For example, let us consider all polynomials of degree <span class="math notranslate nohighlight">\(K\)</span>. Thus, there are <span class="math notranslate nohighlight">\({D + K - 1}\choose {K}\)</span> possible features to select â€“ with <span class="math notranslate nohighlight">\(D\)</span> denoting the number of features in the original feature space <span class="math notranslate nohighlight">\( {\cal X}\)</span> â€“ when designing the non-linear feature space <span class="math notranslate nohighlight">\({\cal Z}\)</span>. In particular, <span class="math notranslate nohighlight">\(D=100\)</span> and <span class="math notranslate nohighlight">\(K=5\)</span> yields <span class="math notranslate nohighlight">\(75 \times 10^6\)</span> possible features. Therefore, checking all possible combinations of non-linear features to select a suitable non-linear transform <span class="math notranslate nohighlight">\(\phi\)</span> is prohibitive.</p>
<p>Fortunately, many learning algorithms can be re-formulated such that they work only with labels <span class="math notranslate nohighlight">\( y_{1}, y_{2}, \ldots, y_{N} \)</span> and <strong>inner products</strong> <span class="math notranslate nohighlight">\( {\bf x}_{i}^{T} {\bf x}_{j} \)</span>. For those algorithms, we can employ the <strong>Kernel Trick</strong> to efficiently work with high-dimensional features spaces without explicitly transforming the original features.</p>
<p>More precisely, let us define a <span class="math notranslate nohighlight">\( N \times N \)</span> pairwise similarity matrix <span class="math notranslate nohighlight">\( {\bf K} \triangleq \left[ K_{i,j} \right] \)</span> â€“ a.k.a. Gram matrix â€“ such that</p>
<div class="math notranslate nohighlight" id="equation-pairwise-similarity1">
<span class="eqno">(89)<a class="headerlink" href="#equation-pairwise-similarity1" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
K_{i,j} &amp;=&amp; k({\bf x}_{i}, {\bf x}_{j}) \\
&amp;=&amp; \phi({\bf x}_{i})^{T} \phi({\bf x}_{j})
\end{eqnarray}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\( \forall i,j \lbrace 1, 2, \ldots, N \rbrace \)</span>. As <div class="math notranslate nohighlight">
\[ \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) = 0 \Leftrightarrow \phi({\bf x}_{i}) \perp \phi({\bf x}_{j}), \]</div>
 the function <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> is a similarity measure â€“ a scalar â€“ of the transformed, non-linear feature vectors <span class="math notranslate nohighlight">\( \phi({\bf x}_{i}) \)</span> and <span class="math notranslate nohighlight">\( \phi({\bf x}_{j}) \)</span>. We can also define the similarity measure between the training samples <span class="math notranslate nohighlight">\( {\cal D}_{i} = \left( {\bf x}_{i}, y_{i} \right) \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{j} = \left( {\bf x}_{j}, y_{j} \right) \)</span> as</p>
<div class="math notranslate nohighlight" id="equation-pairwise-similarity2">
<span class="eqno">(90)<a class="headerlink" href="#equation-pairwise-similarity2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
g({\cal D}_{i}, {\cal D}_{j}) &amp;=&amp; y_{i} y_{j} \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \\
&amp;=&amp;  y_{i} y_{j} k({\bf x}_{i}, {\bf x}_{j}).
\end{eqnarray}\end{split}\]</div>
<p>Note that for normalized feature vectors <span class="math notranslate nohighlight">\( \phi({\bf x}_{i}) \)</span> and <span class="math notranslate nohighlight">\( \phi({\bf x}_{j}) \)</span>, <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \geq 0 \)</span>. Thus, the sign of the product <span class="math notranslate nohighlight">\( y_{i} y_{j} \)</span> indicates either a label matching (<span class="math notranslate nohighlight">\( +1 \)</span>) or mismatching (<span class="math notranslate nohighlight">\( -1 \)</span>) in binary classification problems with labels in <span class="math notranslate nohighlight">\( {\cal Y} = \lbrace -1, +1 \rbrace \)</span>. In this case, we can redefine the Gram matrix as a <span class="math notranslate nohighlight">\( N \times N \)</span> influence matrix <span class="math notranslate nohighlight">\( {\bf G} = \left[ G_{i,j} \right] \)</span> such that <div class="math notranslate nohighlight">
\[ G_{i,j} = g({\cal D}_{i}, {\cal D}_{j}) \]</div>
 for all <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
<p>Now, let us rewrite the dual SVM problem in the non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form12">
<span class="eqno">(91)<a class="headerlink" href="#equation-svm-form12" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace.
\end{eqnarray}\end{split}\]</div>
<p>Thus, as far as we are able to compute the similarity metric <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> implicitly, we can efficiently solve the optimization problem in <a class="reference internal" href="#equation-svm-form12">(91)</a> without computing the non-linear features <span class="math notranslate nohighlight">\( \lbrace \phi({\bf x}_{i}) \rbrace \)</span>, <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. Fortunately, this possible as far as the the kernel <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> satisfies some mild conditions.</p>
<p>Now, we offer without proof a key theorem for the Kernel Trick.</p>
<div class="proof theorem admonition" id="theorem-6">
<p class="admonition-title"><span class="caption-number">Theorem 17 </span> (Representer theorem)</p>
<section class="theorem-content" id="proof-content">
<p>A kernel function <span class="math notranslate nohighlight">\( k: {\cal X} \times {\cal X} \rightarrow \mathbb{R} \)</span> is positive definite i.f.f. it corresponds to the inner product in some feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> defined by the transformation <span class="math notranslate nohighlight">\( \phi: {\cal X} \rightarrow {\cal Z} \)</span>, i.e. <div class="math notranslate nohighlight">
\[ \exists \phi({\bf x}) \mid k({\bf x}_{i}, {\bf x}_{j}) = \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \Leftrightarrow \forall {\bf a} \in \mathbb{R}^{N}  \mid {\bf a}^{T} {\bf K} {\bf a} \geq 0 \]</div>
 with <span class="math notranslate nohighlight">\( {\bf K} = \left[ K_{i,j} \right] \)</span> denoting a <span class="math notranslate nohighlight">\( N \times N \)</span> matrix such that <span class="math notranslate nohighlight">\( K_{i,j} = k({\bf x}_{i}, {\bf x}_{j}) \)</span>, <span class="math notranslate nohighlight">\( \forall i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
</section>
</div><p>As a corollary, if we choose a particular function <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> such that <span class="math notranslate nohighlight">\( \sum_{i=1}^{N} \sum_{j=1}^{N} a_{i} a_{j} {\bf x}_{i}^{T} {\bf x}_{j} \geq 0 \)</span> for all data items <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span> in your training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span>, we can compute the inner product <span class="math notranslate nohighlight">\( \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \)</span> implicitly without even knowing the non-linear transformation <span class="math notranslate nohighlight">\( \phi({\bf x}) \)</span>. Putting in other words, if the matrix <span class="math notranslate nohighlight">\( {\bf K} \)</span> is positive definite such that <a class="reference internal" href="#equation-positive-definite">(88)</a> holds, we can replace the the product <span class="math notranslate nohighlight">\( \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \)</span> in <a class="reference internal" href="#equation-svm-form12">(91)</a> by the kernel <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> to obtain the kernelized SVM formulation</p>
<div class="math notranslate nohighlight" id="equation-svm-form13">
<span class="eqno">(92)<a class="headerlink" href="#equation-svm-form13" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} k({\bf x}_{i}, {\bf x}_{j}) + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace
\end{eqnarray}\end{split}\]</div>
<p>which is still a quadratic program (convex) and therefore the global optimal solution <span class="math notranslate nohighlight">\( \boldsymbol{\lambda}^{\ast} \)</span> to this dual problem also leads to an optimal solution to the primal problem in <a class="reference internal" href="#equation-svm-form-7">(77)</a>. Note that the Kernel Trick allows one to generalize several learning algorithms which rely on inner products (e.g. SVMs, PCAs and LDAs). Moreover, it also allows one to work with <em>non-vector</em> data (e.g. strings and graphs) by means of the selection of a proper kernel / similarity measure <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> between the data items / objects within the original object space <span class="math notranslate nohighlight">\( {\cal X} \)</span>.</p>
<p>Typical kernel functions employed</p>
<ul class="simple">
<li><p><strong>Linear</strong> <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) = {\bf x}_{i}^{T} {\bf x}_{j} \)</span>, which corresponds to working in the original feature space, i.e. <span class="math notranslate nohighlight">\( \phi({\bf x}) = {\bf x} \)</span>;</p></li>
<li><p><strong>Polynomial</strong> <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) = (1 + {\bf x}_{i}^{T} {\bf x}_{j})^{p}\)</span>, which corresponds to <span class="math notranslate nohighlight">\(\phi\)</span> mapping to all polynomials â€“ non-linear features â€“ up to degree <span class="math notranslate nohighlight">\(p\)</span>;</p></li>
<li><p><strong>Gaussian</strong> <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) = \exp(-\gamma || {\bf x}_{i} - {\bf x}_{j} ||^{2}) \)</span>, which corresponds in turn to an infinite feature space or functional space with infinite number of dimensions.</p></li>
</ul>
<p>Recall from <a class="reference internal" href="#equation-w-optimal">(83)</a> and <a class="reference internal" href="#equation-b-optimal">(84)</a> that</p>
<div class="math notranslate nohighlight" id="equation-w-optimal2">
<span class="eqno">(93)<a class="headerlink" href="#equation-w-optimal2" title="Permalink to this equation">#</a></span>\[{\bf w}^{\ast} = \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} {\bf x}_{i}\]</div>
<div class="math notranslate nohighlight" id="equation-b-optimal2">
<span class="eqno">(94)<a class="headerlink" href="#equation-b-optimal2" title="Permalink to this equation">#</a></span>\[b^{\ast} = {{\bf w}^{\ast}}^{T} {\bf x}_{i} - y_{i}.\]</div>
<p>Now, by substituting <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> by <span class="math notranslate nohighlight">\( \phi({\bf x}_{i} ) \)</span> in <a class="reference internal" href="#equation-w-optimal2">(93)</a> and in <a class="reference internal" href="#equation-b-optimal2">(94)</a>, we have</p>
<div class="math notranslate nohighlight" id="equation-w-optimal3">
<span class="eqno">(95)<a class="headerlink" href="#equation-w-optimal3" title="Permalink to this equation">#</a></span>\[{\bf w}^{\ast} = \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} \phi({\bf x}_{i})\]</div>
<div class="math notranslate nohighlight" id="equation-b-optimal3">
<span class="eqno">(96)<a class="headerlink" href="#equation-b-optimal3" title="Permalink to this equation">#</a></span>\[b^{\ast} = {{\bf w}^{\ast}}^{T} \phi({\bf x}_{i}) - y_{i}.\]</div>
<p>Lastly, by plugging <a class="reference internal" href="#equation-w-optimal3">(95)</a> into <a class="reference internal" href="#equation-b-optimal3">(96)</a>, we can compute the optimal offset term</p>
<div class="amsmath math notranslate nohighlight" id="equation-d6d6dc59-d75d-422a-9b97-76525c3117d5">
<span class="eqno">(97)<a class="headerlink" href="#equation-d6d6dc59-d75d-422a-9b97-76525c3117d5" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
b^{\ast} &amp;=&amp; \left( \sum_{j=1}^{N} \lambda^{\ast}_{j} y_{j} \phi({\bf x}_{j}) \right)^{T} \phi({\bf x}_{i}) - y_{i} \nonumber \\
&amp;=&amp; \sum_{j=1}^{N} \lambda^{\ast}_{j} y_{j} \underbrace{\phi({\bf x}_{j})^{T} \phi({\bf x}_{i})}_{k({\bf x}_{j}, {\bf x}_{i})} - y_{i} \nonumber \\
&amp;=&amp; \sum_{j=1}^{N} \lambda^{\ast}_{j} y_{j} k({\bf x}_{j}, {\bf x}_{i}) - y_{i}
\end{eqnarray}\]</div>
<p>using any training example <span class="math notranslate nohighlight">\( {\cal D}_{i} = \left( {\bf x}_{i}, y_{i} \right) \)</span>, <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>, whose corresponding Lagrange multiplier <span class="math notranslate nohighlight">\( \lambda^{\ast}_{i} &gt; 0 \)</span>.</p>
<p>Note however that there is no need to compute the optimal coefficient vector <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> explicitly. Specifically, for an arbitrary feature vector <span class="math notranslate nohighlight">\( {\bf x} \)</span> in the original feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span>, the Kernelized SVM classifier in the non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> â€“ determined by some mapping function <span class="math notranslate nohighlight">\( \phi:{\cal X} \rightarrow {\cal Z} \)</span> â€“ can be written as</p>
<div class="amsmath math notranslate nohighlight" id="equation-85ea345a-96b6-4552-8248-9adec415b0d8">
<span class="eqno">(98)<a class="headerlink" href="#equation-85ea345a-96b6-4552-8248-9adec415b0d8" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
h_{kSVM}({\bf x}) &amp;=&amp; \sgn \left( {{\bf w}^{\ast}}^{T} \phi({\bf x}) - b^{\ast} \right) \nonumber \\
&amp;=&amp; \sgn \left( \left( \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} \phi({\bf x}_{i}) \right)^{T} \phi({\bf x}) - b^{\ast} \right) \nonumber \\
&amp;=&amp; \sgn \left( \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} \underbrace{\phi({\bf x}_{i})^{T} \phi({\bf x})}_{k({\bf x}_{i}, {\bf x})} - b^{\ast} \right). \nonumber
\end{eqnarray}\]</div>
<p>Finally leading to</p>
<div class="math notranslate nohighlight" id="equation-kerneliezed-svm">
<span class="eqno">(99)<a class="headerlink" href="#equation-kerneliezed-svm" title="Permalink to this equation">#</a></span>\[h_{kSVM}({\bf x}) = \sgn \left( \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} k({\bf x}_{i}, {\bf x}) - b^{\ast} \right).\]</div>
<div class="proof example admonition" id="example-7">
<p class="admonition-title"><span class="caption-number">Example 24 </span> (Kernelized SVM classifier in action using a toy example with <em>sklearn</em>)</p>
<section class="example-content" id="proof-content">
<p>Figures below illustrate the effect of different Kernels with <em>sklearn</em> using a toy binary classification example. <a class="reference internal" href="#toyexp-kernel-svm-01-fig"><span class="std std-numref">Fig. 15</span></a> shows a linear kernel failing miserably to separate training data items assigned to <span style="color: blue;">blue</span> and <span style="color: red;">red</span> class labels; <a class="reference internal" href="#toyexp-kernel-svm-02-fig"><span class="std std-numref">Fig. 16</span></a> a polynomial kernel with degree <span class="math notranslate nohighlight">\(p=4\)</span> is able to circumscribe the data items assigned to the <span style="color: blue;">blue</span> labels; and <a class="reference internal" href="#toyexp-kernel-svm-03-fig"><span class="std std-numref">Fig. 17</span></a> a Gaussian kernel â€“ a.k.a. radial basis function â€“ is also able to separate both classes, but using kind of more rounded decision boundary between classes.</p>
<figure class="align-left" id="toyexp-kernel-svm-01-fig">
<a class="reference internal image-reference" href="_images/toyexp_kernel_svm_01.png"><img alt="_images/toyexp_kernel_svm_01.png" src="_images/toyexp_kernel_svm_01.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">classifier = svm.SVC(kernel=â€™linearâ€™, C=C)</span><a class="headerlink" href="#toyexp-kernel-svm-01-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="toyexp-kernel-svm-02-fig">
<a class="reference internal image-reference" href="_images/toyexp_kernel_svm_02.png"><img alt="_images/toyexp_kernel_svm_02.png" src="_images/toyexp_kernel_svm_02.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">classifier = svm.SVC(kernel=â€™polyâ€™, degree=4, C=C)</span><a class="headerlink" href="#toyexp-kernel-svm-02-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="toyexp-kernel-svm-03-fig">
<a class="reference internal image-reference" href="_images/toyexp_kernel_svm_03.png"><img alt="_images/toyexp_kernel_svm_03.png" src="_images/toyexp_kernel_svm_03.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">classifier = svm.SVC(kernel=â€™rbfâ€™, C=C)</span><a class="headerlink" href="#toyexp-kernel-svm-03-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SVMs and Kernels were hot research topics in the 90â€™s and early 2000â€™s. Nevertheless, Kernelized SVMs still one of the strongest classifiers today.</p>
</div>
</aside>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="footnote2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Assuming that the hyperplane <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0\)</span> is a valid decision boundary for the linearly separable training dataset <span class="math notranslate nohighlight">\({\cal D}\)</span>.</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="classification_random_forests.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Random Forests</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="neuralnets.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Neural Networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess and Stiven Dias<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>