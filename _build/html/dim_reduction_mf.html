
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Low Rank Matrix Factorization &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Matrix Completion" href="dim_reduction_matrix_completion.html" />
    <link rel="prev" title="Dimensionality Reduction Techniques" href="dim_reduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_analytic.html">
     Analytic Solutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_gradients.html">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_sparse.html">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_decision_trees.html">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_svms.html">
     Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_kernel_svm.html">
     Kernel SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     From Linear Models to Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     MLPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_backprop.html">
     Backpropagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_sgd.html">
     Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_conv.html">
     Convolution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_pooling.html">
     Pooling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html/issues/new?title=Issue%20on%20page%20%2Fdim_reduction_mf.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/TUEML/intro.html/edit/master/dim_reduction_mf.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/dim_reduction_mf.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formal-problem-definition">
   Formal Problem Definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-simple-matrix-completion-recommender-system">
   A Simple Matrix Completion Recommender System
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation-of-the-factorization">
     Interpretation of the Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-happens-when-we-increase-the-rank">
   What Happens When We Increase the Rank?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#low-rank-mf-on-observed-entries">
   Low-Rank MF on Observed Entries
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-unsupervised-matrix-completion-to-supervised-behavioral-modeling">
     From Unsupervised Matrix Completion to Supervised Behavioral Modeling
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Low Rank Matrix Factorization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formal-problem-definition">
   Formal Problem Definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-simple-matrix-completion-recommender-system">
   A Simple Matrix Completion Recommender System
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation-of-the-factorization">
     Interpretation of the Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-happens-when-we-increase-the-rank">
   What Happens When We Increase the Rank?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#low-rank-mf-on-observed-entries">
   Low-Rank MF on Observed Entries
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-unsupervised-matrix-completion-to-supervised-behavioral-modeling">
     From Unsupervised Matrix Completion to Supervised Behavioral Modeling
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="low-rank-matrix-factorization">
<h1>Low Rank Matrix Factorization<a class="headerlink" href="#low-rank-matrix-factorization" title="Permalink to this headline">#</a></h1>
<figure class="align-center" id="netflix">
<a class="reference internal image-reference" href="_images/Netflix_Screenshot.png"><img alt="_images/Netflix_Screenshot.png" src="_images/Netflix_Screenshot.png" style="height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Low rank matrix factorization can be used for recommender systems like Netflix</span><a class="headerlink" href="#netflix" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Netflix gained its big popularity back in the days because it focused on its recommender system strength. They personalized content discovery, ensuring users stayed engaged by always having something new to watch. The task of <em>recommendation</em> is unsupervised, we donâ€™t know the ground truth recommendations, as opposed to supervised tasks, where we have a label or target variable.  All we have are past user ratings, from which we try to derive common patterns that allow us to provide recommendations.</p>
<p>Letâ€™s go through an example to get a clearer understanding of the recommender task. Imagine we represent all users and movies in a matrix, where each entry corresponds to a userâ€™s rating for a movie. Then we get a massive, sparse matrix (since most users have only rated a small fraction of available movies). The challenge is to predict the missing ratings so that Netflix can suggest movies that a user is likely to enjoy. For example, the user-movie matrix could look like that:</p>
<style>
.custom-table-container th {
    writing-mode: vertical-lr;
    vertical-align: bottom;
    width: 20%;
}
</style>
<div class="custom-table-container">
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>User</p></th>
<th class="head"><p>Star Wars</p></th>
<th class="head"><p>Interstellar</p></th>
<th class="head"><p>Blade Runner</p></th>
<th class="head"><p>Tron</p></th>
<th class="head"><p>2001: Space O.</p></th>
<th class="head"><p>Mars Attacks</p></th>
<th class="head"><p>Dune</p></th>
<th class="head"><p>Matrix</p></th>
<th class="head"><p>Robo Cop</p></th>
<th class="head"><p>Aliens</p></th>
<th class="head"><p>Terminator</p></th>
<th class="head"><p>Solaris</p></th>
<th class="head"><p>Avatar</p></th>
<th class="head"><p>12 Monkeys</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Grace</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ™ˆ</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Carol</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
</tr>
<tr class="row-even"><td><p>Alice</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ™ˆ</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Bob</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>Eve</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>ðŸ™ˆ</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Chuck</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ¤©</p></td>
<td><p>ðŸ¤©</p></td>
<td><p></p></td>
<td><p>ðŸ™ˆ</p></td>
<td><p>ðŸ¤©</p></td>
</tr>
</tbody>
</table>
</div><p>We have six users and 14 movies, that are rated either as <em>I like it</em> (ðŸ¤©) or <em>not for me</em> (ðŸ™ˆ). If no emoji is indicated, then the corresponding user has not seen the movie yet. This example matrix of user-movie preferences exhibits two patterns of preferences. The first pattern consists of the movies <em>Star Wars, Interstellar, Blade Runner, Tron, 2001: Space Odyssey, Matrix, Solaris,</em> and <em>12 Monkeys</em>. This set of movies is popular in the user group of Carol, Alice and Chuck: every movie of the set is liked at least by two of the three users. Hence, we might consider to recommend each person of that group a movie from this set that the person has not watched yet. For example, we could recommend to Carol to watch <em>2001: Space Odyssey</em>.
Likewise, we identify a second pattern of movies that is popular among the group of Grace, Bob, Eve and Chuck. This pattern encompasses the movies <em>Star Wars, Mars Attacks, Matrix, Robo Cop, Aliens, Terminator</em>, and <em>Avatar</em>.</p>
<p>Letâ€™s visualize the model that we are looking for as a matrix. Below you see the abstract representation of the groups of users and movies as a matrix. The first group of users (Carol, Alice and Chuck) and their corresponding set of (largely) liked movies is visualized in blue, and the second set of users (Grace, Bob, Eve and Chuck) and their set of movies are visualized in red.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 sd-d-flex-row sd-align-major-center docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-af253f8d53856b1690458eb7bfcf4a46fa3ff074.svg" alt="Figure made with TikZ" /></p>
</div></div>
</div>
</div>
<p>The matrix above reflects the positive movie indications (ðŸ¤©) by the nonzero cells, that are the ones that are colored. The last row reflects Chuck, who adheres to both movie patterns, that overlap in the movies <em>Star Wars</em> and <em>Matrix</em>. Hence, we see two cells in the last row with overlapping colors. The user-movie matrix can be decomposed into the sum of two matrices, where each matrix reflects one user-movie group.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 sd-d-flex-row sd-align-major-center docutils">
<div class="sd-row sd-g-0 sd-g-xs-0 sd-g-sm-0 sd-g-md-0 sd-g-lg-0 docutils">
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-72923ba1e222095f38264a363b9dc78570f93b13.svg" alt="Figure made with TikZ" /></p>
</div></div>
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<p><span class="math notranslate nohighlight">\(+\)</span></p>
</div>
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-956cc3350ff20d9115ffb6160869fed6cd98f51b.svg" alt="Figure made with TikZ" /></p>
</div></div>
</div>
</div>
<p>Furthermore, each of the single user-group matrices can be represented by an outer product of a user- and a user-vector. And the sum of outer product matrices reflects a low-dimensional matrix product. In this casse, we have a product of two dimensionality two â€“ one for each user-movie matrix.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 sd-d-flex-row sd-align-major-center docutils">
<div class="sd-row sd-g-0 sd-g-xs-0 sd-g-sm-0 sd-g-md-0 sd-g-lg-0 docutils">
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-1863ac78ab9dd75305fbc4494ff2c8dbe5ce8a39.svg" alt="Figure made with TikZ" /></p>
</div></div>
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-40e85b57642ae1bbc40c34298b8b03a52db0f3dc.svg" alt="Figure made with TikZ" /></p>
</div></div>
</div>
</div>
<p>The low-dimensional matrix product describing our user-movie preferences is now represented by two user-vectors (the columns of the left matrix) and two movie-vectors (the rows of the right matrix). Each pair of user- and movie-vectors indicates a group, exhibiting the same movie preferences. Note, that the original <span class="math notranslate nohighlight">\(6\times 14\)</span> matrix is now compressed into a <span class="math notranslate nohighlight">\(6\times 2\)</span>-matrix and a <span class="math notranslate nohighlight">\(2\times 14\)</span> matrix. While the original matrix contains <span class="math notranslate nohighlight">\(6\cdot 14 = 84\)</span> elements, the low-dimensional product needs only <span class="math notranslate nohighlight">\(2\cdot 6 + 2\cdot 14 = 40\)</span> elememts to be stored. That is roughly the idea of recommender systems: using reoccurences in the behaviour or similarities among users and movies to compress the data, and to use the compressed data representation to make recommendations.</p>
<section id="formal-problem-definition">
<h2>Formal Problem Definition<a class="headerlink" href="#formal-problem-definition" title="Permalink to this headline">#</a></h2>
<p>We formalize the idea to obtain recommendations by compressing the data as the task to minimize the squared Frobenius-norm error of a low-rank matrix product and the data matrix.</p>
<div class="tip admonition">
<p class="admonition-title">Task (Rank-r Matrix Factorization)</p>
<p><strong>Given</strong> a data matrix <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}\)</span> and a rank <span class="math notranslate nohighlight">\(r&lt;\min\{n,d\}\)</span>.</p>
<p><strong>Find</strong> matrices <span class="math notranslate nohighlight">\(X\in \mathbb{R}^{d\times r}\)</span> and <span class="math notranslate nohighlight">\(Y\in\mathbb{R}^{n\times r}\)</span> whose product approximates the data matrix:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a9ca050b-e461-4ba9-9abc-884a8b8fc33d">
<span class="eqno">(55)<a class="headerlink" href="#equation-a9ca050b-e461-4ba9-9abc-884a8b8fc33d" title="Permalink to this equation">#</a></span>\[\begin{align}
    \min_{X,Y}&amp;\lVert D- YX^\top\rVert^2 &amp; \text{s.t. } X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}
\end{align}\]</div>
<p><strong>Return</strong> the low-dimensional approximation of the data <span class="math notranslate nohighlight">\((X,Y)\)</span>.</p>
</div>
<p>Note that the Rank-r matrix factorization task is not directly suitable to return recommendations. It only describes the task to compress a given data matrix into a low-dimensional product. To provide recommendations, we need to fill in missing values. We will discuss later how we can do this with a low dimensional matrix factorization.<br />
First, we analyze properties of the objective, as it turns out, the low-dimensional matrix factorization task is nonconvex.</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 27 </span></p>
<section class="theorem-content" id="proof-content">
<p>The rank-<span class="math notranslate nohighlight">\(r\)</span> matrix factorization problem, defined for a matrix <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}\neq\mathbf{0}\)</span> and a rank <span class="math notranslate nohighlight">\(1\leq r&lt;\min\{n,d\}\)</span> as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \min_{X,Y}&amp; RSS(X,Y)=\lVert D- YX^\top\rVert^2 &amp; \text{s.t. } X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}
\end{align*}\]</div>
<p>is a <strong>nonconvex optimization problem</strong>.</p>
</section>
</div><p>The proof follows from the fact that the set of global minimizers is not a convex set.</p>
<div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. We show that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is not a convex function. Therefore we assume first that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is a convex function and show then that this assumption leads to a contradiction. Assuming that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is a convex function means that the following inequality has to hold for all matrices <span class="math notranslate nohighlight">\(X_1,X_2\in\mathbb{R}^{d\times r}\)</span> and <span class="math notranslate nohighlight">\(Y_1,Y_2\in\mathbb{R}^{n\times r}\)</span> and <span class="math notranslate nohighlight">\(\alpha\in[0,1]\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-405436e5-af31-4d77-9d85-5e98e340beea">
<span class="eqno">(56)<a class="headerlink" href="#equation-405436e5-af31-4d77-9d85-5e98e340beea" title="Permalink to this equation">#</a></span>\[\begin{align}
        RSS(\alpha X_1+ (1-\alpha)X_2,\alpha Y_1 + (1-\alpha)Y_2) \leq \alpha RSS(X_1,Y_1) + (1-\alpha)RSS(X_2,Y_2).
\end{align}\]</div>
<p>For any global minimizer <span class="math notranslate nohighlight">\((X,Y)\)</span> of the rank-<span class="math notranslate nohighlight">\(r\)</span> MF problem, <span class="math notranslate nohighlight">\((\gamma X, \frac{1}{\gamma} Y)\)</span> is also a global minimizer for <span class="math notranslate nohighlight">\(\gamma\neq 0\)</span>.
However, for <span class="math notranslate nohighlight">\(\alpha=1/2\)</span> the convex combination attains a function value of</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    RSS(\alpha X + (1-\alpha) (\gamma X), \alpha Y+(1-\alpha)(\tfrac1\gamma Y)) &amp;=RSS\left(\tfrac12 X + \tfrac12 (\gamma X), \tfrac12 Y+\tfrac12(\tfrac1\gamma Y)\right)\\
    &amp;= RSS\left(\tfrac12(1+\gamma) X, \tfrac12(1+ \tfrac1\gamma) Y\right)\\
    &amp;=\lVert D-\tfrac14(1+\gamma)(1+\tfrac1\gamma)YX^\top\rVert^2.
\end{align*}\]</div>
<p>We observe that the approximation error in the last equation goes to infinity if <span class="math notranslate nohighlight">\(\gamma\rightarrow \infty\)</span>. Hence, there exists multiple <span class="math notranslate nohighlight">\(\gamma&gt;0\)</span> for which the <span class="math notranslate nohighlight">\(RSS\)</span> of the convex combination of two global minimizers is larger than zero. This contradicts the assumption that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is convex.</p>
</div>
</div>
<p>We observe from the proof that there are infinitely many global minimizers for the low-dimensional matrix factorization task. Letâ€™s explore this set of global minimizers by means of an example in one dimension.</p>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 21 </span> (One-dimensional matrix factorization   )</p>
<section class="example-content" id="proof-content">
<p>The most <em>easy</em> case of a low-dimensional matrix factorization is the factorization of a single number. Letâ€™s take for example the factorization of the number one into a product of two factors <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, having the objective function  <span class="math notranslate nohighlight">\(f(x_1,x_2) = (1-x_1x_2)^2\)</span>. We plot the graph of the objective function together with three solutions: <span class="math notranslate nohighlight">\((x_1,x_2)=(2,0.5)\)</span>, <span class="math notranslate nohighlight">\((1,1)\)</span>, and <span class="math notranslate nohighlight">\((0.5,2)\)</span>.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-8e5b26bf64acbe9c637716e4bc0ecf64fe70aafb.svg" alt="Figure made with TikZ" /></p>
</div><p>We can observe the nonconvexity of this function by connecting the solution of <span class="math notranslate nohighlight">\((0.5,2)\)</span> with <span class="math notranslate nohighlight">\((2,0.5)\)</span> with a straight line. If the function would be convex, then the graph ofthe function would be under or on the line. Under the line is not possible in this case, since the solution points we picked are global minimizers. Hence, the graph should be flat (<span class="math notranslate nohighlight">\(y\)</span>-value equal to zero) on the line between the two solutions, but we see that the loss increases to the right. However, we also see that the loss function doesnâ€™t look as if there is a multitude of valleys, that are local minima. That gives us hope, that the low-rank matrix factorization task is not that difficult to solve.</p>
</section>
</div></section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">#</a></h2>
<p>In most cases, nonconvexity of an objective implies that we probably have to live with the fact that we can not determine the global minimum, and that we can only hope to get good local minima by numerical optimization methods such as gradient descent. The rank-<span class="math notranslate nohighlight">\(r\)</span> matrix factorization problem is here an exemption to the rule, since we can derive one global minimum by SVD.</p>
<div class="proof theorem admonition" id="theorem-2">
<p class="admonition-title"><span class="caption-number">Theorem 28 </span> (Truncated SVD)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(D=U\Sigma V^\top\in\mathbb{R}^{n\times d}\)</span> be the singular decomposition of <span class="math notranslate nohighlight">\(D\)</span>. Then the global minimizers <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>  of the rank-<span class="math notranslate nohighlight">\(r\)</span> MF problem</p>
<div class="amsmath math notranslate nohighlight" id="equation-a0d70c36-d9db-4158-94d0-b9e622c8cbcc">
<span class="eqno">(57)<a class="headerlink" href="#equation-a0d70c36-d9db-4158-94d0-b9e622c8cbcc" title="Permalink to this equation">#</a></span>\[\begin{align}
\min_{X,Y}\lVert D-YX^\top\rVert^2 \text{s.t. } X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}.
\end{align}\]</div>
<p>satisfy</p>
<div class="amsmath math notranslate nohighlight" id="equation-a064f69c-001d-4955-abd9-5cb468dd59b6">
<span class="eqno">(58)<a class="headerlink" href="#equation-a064f69c-001d-4955-abd9-5cb468dd59b6" title="Permalink to this equation">#</a></span>\[\begin{align} 
YX^\top = U_{\cdot \mathcal{R}}\Sigma_{\mathcal{R}\mathcal{R}}V_{\cdot \mathcal{R}}^\top, \text{ where }\mathcal{R}=\{1,\ldots, r\}.
\end{align}\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof follows from
the orthogonal invariance of the Frobenius norm, yielding:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \min_{X,Y}&amp;\lVert D-YX^\top\rVert^2 = \lVert \Sigma - U^\top YX^\top V\rVert^2 
\end{align*}\]</div>
</div>
</section>
<section id="a-simple-matrix-completion-recommender-system">
<h2>A Simple Matrix Completion Recommender System<a class="headerlink" href="#a-simple-matrix-completion-recommender-system" title="Permalink to this headline">#</a></h2>
<p>We can use truncated SVD to compute a low-rank approximation of the data.
How can we use this to provide recommendations? After all, we need a complete matrix in order to compute the SVD. For now, we consider a quick hack: we fill the missing values with the mean (neutral rating) and compute the truncated SVD with the hope that the SVD reconstructs mainly the given ratings that are often not equal to the mean rating, such that the imputed values get a more accurate prediction of a rating with the SVD.</p>
<p>Letâ€™s go through an example. The table below shows a movie-ratings database that is filled by some ratings, but not all movies have been seen by all costumers and we want to fill in the missing values with the approximate rating that would be given by the user if the had seen the movie.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Id</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(A\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(B\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(C\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(D\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>?</p></td>
<td><p>â˜…â˜…â˜†â˜†â˜†</p></td>
<td><p>â˜…â˜†â˜†â˜†â˜†</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>?</p></td>
<td><p>â˜…â˜†â˜†â˜†â˜†</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>?</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>â˜…â˜†â˜†â˜†â˜†</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>â˜…â˜…â˜†â˜†â˜†</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>?</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>â˜…â˜…â˜…â˜†â˜†</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>?</p></td>
<td><p>?</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>?</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜†</p></td>
<td><p>â˜…â˜…â˜…â˜…â˜…</p></td>
<td><p>â˜…â˜…â˜…â˜†â˜†</p></td>
</tr>
</tbody>
</table>
<p>We apply our quick hack and replace the unobserved entries with the mean rating <span class="math notranslate nohighlight">\(\mu=\frac{1+2+3+4+5}{5}=3\)</span>. This gives us the following data matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">D</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[5, 3, 1, 1],
       [3, 1, 5, 3],
       [2, 1, 5, 3],
       [4, 3, 4, 2],
       [5, 5, 3, 1],
       [3, 1, 5, 3]])
</pre></div>
</div>
</div>
</div>
<p>We visualize the rating matrix with the image below. A white pixel indicates a low rating and a blue pixel indicates a higher rating.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/dim_reduction_mf_17_0.png" src="_images/dim_reduction_mf_17_0.png" />
</div>
</div>
<p>We compute the SVD of the matrix <span class="math notranslate nohighlight">\(D\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Váµ€</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We get a rank-2 approximation of <span class="math notranslate nohighlight">\(D\)</span> by truncating the SVD to two singular values and vectors:
<div class="math notranslate nohighlight">
\[D\approx U_{\cdot \{1,2\}}\Sigma_{\{1,2\}\{1,2\}}V_{\cdot \{1,2\}}^\top.\]</div>

The tri-factorization of SVD can be expressed as a factorization into two matrices by making an arbitrary split into the product of two matrices. For example, we could set <span class="math notranslate nohighlight">\(Y=U_{\cdot \{1,2\}}\Sigma_{\{1,2\}\{1,2\}}^{1/2}\)</span> and <span class="math notranslate nohighlight">\(X=V_{\cdot \{1,2\}}\Sigma_{\{1,2\}\{1,2\}}^{1/2}\)</span>. This way, we can expect that <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are similarly scaled, since they both are matrices with unitary vectors that are scaled by the square roots of the singular values. However, in principle, any split of the SVD product into two matrices will do.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The matrix <span class="math notranslate nohighlight">\(A^{1/2}\)</span> is defined as the matrix that satisfies the equation <span class="math notranslate nohighlight">\(A^{1/2}A^{1/2}=A\)</span>. Not for all matrices <span class="math notranslate nohighlight">\(A\)</span> exists such a matrix <span class="math notranslate nohighlight">\(A^{1/2}\)</span>. However, for nonnegative, diagonal matrices <span class="math notranslate nohighlight">\(\Sigma\)</span>, the matrix <span class="math notranslate nohighlight">\(\Sigma^{1/2}=\diag(\sqrt{\sigma_1},\ldots, \sqrt{\sigma_r})\)</span> exists.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Váµ€</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">Y</span><span class="p">,</span><span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[-1.3 ,  1.26],
        [-1.61, -0.85],
        [-1.46, -1.04],
        [-1.71,  0.21],
        [-1.81,  1.27],
        [-1.61, -0.85]]),
 array([[-2.3 ,  1.08],
        [-1.49,  1.38],
        [-2.43, -1.36],
        [-1.35, -0.92]]))
</pre></div>
</div>
</div>
</div>
<p>The low rank approximation can be used to give recommendations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[4.34, 3.68, 1.43, 0.59],
       [2.78, 1.23, 5.08, 2.97],
       [2.23, 0.75, 4.97, 2.93],
       [4.16, 2.84, 3.88, 2.13],
       [5.53, 4.46, 2.68, 1.28],
       [2.78, 1.23, 5.08, 2.97]])
</pre></div>
</div>
</div>
</div>
<p>If we compare the matrix above with the matrix having missing values, then we see that the low rank approximation gives some tendencies for recommendations, but often no very clear recommendation indications. This is not very surprising, since we had just a small dataset and comparatively many missing values. The rank-2 approximation is already a bit too well adapting to the missing values neutral rating.</p>
<div class="amsmath math notranslate nohighlight" id="equation-4e2a9c18-8799-4691-b1da-64f0e83ef9a1">
<span class="eqno">(59)<a class="headerlink" href="#equation-4e2a9c18-8799-4691-b1da-64f0e83ef9a1" title="Permalink to this equation">#</a></span>\[\begin{align}
  \begin{pmatrix}
    5 &amp; \mu &amp; 1 &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; \mu  \\\
    2 &amp; 1 &amp; 5 &amp; 3 \\
    4 &amp; \mu &amp; 4 &amp; 2\\
    5 &amp; 5 &amp; \mu &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; 3 \\
  \end{pmatrix}
  &amp;\approx
  \begin{pmatrix}
    4.3 &amp; 3.7 &amp; 1.4 &amp; 0.6\\
    2.8 &amp; 1.2 &amp; 5.1 &amp; 3.0\\
    2.2 &amp; 0.7 &amp; 5.0 &amp; 2.9\\
    4.2 &amp; 2.8 &amp; 3.9 &amp; 2.1\\
    5.5 &amp; 4.5 &amp; 2.7 &amp; 1.3\\
    2.8 &amp; 1.2 &amp; 5.1 &amp; 3.0
  \end{pmatrix}
\end{align}\]</div>
<p>If we plot the original data and the approximation next to another, then we also see that there are no big differences.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/dim_reduction_mf_27_0.png" src="_images/dim_reduction_mf_27_0.png" />
</div>
</div>
<section id="interpretation-of-the-factorization">
<h3>Interpretation of the Factorization<a class="headerlink" href="#interpretation-of-the-factorization" title="Permalink to this headline">#</a></h3>
<p>The rank-two matrix product is composed by the sum of two outer products. Every outer product indicates the interaction of a user-pattern with a movie pattern. Hence, looking at the outer-product decomposition is useful for interpretation purposes. The equation below shows again the rank-two approximation.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  \begin{pmatrix}
    5 &amp; \mu &amp; 1 &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; \mu  \\\
    2 &amp; 1 &amp; 5 &amp; 3 \\
    4 &amp; \mu &amp; 4 &amp; 2\\
    5 &amp; 5 &amp; \mu &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; 3 \\
  \end{pmatrix}
  &amp;\approx
  \begin{pmatrix}
    -0.3 &amp; 0.5\\
    -0.4 &amp; -0.4\\
    -0.4 &amp; -0.4\\
    -0.4 &amp; 0.1\\
    -0.5 &amp; 0.5\\
    -0.4 &amp; -0.4\\
  \end{pmatrix}
  \begin{pmatrix}
    -9.0 &amp; -5.8 &amp; -9.5 &amp; -5.3\\
    2.6 &amp; 3.3 &amp; -3.3 &amp; -2.2\\
  \end{pmatrix}
\end{align*}\]</div>
<p>Every userâ€™s preferences are approximated by a linear combination of the rows in the second matrix. The rows in the second matrix have an interpretation as movie patterns. For example, the first user adheres to the first movie pattern with a factor of <span class="math notranslate nohighlight">\(-0.3\)</span> and to the second movie pattern with a factor of <span class="math notranslate nohighlight">\(0.5\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \begin{pmatrix}
        5 &amp; \mu &amp; 1 &amp; 1 
    \end{pmatrix}
    \approx&amp;
    -0.3\cdot 
    \begin{pmatrix}
        -9.0 &amp; -5.8 &amp; -9.5 &amp; -5.3
    \end{pmatrix}
    +0.5\cdot
    \begin{pmatrix}
        2.6 &amp; 3.3 &amp; -3.3 &amp; -2.2
    \end{pmatrix}    
\end{align*}\]</div>
<p>We visualize the sum of the two outer products with colored matrices. The more saturated the color, the  higher is the absolute value of the corresponding element in the matrix. Positive values are blue and negative values are pink.<br />
The visualization makes the grid structure apparent which is induced by the outer product. The first movie pattern is <span class="math notranslate nohighlight">\(\begin{pmatrix}-9.0 &amp; -5.8 &amp; -9.5 &amp; -5.3\end{pmatrix}\)</span>, but because all values in the first user pattern are also negative, it would be more intuitive to consider the first movie pattern as <span class="math notranslate nohighlight">\(\begin{pmatrix}9 &amp; 5.8 &amp; 9.5 &amp; 5.3\end{pmatrix}\)</span>. We see this pattern in the first outer product <span class="math notranslate nohighlight">\(Y_{\cdot 1}X_{\cdot 1}^\top\)</span>: the first and third column have a higher intensity than the other columns, corresponding to the high values in the pattern matrix <span class="math notranslate nohighlight">\(9\)</span> and <span class="math notranslate nohighlight">\(9.5\)</span>. We can roughly say that the first outer product indicates how much the user likes movie 1 and movie 3 but not so much the other movies.<br />
However, we cannot make general statements from the first outer product (e.g., user 2 likes movie 1 and movie 3 because their (sign-corrected) coefficient for the first movie pattern is 0.4, which is comparatively high). That is because the second outer product may correct what the first outer product indicates by subtracting and adding values. For evample, user 2 doesnâ€™t particularly like movie 1 (in fact, itâ€™s a missing value) and although the first outer product indicates a score of <span class="math notranslate nohighlight">\(0.4\cdot 9=3.6\)</span>. The second outer product corrects this by subtracting <span class="math notranslate nohighlight">\(-0.4\cdot2.6=-1.04\)</span>.</p>
<p>Hence, we canâ€™t make general statements based on one outer product alone. However, the significance of the outer products in the sense of how they influence the approximation, drops with the index of the outer product. That is because the singular values are decreasing in magnitude with the index. Visually, we can observe this with the fading colors making up the second outer product.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/dim_reduction_mf_30_0.png" src="_images/dim_reduction_mf_30_0.png" />
</div>
</div>
</section>
</section>
<section id="what-happens-when-we-increase-the-rank">
<h2>What Happens When We Increase the Rank?<a class="headerlink" href="#what-happens-when-we-increase-the-rank" title="Permalink to this headline">#</a></h2>
<p>The rank of the matrix factorization is a hyperparameter. If we choose the rank too low, then the approximation might underfit and some of the patterns in the data remain undiscovered. If the rank is too high, then the model also approximates noise-effects from the data and overfits. Noise effects could be in the recommender setting fluctuations in the mood of the user at the time of the rating. In the setting of the matrix completion task, a too high rank also results in an approximation of the neutral rankings that are imputed for the missing values.</p>
<p>We visualize the factorization with a rank of three below. We see that the third outer product does make some minor corrections for user 1 and 5. At this point, the information in the third outer product becomes neglectable and should not be included in the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">Váµ€</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/dim_reduction_mf_33_0.png" src="_images/dim_reduction_mf_33_0.png" />
</div>
</div>
</section>
<section id="low-rank-mf-on-observed-entries">
<h2>Low-Rank MF on Observed Entries<a class="headerlink" href="#low-rank-mf-on-observed-entries" title="Permalink to this headline">#</a></h2>
<p>The SVD of the rating matrix with neutral rating imputations gave us some indications of possible recommendations, but an issue with this simple approach is that the imputed missing values are also approximated by the factorization. Especially when observations are sparse, which is usually the case for movie recommendations for example, then this method wonâ€™t work because the actual ratings will be perceived as outliers.</p>
<p>An idea to solve the matrix completion task more accurately is to compute a factorization that approximates only the values of observed entries.
This approach has been used in the Netflix Price 2009 competition and made it to the top-3.</p>
<div class="tip admonition">
<p class="admonition-title">Task (Low-Rank Matrix Factorization with Missing Values)</p>
<p><strong>Given</strong> a data matrix <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}\)</span> having observed entries <span class="math notranslate nohighlight">\(D_{ik}\)</span> for indices <span class="math notranslate nohighlight">\((i,k)\)</span> where the binary indicator matrix <span class="math notranslate nohighlight">\(O\in\{0,1\}^{n\times d}\)</span> has an entry of one <span class="math notranslate nohighlight">\(O_{ik}=1\)</span>, and a rank <span class="math notranslate nohighlight">\(r&lt;\min\{n,d\}\)</span>.</p>
<p><strong>Find</strong> matrices <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{d\times r}\)</span> and <span class="math notranslate nohighlight">\(Y\in\mathbb{R}^{n\times r}\)</span> whose product approximates the data matrix only on observed entries:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f94ceb6b-bdf9-420a-99dd-4f3383265247">
<span class="eqno">(60)<a class="headerlink" href="#equation-f94ceb6b-bdf9-420a-99dd-4f3383265247" title="Permalink to this equation">#</a></span>\[\begin{align}
    \min_{X,Y}&amp;\lVert O\circ(D- YX^\top)\rVert^2 +\lambda\lVert X\rVert^2+\lambda\lVert Y\rVert^2\\ 
    &amp;=\sum_{(i,k):O_{ik}=1}(D_{ik}-Y_{i\cdot}X_{k\cdot}^\top)^2+\lambda\lVert X\rVert^2+\lambda\lVert Y\rVert^2\\ 
    \text{s.t. }&amp; X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}
\end{align}\]</div>
<p><strong>Return</strong> the low-dimensional approximation of the data <span class="math notranslate nohighlight">\((X,Y)\)</span>.</p>
</div>
<section id="id1">
<h3>Optimization<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>The low-rank MF on observed entries can not be computed directly by SVD. However, we can derive the minimizers of one column of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> when fixing the other matrix.</p>
<div class="proof theorem admonition" id="theorem-3">
<p class="admonition-title"><span class="caption-number">Theorem 29 </span></p>
<section class="theorem-content" id="proof-content">
<p>The minimizers of the objective to minimize <span class="math notranslate nohighlight">\(\lVert O\circ(D- YX^\top)\rVert^2\)</span> subject to a row of <span class="math notranslate nohighlight">\(X\)</span> or <span class="math notranslate nohighlight">\(Y\)</span> is given as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;D_{\cdot k}^\top \diag(O_{\cdot k})Y(Y^\top \diag(O_{\cdot k}) Y+\lambda I)^{-1}\\ 
&amp;\quad = \argmin_{X_{k\cdot}}
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert X\rVert^2\\
&amp;D_{i\cdot} \diag(O_{i\cdot})X(X^\top \diag(O_{i\cdot}) X+\lambda I)^{-1}\\ 
&amp;\quad = \argmin_{Y_{i\cdot}}
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert Y\rVert^2
\end{align*}\]</div>
</section>
</div><div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. We show the result for minimizing over <span class="math notranslate nohighlight">\(X_{k\cdot}\)</span>, the result for <span class="math notranslate nohighlight">\(Y_{i\cdot}\)</span> follows by transposing the factorization. First, we observe that the minimization subject to <span class="math notranslate nohighlight">\(X_{k\cdot}\)</span> reduces to the minimization over the <span class="math notranslate nohighlight">\(k\)</span>-th column:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\argmin_{X_{k\cdot}}&amp;
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert X\rVert^2\\
&amp;= \argmin_{X_{k\cdot}}
\lVert O_{\cdot k}\circ(D_{\cdot k}- YX_{k\cdot }^\top)\rVert^2+ \lambda\lVert X_{k\cdot}\rVert^2
\end{align*}\]</div>
<p>The element-wise multiplication with the binary vector <span class="math notranslate nohighlight">\(O_{\cdot k}\)</span> selects the rows for which we have observed entries in column <span class="math notranslate nohighlight">\(k\)</span>. This selection of rows can also be performed with a multiplication of <span class="math notranslate nohighlight">\(\diag(O_{\cdot k})\)</span> from the left. This way, we can write the objective to optimize subject to <span class="math notranslate nohighlight">\(X_{k\cdot}\)</span> as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\argmin_{X_{k\cdot}}&amp;
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert X\rVert^2\\ 
&amp;= \argmin_{X_{k\cdot}}
\lVert \underbrace{\diag(O_{\cdot k})D_{\cdot k}}_{=\tilde{\vvec{y}}}- \underbrace{\diag(O_{\cdot k})Y}_{=\tilde{X}}\underbrace{X_{k\cdot }^\top}_{=\tilde{\beta}})\rVert^2 + \lambda\lVert X\rVert^2 
\end{align*}\]</div>
<p>The objective above is equivalent to a ridge regression objective (using the notation of ridge regression, the target vector <span class="math notranslate nohighlight">\(\tilde{\vvec{y}}\)</span>, the design matrix <span class="math notranslate nohighlight">\(\tilde{X}\)</span> and the parameter vector <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span> are annotated above). We know the minimizer of this objective, it is given by the vector</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(\tilde{X}^\top \tilde{X}+\lambda I)^{-1}\tilde{X}^\top\tilde{\vvec{y}}
&amp;= (Y^\top \diag(O_{\cdot k})^2 Y +\lambda I)^{-1} Y^\top\diag(O_{\cdot k})^2D_{\cdot k}\\
&amp;=(Y^\top \diag(O_{\cdot k}) Y +\lambda I)^{-1} Y^\top\diag(O_{\cdot k})D_{\cdot k}
\end{align*}\]</div>
<p>where the last equation follows from the fact that binary values do not change when they are squared.</p>
</div>
</div>
<p>The theorem above motivates a block-coordinate descent approach, where we go in every iteration through each column of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> and update it. This procedure is described in the algorithm below.</p>
<div class="proof algorithm admonition" id="algorithm-4">
<p class="admonition-title"><span class="caption-number">Algorithm 13 </span> (MatrixCompletion)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: the dataset <span class="math notranslate nohighlight">\(D\)</span>, rank <span class="math notranslate nohighlight">\(r\)</span>, maximum number of iterations <span class="math notranslate nohighlight">\(t_{max} = 100\)</span>, and regularization weight  <span class="math notranslate nohighlight">\(\lambda = 0.1\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\((X, Y) \gets\)</span> <code class="docutils literal notranslate"><span class="pre">InitRandom</span></code><span class="math notranslate nohighlight">\((n, d, r)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(O \gets\)</span> <code class="docutils literal notranslate"><span class="pre">IndicatorObserved</span></code><span class="math notranslate nohighlight">\((D)\)</span></p></li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(t\in\{1,\ldots,t_{max}\}\)</span></p>
<ol class="simple">
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(k \in \{1, \ldots, d\}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(X_{k\cdot} \leftarrow D_{\cdot k}^{\top} \diag(O_{\cdot k})Y (Y^{\top} \diag(O_{\cdot k}) Y + \lambda I)^{-1}\)</span></p></li>
</ol>
</li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(i \in \{1, \ldots, n\}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(Y_{i\cdot} \leftarrow D_{i\cdot}\diag(O_{i\cdot}) X (X^{\top} \diag(O_{i\cdot}) X + \lambda I)^{-1}\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\((X,Y)\)</span></p></li>
</ol>
</section>
</div></section>
<section id="from-unsupervised-matrix-completion-to-supervised-behavioral-modeling">
<h3>From Unsupervised Matrix Completion to Supervised Behavioral Modeling<a class="headerlink" href="#from-unsupervised-matrix-completion-to-supervised-behavioral-modeling" title="Permalink to this headline">#</a></h3>
<p>Early recommender systems, such as those based on matrix factorization, are traditionally framed as unsupervised learning problems. The central task is to complete a sparse user-item rating matrix by learning latent factors that explain the observed ratings. This process resembled dimensionality reduction (e.g., via SVD), and the goal was to estimate the missing entries as accurately as possible.</p>
<p>However, this perspective has shifted in modern systems.
Today, most recommendation systems rely not on explicit ratings, but on <strong>implicit feedback</strong>:</p>
<ul class="simple">
<li><p>Clicks, taps, swipes</p></li>
<li><p>Watch time or scroll depth</p></li>
<li><p>Song plays, skips, replays</p></li>
<li><p>Add-to-cart or wishlist events</p></li>
</ul>
<p>These behaviors are logged at scale, and treated as training signals for supervised learning. The recommendation problem becomes:</p>
<blockquote>
<div><p><em>Given past behavior, predict whether a user will interact with an item.</em></p>
</div></blockquote>
<p>This makes modern recommendation fundamentally a prediction task, rather than a pure matrix completion task. Reformulating the unsupervised recommender system task into a supervised one has the advantages that we can test the performance. In addition, it provides guidance to the model in terms of what makes a user hooked to the screen, which is generally easier to monetize than solving the more general task to provide good recommendations.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="dim_reduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Dimensionality Reduction Techniques</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="dim_reduction_matrix_completion.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Matrix Completion</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>