
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Decision Trees &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Random Forests" href="classification_random_forests.html" />
    <link rel="prev" title="Naive Bayes" href="classification_naive_bayes.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_analytic.html">
     Analytic Solutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_gradients.html">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_sparse.html">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_svms.html">
     Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_kernel_svm.html">
     Kernel SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     From Linear Models to Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     MLPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_backprop.html">
     Backpropagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_sgd.html">
     SGD
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_conv.html">
     Convolution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_architecture.html">
     Architecture
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_mf.html">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html/issues/new?title=Issue%20on%20page%20%2Fclassification_decision_trees.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/TUEML/intro.html/edit/master/classification_decision_trees.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/classification_decision_trees.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-splits">
     Finding Splits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-gain">
     Information Gain
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stopping-criterion">
     Stopping Criterion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decision-boundary">
   Decision Boundary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Decision Trees</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-splits">
     Finding Splits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-gain">
     Information Gain
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stopping-criterion">
     Stopping Criterion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decision-boundary">
   Decision Boundary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="decision-trees">
<h1>Decision Trees<a class="headerlink" href="#decision-trees" title="Permalink to this headline">#</a></h1>
<figure class="align-center" id="got-decision-tree">
<a class="reference internal image-reference" href="_images/GOT_DT.png"><img alt="_images/GOT_DT.png" src="_images/GOT_DT.png" style="height: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">A decision tree Game of Thrones personality test.</span><a class="headerlink" href="#got-decision-tree" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Decision trees are a fundamental method in machine learning due to their inherently interpretable structure. They model decisions in a tree-like structure, where each internal node splits the outcomes based on a feature value, until we arrive at a leaf node that assigns a class prediction. This structure is so intuitive, that no further information is needed to perform the inference step. As an example, have a look at the Game of Thrones personality test decision tree above. You probably already got your character prediction without having to know what a decision tree is. This is particularly useful when you work in application domains where your project partners have no deepened mathematical training, for example in the medical domain. You can show a decision tree to experts of their field and they will be able to assess whether the predictions are sensible. Of course, this advantage gets lost if the decision tree is very big.</p>
<p>One of the most widely used algorithms for constructing decision trees is the CART (Classification and Regression Trees) algorithm, which builds binary decision trees using impurity measures such as Gini impurity (for classification).</p>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">#</a></h2>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 23 </span> (Decision Tree)</p>
<section class="definition-content" id="proof-content">
<p>A Decision Tree is a directed binary tree that represents a classifying function <span class="math notranslate nohighlight">\(f_{dt}:\mathbb{R}^d\rightarrow [0,1]^c\)</span>. The classifying function is defined recursively:
<div class="math notranslate nohighlight">
\[f_{dt}(\vvec{x})=q(\vvec{x})f_0(\vvec{x}) + (1-q(\vvec{x}))f_1(\vvec{x})\]</div>

where</p>
<ul class="simple">
<li><p>the <strong>decision function</strong> <span class="math notranslate nohighlight">\(q:\mathbb{R}^d\rightarrow \{0,1\}\)</span> performs a test on a feature of the input <span class="math notranslate nohighlight">\(\vvec{x}\)</span> that is either true (returning 1) or false (returning 0).</p></li>
<li><p>the function <span class="math notranslate nohighlight">\(f_b:\mathbb{R}^d\rightarrow [0,1]^c\)</span> returns either directly a probability vector over all classes, or it is recursively defined as
<div class="math notranslate nohighlight">
\[f_b(\vvec{x}) = q_b(\vvec{x}){f_b}_0(\vvec{x}) + (1-q_b(\vvec{x})){f_b}_1(\vvec{x}).\]</div>
</p></li>
</ul>
<p>In the tree, these functions are represented by</p>
<ul class="simple">
<li><p><strong>Decision nodes:</strong> each decision function <span class="math notranslate nohighlight">\(q_b\)</span> is represented by an internal node, performing a split based on feature values.</p></li>
<li><p><strong>Prediction nodes:</strong> each function <span class="math notranslate nohighlight">\(f_b(\vvec{x})=\vvec{p}\)</span> that directly returns a probability vector over all classes, (that is hence not defined over further recursions) is represented by a leaf node.</p></li>
</ul>
</section>
</div><p>Although decision trees are often represented as if they only store the predicted label at a leaf, in practice, the class probabilities are stored. Class probabilities are useful as a confidence measure of the prediction <span class="math notranslate nohighlight">\( \hat{y} = \argmax_y\ f_{dt}(\vvec{x}) \)</span>.</p>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 19 </span></p>
<section class="example-content" id="proof-content">
<p>The data underlying the GOT personality test dataset could for example have the features <span class="math notranslate nohighlight">\(\mathtt{x}_1,\ldots,\mathtt{x}_4\)</span>, where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_1\)</span> represents the glasses of wine per day (contiuous feature)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_2\)</span> represents the ability to get others to do their work (binary feature)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_3\)</span> represents the characteristic to burn everything down if issues persist (binary feature)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathtt{x}_4\)</span> represents the reliance on lists to organize big projects (binary feature).</p></li>
</ul>
<p>The root node (a decision node) of our GOT personality test could then be expressed by the split function
<div class="math notranslate nohighlight">
\[\begin{split}q(\vvec{x})=\begin{cases}1 &amp; \text{if }x_1\geq 5\\0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>

We build the decision tree classifier now recursively. We have
<div class="math notranslate nohighlight">
\[f_{dt}(\vvec{x})=q(\vvec{x})f_0(\vvec{x}) + (1-q(\vvec{x}))f_1(\vvec{x}).\]</div>

The function <span class="math notranslate nohighlight">\(f_0\)</span> represents the left subtree and the function <span class="math notranslate nohighlight">\(f_1\)</span> the right subtree. The left subtree is defined as
<div class="math notranslate nohighlight">
\[f_0(\vvec{x})=q_0(\vvec{x}){f_0}_0(\vvec{x})+(1-q_0(\vvec{x})){f_0}_1(\vvec{x}),\]</div>

where
<div class="math notranslate nohighlight">
\[\begin{split}q_0(\vvec{x})=\begin{cases}1 &amp; \text{if }x_2=1\\0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>

and <span class="math notranslate nohighlight">\({f_0}_0(\vvec{x})\)</span> and <span class="math notranslate nohighlight">\({f_0}_1(\vvec{x})\)</span> are returning a probability vector over the five classes, where the highest probability of <span class="math notranslate nohighlight">\({f_0}_0(\vvec{x})\)</span> is assigned for class <code class="docutils literal notranslate"><span class="pre">Cersei</span></code> and the highest probability of <span class="math notranslate nohighlight">\({f_0}_1(\vvec{x})\)</span> is assigned for class <code class="docutils literal notranslate"><span class="pre">Tyrion</span></code>.</p>
<p>The right subtree is defined as
<div class="math notranslate nohighlight">
\[f_1(\vvec{x})=q_1(\vvec{x}){f_1}_0(\vvec{x})+(1-q_1(\vvec{x})){f_0}_1(\vvec{x}),\]</div>

where
<div class="math notranslate nohighlight">
\[\begin{split}q_1(\vvec{x})=\begin{cases}1 &amp; \text{if }x_3=1\\0 &amp; \text{ otherwise}\end{cases}.\end{split}\]</div>

<span class="math notranslate nohighlight">\({f_1}_0(\vvec{x})\)</span> is represented by a prediction node and returns a probability vector where the highest probability is assigned to <code class="docutils literal notranslate"><span class="pre">Daenerys</span></code>. The function <span class="math notranslate nohighlight">\({f_0}_1\)</span> returns a subtree that is defined as
<div class="math notranslate nohighlight">
\[{f_0}_1(\vvec{x})={q_0}_1(\vvec{x}){{f_0}_1}_0(\vvec{x})+(1-{q_0}_1(\vvec{x})){{f_0}_1}_1(\vvec{x}),\]</div>

where
<div class="math notranslate nohighlight">
\[\begin{split}{q_0}_1(\vvec{x})=\begin{cases}1 &amp; \text{if }x_4=1\\0 &amp; \text{ otherwise}\end{cases}\end{split}\]</div>

and <span class="math notranslate nohighlight">\({{f_0}_1}_0(\vvec{x})\)</span> returns a probability vector with the highest probability assigned to <code class="docutils literal notranslate"><span class="pre">Arya</span></code> and <span class="math notranslate nohighlight">\({{f_0}_1}_1(\vvec{x})\)</span> assigns the highest probability to <code class="docutils literal notranslate"><span class="pre">John</span> <span class="pre">Snow</span></code>.</p>
</section>
</div></section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">#</a></h2>
<p>We provide the pseudocode for the recursive creation of the Classification and Regression Tree (CART) algorithm <span id="id1">[<a class="reference internal" href="bibliography.html#id8" title="Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classification and regression trees. Wadsworth Inc, 1984.">2</a>]</span> in <a class="reference internal" href="#dt_training">Algorithm 8</a>. The input of CART is here specified to be the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> only, but in principle CART has various hyperparameters that are here summarized by the hard-coded stopping criterion. We discuss possible stopping criteria in a subsection below. If the stopping criterium is satisfied, then the CART algorithm returns a probability vector that reflects the empirical class-distribution in the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{\cal D} (y) = \frac{\lvert\{({\bf x}, l) \in {\cal D}\mid  l = y \}\rvert}{\lvert{\cal D}\rvert}. 
\end{align*}\]</div>
<p>Otherwise, a split is found (to be discussed below), returning the decision function <span class="math notranslate nohighlight">\(q\)</span> and the split datasets <span class="math notranslate nohighlight">\(\mathcal{L}_0\cup \mathcal{L}_1 = \mathcal{D}\)</span>. CART proceeds then with the recursion.</p>
<div class="proof algorithm admonition" id="dt_training">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Classification and Regression Tree (CART))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> The dataset <span class="math notranslate nohighlight">\({\cal D} \)</span><br />
<strong>Function</strong> <span class="math notranslate nohighlight">\(\mathtt{CART}({\cal D})\)</span></p>
<ol class="simple">
<li><p><strong>if</strong> stopping criterion is satisfied:</p>
<ol class="simple">
<li><p><strong>return</strong> class probability vector <span class="math notranslate nohighlight">\(\vvec{p}\in[0,1]^c\)</span> such that <span class="math notranslate nohighlight">\(p_y = p_\mathcal{D}(y)\)</span> for <span class="math notranslate nohighlight">\(1\leq y\leq c\)</span></p></li>
</ol>
</li>
<li><p><strong>else</strong></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(q,\ \mathcal{L}_0\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1\leftarrow\mathtt{split}(\mathcal{D})\)</span></p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(q\circ \mathtt{CART}(\mathcal{L}_0) + (1-q) \circ \mathtt{CART}(\mathcal{L}_1) \)</span></p></li>
</ol>
</li>
</ol>
</section>
</div><section id="finding-splits">
<h3>Finding Splits<a class="headerlink" href="#finding-splits" title="Permalink to this headline">#</a></h3>
<p>Decision tree splits are determined using a greedy approach, meaning each split is chosen based on the immediate best improvement for the classification task. Greedy methods are usually easy to implement and to understand and provide good enough solutions, but the model decisions are often short sighted, such that the resulting tree might be bigger than it would be needed. The improvement is measured by the information gain (<span class="math notranslate nohighlight">\(IG\)</span>), which is high when the resulting child nodes have a purer class distribution than the parent node. A more detailed discussion of this concept follows below.</p>
<div class="proof algorithm admonition" id="dt_split">
<p class="admonition-title"><span class="caption-number">Algorithm 9 </span> (Split)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> A dataset <span class="math notranslate nohighlight">\({\cal D}\)</span><br />
<strong>Function</strong> <span class="math notranslate nohighlight">\(\mathtt{split}({\cal D})\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathtt{max\_ig}\)</span>, <span class="math notranslate nohighlight">\(q^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_0^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1^*\)</span> <span class="math notranslate nohighlight">\(\leftarrow 0,\ \emptyset,\ \emptyset,\ \emptyset\)</span></p></li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(k\in\{1,\ldots, d\}\)</span></p>
<ol class="simple">
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(t\in\mathtt{thresholds}(\mathtt{x}_k, \mathcal{D})\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(q(\vvec{x})=\begin{cases}1&amp; \text{ if } x_k= t\ (\mathtt{x}_k \text{ is discrete) or } x_k\geq t\ (\mathtt{x}_k \text{ is continuous})\\0&amp; \text{ otherwise}\end{cases}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_0\leftarrow \{(\vvec{x},y)\in\mathcal{D}\mid q(\vvec{x})=1\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_1\leftarrow \mathcal{D}\setminus \mathcal{L}_0\)</span></p></li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(IG(\mathcal{D},\mathcal{L}_0,\mathcal{L}_1)&gt;\mathtt{max\_ig}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathtt{max\_ig}\leftarrow IG(\mathcal{L}_0,\mathcal{L}_1,\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_0^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1^*\leftarrow \mathcal{L}_0,\ \mathcal{L}_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(q^*\gets q\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(q^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_0^*\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_1^*\)</span></p></li>
</ol>
</section>
</div><p>The function <span class="math notranslate nohighlight">\(\mathtt{thresholds}(\mathtt{x}_k,\mathcal{D})\)</span> can be defined in various ways. A straightforward choice is for discrete features <span class="math notranslate nohighlight">\(\mathtt{x}_k\)</span> to return the set of unique values in the dataset:
<div class="math notranslate nohighlight">
\[\mathtt{thresholds}(\mathtt{x}_k,\mathcal{D}) = \{x_k\mid (\vvec{x},y)\in\mathcal{D}\}.\]</div>

If the feature is continuous, then we can try as thresholds the midpoints between consecutive values of the feature. Let <span class="math notranslate nohighlight">\(\vvec{x}_{(i)}\)</span> denote the observation with the <span class="math notranslate nohighlight">\(i\)</span>-th smallest feature value <span class="math notranslate nohighlight">\(x_k\)</span>, then we define
<div class="math notranslate nohighlight">
\[\mathtt{thresholds}(\mathtt{x}_k,\mathcal{D}) = \left\{\frac{{x_{(i)}}_k + {x_{(i+1)}}_k}{2}\mid (\vvec{x}_{(i)},y),(\vvec{x}_{(i+1)},y')\in\mathcal{D}\right\}.\]</div>
</p>
</section>
<section id="information-gain">
<h3>Information Gain<a class="headerlink" href="#information-gain" title="Permalink to this headline">#</a></h3>
<p>The information gain measures how beneficial a split is to the classification task. A split is defined over a data sub-set <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> that is split into disjoint data sub-sets <span class="math notranslate nohighlight">\(\mathcal{L}=\mathcal{L}_0\cup\mathcal{L}_1\)</span>.  The more <em>pure</em> the split datasets <span class="math notranslate nohighlight">\(\mathcal{L}_0\)</span> and <span class="math notranslate nohighlight">\(\mathcal{L}_1\)</span> are in comparison to their parent dataset <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>, the higher the information gain.</p>
<div class="math notranslate nohighlight" id="equation-information-gain">
<span class="eqno">(37)<a class="headerlink" href="#equation-information-gain" title="Permalink to this equation">#</a></span>\[IG(\mathcal{L},{\cal L}_0, {\cal L}_1) = \mathrm{Impurity}({\cal L}) - \left\{ \frac{\lvert {\cal L}_0\rvert}{\lvert{\cal L}\rvert} \mathrm{Impurity}({\cal L}_0) + \frac{\lvert{\cal L}_1\rvert}{\lvert{\cal L}\rvert} \mathrm{Impurity}({\cal L}_{1}) \right\} \]</div>
<p>The function <span class="math notranslate nohighlight">\(\mathrm{Impurity}\)</span> measures the uncertainty of the prediction of the class with the highest empirical disctribution <span class="math notranslate nohighlight">\(p_{\cal L}(y)\)</span>. If the empirical distribution is very high for one class and very low for the remaining classes, then the impurity is low. We consider the following definitions of impurity:</p>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 24 </span> (Impurity measures)</p>
<section class="definition-content" id="proof-content">
<p>We define the following three impurity measures that can be used to compute the information gain:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
G({\cal L}) &amp; = 1 - \sum_{y=1}^c p_{\cal L}(y)^{2} &amp;\text{ (Gini impurity)}\\
E({\cal L}) &amp;= - \sum_{y =1}^c p_{\cal L}(y) \log p_{\cal L}(y) &amp;\text{ (Entropy)}\\
C({\cal L}) &amp;= 1 - \max_{1\leq y \leq c} p_{\cal L}(y) &amp;\text{(Self classification error)}
\end{align*}\]</div>
</section>
</div><p>For a comparison of the impurity measures, we plot them for the case of two classes. The horizontal axis indicates the empirical distribution of one of the two classes (class <span class="math notranslate nohighlight">\(y=0\)</span>).</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-24d617119f0db07dcb518ed21653fadc6a1354f3.svg" alt="Figure made with TikZ" /></p>
</div><p>We observe that there are no big differences between the three impurity plots. Gini impurity and entropy increase visibly more steeply from the completely pure class distributions where <span class="math notranslate nohighlight">\(p_{\mathcal{L}}(0)\)</span> is either zero or one.</p>
</section>
<section id="stopping-criterion">
<h3>Stopping Criterion<a class="headerlink" href="#stopping-criterion" title="Permalink to this headline">#</a></h3>
<p>Common stopping criterions, that can also be combined, are the following:</p>
<ul class="simple">
<li><p>Maximum depth is reached: in this case, we should keep track of the recursion level to see whether we exceed a predefined hyperparameter indicating the maximum tree depth.</p></li>
<li><p>Minimum number of samples per split: stop if one of the split sets <span class="math notranslate nohighlight">\(\mathcal{L}_0\)</span> or <span class="math notranslate nohighlight">\(\mathcal{L}_1\)</span> contain fewer observations than a predefined threshold.</p></li>
<li><p>No significant impurity gain: test after evoking the <span class="math notranslate nohighlight">\(\mathtt{split}\)</span> function if the impurity gain is exceeding a specified threshold.</p></li>
</ul>
<p>The stopping criterion is the main property to prohibit overfitting, which is generally associated with a big tree. In addition, pruning strategies exist to remove less significant branches after the training. But this is out of the scope of this section.</p>
</section>
</section>
<section id="decision-boundary">
<h2>Decision Boundary<a class="headerlink" href="#decision-boundary" title="Permalink to this headline">#</a></h2>
<p>The plot below indicates the decision boundary and the corresponding tree for the two moons dataset, setting the maximum tree depth to five. We observe the typical rectangular decision boundaries of the decision tree, that stem from feature-wise partitioning of the space. Each decision node partitions the feature space in two halves. The root node splits at <span class="math notranslate nohighlight">\(x[1]\leq 0.218\)</span>, resulting in a horizontal split that separates roughly the two moons from each other. The next split in the lower horizontal half (for <span class="math notranslate nohighlight">\(x[1]\leq 0.218\)</span>) is a vertical split at <span class="math notranslate nohighlight">\(x[0]\leq 0.363\)</span>. The rectangle defined by <span class="math notranslate nohighlight">\(x[1]\leq 0.218\)</span> and <span class="math notranslate nohighlight">\(x[0]\leq 0.363\)</span> is then assigned to the blue class by the prediction node when going twice left in the decision tree. This way, rectangular decision boundaries are created.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span><span class="p">,</span> <span class="n">LinearSegmentedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>


<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#a0c3ff&quot;</span><span class="p">,</span> <span class="s2">&quot;#ffa1cf&quot;</span><span class="p">])</span>
<span class="n">cm_points</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#007bff&quot;</span><span class="p">,</span> <span class="s2">&quot;magenta&quot;</span><span class="p">])</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
    <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
    <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">cm_points</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Decision tree training data and decision boundary.</span><span class="se">\n</span><span class="s2">Test Acc </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;scaled&#39;</span><span class="p">)</span>

<span class="c1"># Plot tree</span>
<span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">impurity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DT&quot;</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/classification_decision_trees_2_0.png" src="_images/classification_decision_trees_2_0.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="classification_naive_bayes.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Naive Bayes</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="classification_random_forests.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Random Forests</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>