
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Matrix Derivatives &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Exercises" href="optimization_exercises.html" />
    <link rel="prev" title="Numerical Optimization" href="optimization_numerical.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_analytic.html">
     Analytic Solutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_sparse.html">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_decision_trees.html">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_svms.html">
     Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_kernel_svm.html">
     Kernel SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     From Linear Models to Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     MLPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_backprop.html">
     Backpropagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_sgd.html">
     SGD
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_conv.html">
     Convolution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_architecture.html">
     Architecture
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_mf.html">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html/issues/new?title=Issue%20on%20page%20%2Foptimization_gradients.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/TUEML/intro.html/edit/master/optimization_gradients.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/optimization_gradients.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Matrix Derivatives</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="matrix-derivatives">
<h1>Matrix Derivatives<a class="headerlink" href="#matrix-derivatives" title="Permalink to this headline">#</a></h1>
<p>In principle, we can compute matrix derivatives using partial derivatives and familiar rules like the chain rule. However, this process can be tedious and complex. Fortunately, most of the rules you know for one-dimensional derivatives also apply to higher-order derivatives. However, unlike in one-dimensional calculus, the order of multiplication matters for matrix derivatives because matrix multiplication is generally not commutative.</p>
<p>We’ll start by examining the dimensionalities involved in matrix derivatives. For a function <span class="math notranslate nohighlight">\(f:\mathbb{R}^{n\times d}\rightarrow \mathbb{R}\)</span> that maps matrices to real values, we can define its derivative in two ways: as the gradient or the Jacobian.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial f(X)}{\partial X} &amp;=
    \begin{pmatrix}
    \frac{\partial f(X)}{\partial X_{11}} &amp; \ldots &amp; \frac{\partial f(X)}{\partial X_{n1}}\\
    \vdots &amp; \ddots &amp; \vdots\\ 
    \frac{\partial f(X)}{\partial X_{1d}} &amp; \ldots &amp; \frac{\partial f(X)}{\partial X_{nd}}
    \end{pmatrix}\in\mathbb{R}^{d\times n} &amp;\text{(Jacobian)}\\
    \nabla{f(X)} &amp;=
    \begin{pmatrix}
    \frac{\partial f(X)}{\partial X_{11}} &amp; \ldots &amp; \frac{\partial f(X)}{\partial X_{1d}}\\
    \vdots &amp; \ddots &amp; \vdots\\ 
    \frac{\partial f(X)}{\partial X_{n1}} &amp; \ldots &amp; \frac{\partial f(X)}{\partial X_{nd}}
    \end{pmatrix}\in\mathbb{R}^{n\times d} &amp;\text{(Gradient)}
\end{align*}\]</div>
<p>You might notice that the Jacobian is the transposed of the gradient, and vice versa.
<div class="math notranslate nohighlight">
\[\nabla_\vvec{x} \vvec{f}(\vvec{x}) = \left(\frac{\partial \vvec{f}(\vvec{x})}{\partial\vvec{x}}\right)^\top\]</div>

Be aware that the notation above is not used by all authors. Some authors define the Jacobian as we define the gradient here. To remember what is what, consider the gradient descent update rule, where we subtract the scaled gradient from the iterates. For functions mapping to real values (such as loss or objective functions), the gradient must have the same dimensionality as the function’s arguments; otherwise, subtraction would not be possible.</p>
<p>From the definition of matrix derivatives, we can also infer the definition of a vector derivative for a function <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\rightarrow \mathbb{R}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial f(\vvec{x})}{\partial \vvec{x}} &amp;=
    \begin{pmatrix}
    \frac{\partial f(\vvec{x})}{\partial x_1} &amp; \ldots &amp; \frac{\partial f(\vvec{x})}{\partial x_d}
    \end{pmatrix}\in\mathbb{R}^{1\times d} &amp; \text{(Jacobian)}\\
    \nabla_\vvec{x} f(\vvec{x}) &amp;=
    \begin{pmatrix}
    \frac{\partial f(\vvec{x})}{\partial x_1} \\ \vdots \\ \frac{\partial f(\vvec{x})}{\partial x_d}
    \end{pmatrix}\in\mathbb{R}^{d} &amp;\text{(Gradient)}
\end{align*}\]</div>
<p>If we have a function that maps to a vector space, then we can compute the partial derivatives for each coordimate of the function value. For example, if we have a function mapping from real values to the <span class="math notranslate nohighlight">\(c\)</span>-dimensional real-valued vector space <div class="math notranslate nohighlight">
\[\begin{split}\vvec{f}:\mathbb{R}\rightarrow \mathbb{R}^{c},\ \vvec{f}(x)=\begin{pmatrix}f_1(x)\\\vdots\\f_c(x)\end{pmatrix},\end{split}\]</div>
 then the Jacobian and gradient are defined as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial \vvec{f}(x)}{\partial x} &amp;=
    \begin{pmatrix}
    \frac{\partial f_1(x)}{\partial x} \\ \vdots \\ \frac{\partial f_c(x)}{\partial x}
    \end{pmatrix} \in\mathbb{R}^{c} &amp; \text{(Jacobian)}\\
    \nabla_x \vvec{f}(x) &amp;=
    \begin{pmatrix}
    \frac{\partial f_1(x)}{\partial x} &amp; \ldots &amp; \frac{\partial f_c(x)}{\partial x}
    \end{pmatrix} \in\mathbb{R}^{1\times c} &amp;\text{(Gradient)}
\end{align*}\]</div>
<p>Note that the Jacobian preserves now the dimensionality of the output of the function: function vales are in <span class="math notranslate nohighlight">\(\mathbb{R}^c\)</span> and so is the Jacobian. Likewise, we can define the derivatives for a function <span class="math notranslate nohighlight">\(\vvec{f}:\mathbb{R}^d\rightarrow \mathbb{R}^{c}\)</span> from a vector space to a vector space:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial \vvec{f}(\vvec{x})}{\partial \vvec{x}} &amp;=
    \begin{pmatrix}
    \frac{\partial f_1(x)}{\partial x_1}&amp; \ldots &amp;  \frac{\partial f_1(x)}{\partial x_d}\\ 
    \vdots &amp; \ddots &amp; \vdots \\ 
    \frac{\partial f_c(x)}{\partial x_1} &amp;\ldots &amp; \frac{\partial f_c(x)}{\partial x_d}
    \end{pmatrix} \in\mathbb{R}^{c\times d} &amp; \text{(Jacobian)}\\
    \nabla_\vvec{x}\vvec{f}(\vvec{x}) &amp;=
    \begin{pmatrix}
    \frac{\partial f_1(x)}{\partial x_1}&amp; \ldots &amp;  \frac{\partial f_c(x)}{\partial x_1}\\ 
    \vdots &amp; \ddots &amp; \vdots \\ 
    \frac{\partial f_1(x)}{\partial x_d} &amp;\ldots &amp; \frac{\partial f_c(x)}{\partial x_d}
    \end{pmatrix} \in\mathbb{R}^{d\times c} &amp;\text{(Gradient)}
\end{align*}\]</div>
<p>Of course we could now consider more cases, like a function mapping a matrix to a matrix. Unfortunately, from this point on, it gets really complicated. There are multiple ways to define such derivatives - as tensors or as specifically structured matrices. We’re going to keep it comparatively simple and circumvent these cases in this course.<br />
We can now concatenate these derivatives according to linearity and the chain rule for matrix derivatives.</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 10 </span> (The Jacobian is linear)</p>
<section class="theorem-content" id="proof-content">
<p>For any function whose Jacobian is defined as a matrix of partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial \vvec{f}(\vvec{x})}{\partial \vvec{x}} = \begin{pmatrix}\frac{\partial f_j(\vvec{x})}{\partial x_i}\end{pmatrix}_{i,j}\)</span> for some indexes <span class="math notranslate nohighlight">\(i,j\)</span>, the Jacobian is linear:
<div class="math notranslate nohighlight">
\[\frac{\partial\alpha\vvec{f}(\vvec{x})+\vvec{g}(\vvec{x})}{\partial\vvec{x}}
        =\alpha\frac{\partial\vvec{f}(\vvec{x})}{\partial\vvec{x}}+\frac{\partial\vvec{g}(\vvec{x})}{\partial\vvec{x}}\]</div>
</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof follows from the linearity of the partial derivatives:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial \alpha\vvec{f}(\vvec{x})+\vvec{g}(\vvec{x})}{\partial \vvec{x}} &amp;= \begin{pmatrix}\frac{\partial \alpha f_j(\vvec{x}) +g_j(\vvec{x})}{\partial x_i}\end{pmatrix}_{i,j}\\
&amp;= \begin{pmatrix}\alpha\frac{\partial  f_j(\vvec{x})}{\partial x_i}+\frac{\partial g_j(\vvec{x})}{\partial x_i}\end{pmatrix}_{i,j}\\
&amp;= \alpha\begin{pmatrix}\frac{\partial  f_j(\vvec{x})}{\partial x_i}\end{pmatrix}_{i,j} + \begin{pmatrix}\frac{\partial g_j(\vvec(x))}{\partial x_i}\end{pmatrix}_{i,j}\\
&amp;=\alpha\frac{\partial\vvec{f}(\vvec{x})}{\partial\vvec{x}}+\frac{\partial\vvec{g}(\vvec{x})}{\partial\vvec{x}}
\end{align*}\]</div>
</div>
<div class="proof theorem admonition" id="theorem-1">
<p class="admonition-title"><span class="caption-number">Theorem 11 </span> (Chain Rule for the Jacobian)</p>
<section class="theorem-content" id="proof-content">
<p>For any continuously differentiable functions <span class="math notranslate nohighlight">\(\vvec{f}:\mathbb{R}^c\rightarrow \mathbb{R}^p\)</span> and <span class="math notranslate nohighlight">\(\vvec{g}:\mathbb{R}^d\rightarrow \mathbb{R}^c\)</span>, the Jacobian of the composition <span class="math notranslate nohighlight">\(\vvec{f}\circ\vvec{g}\)</span> is given by the chain rule:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
        \frac{\partial \vvec{f}(\vvec{g}(\vvec{x}))}{\partial\vvec{x}}
        &amp;= \underbrace{\frac{\partial \vvec{f}(\vvec{g}(\vvec{x}))}{\partial \vvec{g}(\vvec{x})}}_{p\times c}\underbrace{\frac{\partial \vvec{g}(\vvec{x})}{\partial \vvec{x}}}_{c\times d} 
    \end{align*}\]</div>
</section>
</div><div class="proof example admonition" id="example-2">
<p class="admonition-title"><span class="caption-number">Example 10 </span></p>
<section class="example-content" id="proof-content">
<p>Consider the exponential function applied element-wise to a vector
<div class="math notranslate nohighlight">
\[\exp(\vvec{x}) = (\exp(x_1),\ldots,\exp(x_d)).\]</div>

The gradient of this function is the diagonal matrix
<div class="math notranslate nohighlight">
\[\nabla \exp(\vvec{x}) = \diag(\exp(\vvec{x})).\]</div>
</p>
</section>
</div></section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="optimization_numerical.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Numerical Optimization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="optimization_exercises.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Exercises</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>