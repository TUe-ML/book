
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Low Rank Matrix Factorization &#8212; Data Mining and Machine Learning Jupyter Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dim_reduction_mf';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Principal Component Analysis" href="dim_reduction_pca.html" />
    <link rel="prev" title="Dimensionality Reduction Techniques" href="dim_reduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Data Mining and Machine Learning Jupyter Book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Data Mining and Machine Learning Jupyter Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linalg.html">Linear Algebra</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="linalg_spaces.html">Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_normed_vs.html">Normed Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="linalg_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="optimization.html">Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="optimization_problems.html">Optimization Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_convex.html">Convex Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_analytic.html">Analytic Solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_numerical.html">Numerical Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_gradients.html">Matrix Derivatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimization_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="regression_objective.html">Regression Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_functions.html">Regression Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_optimization.html">Minimizing the RSS</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_bias_var.html">The Bias-Variance Tradeoff</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_sparse.html">The Sparse Regression Task</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_ridge.html">Ridge Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_lasso.html">Lasso</a></li>
<li class="toctree-l2"><a class="reference internal" href="regression_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="classification.html">Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="classification_problem.html">Classification Objective</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_knn.html">K-Nearest Neighbor</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_naive_bayes.html">Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_random_forests.html">Random Forests</a></li>

<li class="toctree-l2"><a class="reference internal" href="classification_svms.html">Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_kernel_svm.html">Kernel SVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="neuralnets.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_intro.html">From Linear Models to Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_mlps.html">MLPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_backprop.html">Backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_sgd.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_conv.html">Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="neuralnets_pooling.html">Pooling</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="dim_reduction.html">Dimensionality Reduction Techniques</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Low Rank Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_pca.html">Principal Component Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="dim_reduction_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="clustering.html">Clustering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="clustering_k_means.html">k-Means</a></li>



<li class="toctree-l2"><a class="reference internal" href="clustering_k_means_mf.html">k-Means is MF</a></li>


<li class="toctree-l2"><a class="reference internal" href="clustering_kernel_kmeans.html">Kernel k-means</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering_spectral.html">Spectral Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering_exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tue-ml/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/edit/main/dim_reduction_mf.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tue-ml/book/issues/new?title=Issue%20on%20page%20%2Fdim_reduction_mf.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/dim_reduction_mf.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Low Rank Matrix Factorization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formal-problem-definition">Formal Problem Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-matrix-completion-recommender-system">A Simple Matrix Completion Recommender System</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-factorization">Interpretation of the Factorization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-when-we-increase-the-rank">What Happens When We Increase the Rank?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-mf-on-observed-entries">Low-Rank MF on Observed Entries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-unsupervised-matrix-completion-to-supervised-behavioral-modeling">From Unsupervised Matrix Completion to Supervised Behavioral Modeling</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="low-rank-matrix-factorization">
<h1>Low Rank Matrix Factorization<a class="headerlink" href="#low-rank-matrix-factorization" title="Link to this heading">#</a></h1>
<figure class="align-center" id="netflix">
<a class="reference internal image-reference" href="_images/Netflix_Screenshot.png"><img alt="_images/Netflix_Screenshot.png" src="_images/Netflix_Screenshot.png" style="height: 300px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Low rank matrix factorization can be used for recommender systems like Netflix</span><a class="headerlink" href="#netflix" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Netflix gained its big popularity back in the days because it focused on its recommender system strength. They personalized content discovery, ensuring users stayed engaged by always having something new to watch. The task of <em>recommendation</em> is unsupervised, we don’t know the ground truth recommendations, as opposed to supervised tasks, where we have a label or target variable.  All we have are past user ratings, from which we try to derive common patterns that allow us to provide recommendations.</p>
<p>Let’s go through an example to get a clearer understanding of the recommender task. Imagine we represent all users and movies in a matrix, where each entry corresponds to a user’s rating for a movie. Then we get a massive, sparse matrix (since most users have only rated a small fraction of available movies). The challenge is to predict the missing ratings so that Netflix can suggest movies that a user is likely to enjoy. For example, the user-movie matrix could look like that:</p>
<style>
.custom-table-container th {
    writing-mode: vertical-lr;
    vertical-align: bottom;
    width: 20%;
}
</style>
<div class="custom-table-container">
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>User</p></th>
<th class="head"><p>Star Wars</p></th>
<th class="head"><p>Interstellar</p></th>
<th class="head"><p>Blade Runner</p></th>
<th class="head"><p>Tron</p></th>
<th class="head"><p>2001: Space O.</p></th>
<th class="head"><p>Mars Attacks</p></th>
<th class="head"><p>Dune</p></th>
<th class="head"><p>Matrix</p></th>
<th class="head"><p>Robo Cop</p></th>
<th class="head"><p>Aliens</p></th>
<th class="head"><p>Terminator</p></th>
<th class="head"><p>Solaris</p></th>
<th class="head"><p>Avatar</p></th>
<th class="head"><p>12 Monkeys</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Grace</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🙈</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Carol</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
</tr>
<tr class="row-even"><td><p>Alice</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p>🙈</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Bob</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>Eve</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>🙈</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Chuck</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🤩</p></td>
<td><p>🤩</p></td>
<td><p></p></td>
<td><p>🙈</p></td>
<td><p>🤩</p></td>
</tr>
</tbody>
</table>
</div>
</div><p>We have six users and 14 movies, that are rated either as <em>I like it</em> (🤩) or <em>not for me</em> (🙈). If no emoji is indicated, then the corresponding user has not seen the movie yet. This example matrix of user-movie preferences exhibits two patterns of preferences. The first pattern consists of the movies <em>Star Wars, Interstellar, Blade Runner, Tron, 2001: Space Odyssey, Matrix, Solaris,</em> and <em>12 Monkeys</em>. This set of movies is popular in the user group of Carol, Alice and Chuck: every movie of the set is liked at least by two of the three users. Hence, we might consider to recommend each person of that group a movie from this set that the person has not watched yet. For example, we could recommend to Carol to watch <em>2001: Space Odyssey</em>.
Likewise, we identify a second pattern of movies that is popular among the group of Grace, Bob, Eve and Chuck. This pattern encompasses the movies <em>Star Wars, Mars Attacks, Matrix, Robo Cop, Aliens, Terminator</em>, and <em>Avatar</em>.</p>
<p>Let’s visualize the model that we are looking for as a matrix. Below you see the abstract representation of the groups of users and movies as a matrix. The first group of users (Carol, Alice and Chuck) and their corresponding set of (largely) liked movies is visualized in blue, and the second set of users (Grace, Bob, Eve and Chuck) and their set of movies are visualized in red.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 sd-d-flex-row sd-align-major-center docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-af253f8d53856b1690458eb7bfcf4a46fa3ff074.png" alt="Figure made with TikZ" /></p>
</div></div>
</div>
</div>
<p>The matrix above reflects the positive movie indications (🤩) by the nonzero cells, that are the ones that are colored. The last row reflects Chuck, who adheres to both movie patterns, that overlap in the movies <em>Star Wars</em> and <em>Matrix</em>. Hence, we see two cells in the last row with overlapping colors. The user-movie matrix can be decomposed into the sum of two matrices, where each matrix reflects one user-movie group.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 sd-d-flex-row sd-align-major-center docutils">
<div class="sd-row sd-g-0 sd-g-xs-0 sd-g-sm-0 sd-g-md-0 sd-g-lg-0 docutils">
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-72923ba1e222095f38264a363b9dc78570f93b13.png" alt="Figure made with TikZ" /></p>
</div></div>
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<p><span class="math notranslate nohighlight">\(+\)</span></p>
</div>
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-956cc3350ff20d9115ffb6160869fed6cd98f51b.png" alt="Figure made with TikZ" /></p>
</div></div>
</div>
</div>
<p>Furthermore, each of the single user-group matrices can be represented by an outer product of a user- and a user-vector. And the sum of outer product matrices reflects a low-dimensional matrix product. In this casse, we have a product of two dimensionality two – one for each user-movie matrix.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 sd-d-flex-row sd-align-major-center docutils">
<div class="sd-row sd-g-0 sd-g-xs-0 sd-g-sm-0 sd-g-md-0 sd-g-lg-0 docutils">
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-1863ac78ab9dd75305fbc4494ff2c8dbe5ce8a39.png" alt="Figure made with TikZ" /></p>
</div></div>
<div class="sd-col sd-d-flex-column sd-col-auto sd-col-xs-auto sd-col-sm-auto sd-col-md-auto sd-col-lg-auto sd-align-major-center docutils">
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-40e85b57642ae1bbc40c34298b8b03a52db0f3dc.png" alt="Figure made with TikZ" /></p>
</div></div>
</div>
</div>
<p>The low-dimensional matrix product describing our user-movie preferences is now represented by two user-vectors (the columns of the left matrix) and two movie-vectors (the rows of the right matrix). Each pair of user- and movie-vectors indicates a group, exhibiting the same movie preferences. Note, that the original <span class="math notranslate nohighlight">\(6\times 14\)</span> matrix is now compressed into a <span class="math notranslate nohighlight">\(6\times 2\)</span>-matrix and a <span class="math notranslate nohighlight">\(2\times 14\)</span> matrix. While the original matrix contains <span class="math notranslate nohighlight">\(6\cdot 14 = 84\)</span> elements, the low-dimensional product needs only <span class="math notranslate nohighlight">\(2\cdot 6 + 2\cdot 14 = 40\)</span> elememts to be stored. That is roughly the idea of recommender systems: using reoccurences in the behaviour or similarities among users and movies to compress the data, and to use the compressed data representation to make recommendations.</p>
<section id="formal-problem-definition">
<h2>Formal Problem Definition<a class="headerlink" href="#formal-problem-definition" title="Link to this heading">#</a></h2>
<p>We formalize the idea to obtain recommendations by compressing the data as the task to minimize the squared Frobenius-norm error of a low-rank matrix product and the data matrix.</p>
<div class="tip admonition">
<p class="admonition-title">Task (Rank-r Matrix Factorization)</p>
<p><strong>Given</strong> a data matrix <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}\)</span> and a rank <span class="math notranslate nohighlight">\(r&lt;\min\{n,d\}\)</span>.</p>
<p><strong>Find</strong> matrices <span class="math notranslate nohighlight">\(X\in \mathbb{R}^{d\times r}\)</span> and <span class="math notranslate nohighlight">\(Y\in\mathbb{R}^{n\times r}\)</span> whose product approximates the data matrix:</p>
<div class="amsmath math notranslate nohighlight" id="equation-da81cf77-fd5f-4e7d-95c2-17edafaed378">
<span class="eqno">(54)<a class="headerlink" href="#equation-da81cf77-fd5f-4e7d-95c2-17edafaed378" title="Permalink to this equation">#</a></span>\[\begin{align}
    \min_{X,Y}&amp;\lVert D- YX^\top\rVert^2 &amp; \text{s.t. } X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}
\end{align}\]</div>
<p><strong>Return</strong> the low-dimensional approximation of the data <span class="math notranslate nohighlight">\((X,Y)\)</span>.</p>
</div>
<p>Note that the Rank-r matrix factorization task is not directly suitable to return recommendations. It only describes the task to compress a given data matrix into a low-dimensional product. To provide recommendations, we need to fill in missing values. We will discuss later how we can do this with a low dimensional matrix factorization.<br />
First, we analyze properties of the objective, as it turns out, the low-dimensional matrix factorization task is nonconvex.</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 28 </span></p>
<section class="theorem-content" id="proof-content">
<p>The rank-<span class="math notranslate nohighlight">\(r\)</span> matrix factorization problem, defined for a matrix <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}\neq\mathbf{0}\)</span> and a rank <span class="math notranslate nohighlight">\(1\leq r&lt;\min\{n,d\}\)</span> as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \min_{X,Y}&amp; RSS(X,Y)=\lVert D- YX^\top\rVert^2 &amp; \text{s.t. } X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}
\end{align*}\]</div>
<p>is a <strong>nonconvex optimization problem</strong>.</p>
</section>
</div><p>The proof follows from the fact that the set of global minimizers is not a convex set.</p>
<div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. We show that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is not a convex function. Therefore we assume first that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is a convex function and show then that this assumption leads to a contradiction. Assuming that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is a convex function means that the following inequality has to hold for all matrices <span class="math notranslate nohighlight">\(X_1,X_2\in\mathbb{R}^{d\times r}\)</span> and <span class="math notranslate nohighlight">\(Y_1,Y_2\in\mathbb{R}^{n\times r}\)</span> and <span class="math notranslate nohighlight">\(\alpha\in[0,1]\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e14ca69a-b870-4c88-8d1e-9f6fbc54fafa">
<span class="eqno">(55)<a class="headerlink" href="#equation-e14ca69a-b870-4c88-8d1e-9f6fbc54fafa" title="Permalink to this equation">#</a></span>\[\begin{align}
        RSS(\alpha X_1+ (1-\alpha)X_2,\alpha Y_1 + (1-\alpha)Y_2) \leq \alpha RSS(X_1,Y_1) + (1-\alpha)RSS(X_2,Y_2).
\end{align}\]</div>
<p>For any global minimizer <span class="math notranslate nohighlight">\((X,Y)\)</span> of the rank-<span class="math notranslate nohighlight">\(r\)</span> MF problem, <span class="math notranslate nohighlight">\((\gamma X, \frac{1}{\gamma} Y)\)</span> is also a global minimizer for <span class="math notranslate nohighlight">\(\gamma\neq 0\)</span>.
However, for <span class="math notranslate nohighlight">\(\alpha=1/2\)</span> the convex combination attains a function value of</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    RSS(\alpha X + (1-\alpha) (\gamma X), \alpha Y+(1-\alpha)(\tfrac1\gamma Y)) &amp;=RSS\left(\tfrac12 X + \tfrac12 (\gamma X), \tfrac12 Y+\tfrac12(\tfrac1\gamma Y)\right)\\
    &amp;= RSS\left(\tfrac12(1+\gamma) X, \tfrac12(1+ \tfrac1\gamma) Y\right)\\
    &amp;=\lVert D-\tfrac14(1+\gamma)(1+\tfrac1\gamma)YX^\top\rVert^2.
\end{align*}\]</div>
<p>We observe that the approximation error in the last equation goes to infinity if <span class="math notranslate nohighlight">\(\gamma\rightarrow \infty\)</span>. Hence, there exists multiple <span class="math notranslate nohighlight">\(\gamma&gt;0\)</span> for which the <span class="math notranslate nohighlight">\(RSS\)</span> of the convex combination of two global minimizers is larger than zero. This contradicts the assumption that the <span class="math notranslate nohighlight">\(RSS(X,Y)\)</span> is convex.</p>
</div>
</div>
<p>We observe from the proof that there are infinitely many global minimizers for the low-dimensional matrix factorization task. Let’s explore this set of global minimizers by means of an example in one dimension.</p>
<div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 21 </span> (One-dimensional matrix factorization)</p>
<section class="example-content" id="proof-content">
<p>The most <em>easy</em> case of a low-dimensional matrix factorization is the factorization of a single number. Let’s take for example the factorization of the number one into a product of two factors <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, having the objective function  <span class="math notranslate nohighlight">\(f(x_1,x_2) = (1-x_1x_2)^2\)</span>. We plot the graph of the objective function together with three solutions: <span class="math notranslate nohighlight">\((x_1,x_2)=(2,0.5)\)</span>, <span class="math notranslate nohighlight">\((1,1)\)</span>, and <span class="math notranslate nohighlight">\((0.5,2)\)</span>.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-8e5b26bf64acbe9c637716e4bc0ecf64fe70aafb.png" alt="Figure made with TikZ" /></p>
</div><p>We can observe the nonconvexity of this function by connecting the solution of <span class="math notranslate nohighlight">\((0.5,2)\)</span> with <span class="math notranslate nohighlight">\((2,0.5)\)</span> with a straight line. If the function would be convex, then the graph ofthe function would be under or on the line. Under the line is not possible in this case, since the solution points we picked are global minimizers. Hence, the graph should be flat (<span class="math notranslate nohighlight">\(y\)</span>-value equal to zero) on the line between the two solutions, but we see that the loss increases to the right. However, we also see that the loss function doesn’t look as if there is a multitude of valleys, that are local minima. That gives us hope, that the low-rank matrix factorization task is not that difficult to solve.</p>
</section>
</div></section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h2>
<p>In most cases, nonconvexity of an objective implies that we probably have to live with the fact that we can not determine the global minimum, and that we can only hope to get good local minima by numerical optimization methods such as gradient descent. The rank-<span class="math notranslate nohighlight">\(r\)</span> matrix factorization problem is here an exemption to the rule, since we can derive one global minimum by SVD.</p>
<div class="proof theorem admonition" id="theorem-2">
<p class="admonition-title"><span class="caption-number">Theorem 29 </span> (Truncated SVD)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(D=U\Sigma V^\top\in\mathbb{R}^{n\times d}\)</span> be the singular decomposition of <span class="math notranslate nohighlight">\(D\)</span>. Then the global minimizers <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>  of the rank-<span class="math notranslate nohighlight">\(r\)</span> MF problem</p>
<div class="amsmath math notranslate nohighlight" id="equation-0da2f500-122e-4c53-a77a-f3c3a85b04be">
<span class="eqno">(56)<a class="headerlink" href="#equation-0da2f500-122e-4c53-a77a-f3c3a85b04be" title="Permalink to this equation">#</a></span>\[\begin{align}
\min_{X,Y}\lVert D-YX^\top\rVert^2 \text{s.t. } X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}.
\end{align}\]</div>
<p>satisfy</p>
<div class="amsmath math notranslate nohighlight" id="equation-299a9c03-69d5-4d5e-8725-b62a4cdebc07">
<span class="eqno">(57)<a class="headerlink" href="#equation-299a9c03-69d5-4d5e-8725-b62a4cdebc07" title="Permalink to this equation">#</a></span>\[\begin{align} 
YX^\top = U_{\cdot \mathcal{R}}\Sigma_{\mathcal{R}\mathcal{R}}V_{\cdot \mathcal{R}}^\top, \text{ where }\mathcal{R}=\{1,\ldots, r\}.
\end{align}\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof follows from
the orthogonal invariance of the Frobenius norm, yielding:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \min_{X,Y}&amp;\lVert D-YX^\top\rVert^2 = \lVert \Sigma - U^\top YX^\top V\rVert^2 
\end{align*}\]</div>
</div>
</section>
<section id="a-simple-matrix-completion-recommender-system">
<h2>A Simple Matrix Completion Recommender System<a class="headerlink" href="#a-simple-matrix-completion-recommender-system" title="Link to this heading">#</a></h2>
<p>We can use truncated SVD to compute a low-rank approximation of the data.
How can we use this to provide recommendations? After all, we need a complete matrix in order to compute the SVD. For now, we consider a quick hack: we fill the missing values with the mean (neutral rating) and compute the truncated SVD with the hope that the SVD reconstructs mainly the given ratings that are often not equal to the mean rating, such that the imputed values get a more accurate prediction of a rating with the SVD.</p>
<p>Let’s go through an example. The table below shows a movie-ratings database that is filled by some ratings, but not all movies have been seen by all costumers and we want to fill in the missing values with the approximate rating that would be given by the user if the had seen the movie.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Id</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(A\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(B\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(C\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(D\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>★★★★★</p></td>
<td><p>?</p></td>
<td><p>★★☆☆☆</p></td>
<td><p>★☆☆☆☆</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>?</p></td>
<td><p>★☆☆☆☆</p></td>
<td><p>★★★★★</p></td>
<td><p>?</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>★★★★★</p></td>
<td><p>★☆☆☆☆</p></td>
<td><p>★★★★★</p></td>
<td><p>★★☆☆☆</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>★★★★★</p></td>
<td><p>?</p></td>
<td><p>★★★★★</p></td>
<td><p>★★★☆☆</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>★★★★★</p></td>
<td><p>★★★★★</p></td>
<td><p>?</p></td>
<td><p>?</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>?</p></td>
<td><p>★★★★☆</p></td>
<td><p>★★★★★</p></td>
<td><p>★★★☆☆</p></td>
</tr>
</tbody>
</table>
</div>
<p>We apply our quick hack and replace the unobserved entries with the mean rating <span class="math notranslate nohighlight">\(\mu=\frac{1+2+3+4+5}{5}=3\)</span>. This gives us the following data matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">D</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[5, 3, 1, 1],
       [3, 1, 5, 3],
       [2, 1, 5, 3],
       [4, 3, 4, 2],
       [5, 5, 3, 1],
       [3, 1, 5, 3]])
</pre></div>
</div>
</div>
</div>
<p>We visualize the rating matrix with the image below. A white pixel indicates a low rating and a blue pixel indicates a higher rating.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/0829a5f00392bf86e2b72645919e7f24ec8cacbe8ce9c6518c0f7824431e5e14.png" src="_images/0829a5f00392bf86e2b72645919e7f24ec8cacbe8ce9c6518c0f7824431e5e14.png" />
</div>
</div>
<p>We compute the SVD of the matrix <span class="math notranslate nohighlight">\(D\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vᵀ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We get a rank-2 approximation of <span class="math notranslate nohighlight">\(D\)</span> by truncating the SVD to two singular values and vectors:
<div class="math notranslate nohighlight">
\[D\approx U_{\cdot \{1,2\}}\Sigma_{\{1,2\}\{1,2\}}V_{\cdot \{1,2\}}^\top.\]</div>

The tri-factorization of SVD can be expressed as a factorization into two matrices by making an arbitrary split into the product of two matrices. For example, we could set <span class="math notranslate nohighlight">\(Y=U_{\cdot \{1,2\}}\Sigma_{\{1,2\}\{1,2\}}^{1/2}\)</span> and <span class="math notranslate nohighlight">\(X=V_{\cdot \{1,2\}}\Sigma_{\{1,2\}\{1,2\}}^{1/2}\)</span>. This way, we can expect that <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are similarly scaled, since they both are matrices with unitary vectors that are scaled by the square roots of the singular values. However, in principle, any split of the SVD product into two matrices will do.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The matrix <span class="math notranslate nohighlight">\(A^{1/2}\)</span> is defined as the matrix that satisfies the equation <span class="math notranslate nohighlight">\(A^{1/2}A^{1/2}=A\)</span>. Not for all matrices <span class="math notranslate nohighlight">\(A\)</span> exists such a matrix <span class="math notranslate nohighlight">\(A^{1/2}\)</span>. However, for nonnegative, diagonal matrices <span class="math notranslate nohighlight">\(\Sigma\)</span>, the matrix <span class="math notranslate nohighlight">\(\Sigma^{1/2}=\diag(\sqrt{\sigma_1},\ldots, \sqrt{\sigma_r})\)</span> exists.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Vᵀ</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">Y</span><span class="p">,</span><span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[-1.3 ,  1.26],
        [-1.61, -0.85],
        [-1.46, -1.04],
        [-1.71,  0.21],
        [-1.81,  1.27],
        [-1.61, -0.85]]),
 array([[-2.3 ,  1.08],
        [-1.49,  1.38],
        [-2.43, -1.36],
        [-1.35, -0.92]]))
</pre></div>
</div>
</div>
</div>
<p>The low rank approximation can be used to give recommendations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[4.34, 3.68, 1.43, 0.59],
       [2.78, 1.23, 5.08, 2.97],
       [2.23, 0.75, 4.97, 2.93],
       [4.16, 2.84, 3.88, 2.13],
       [5.53, 4.46, 2.68, 1.28],
       [2.78, 1.23, 5.08, 2.97]])
</pre></div>
</div>
</div>
</div>
<p>If we compare the matrix above with the matrix having missing values, then we see that the low rank approximation gives some tendencies for recommendations, but often no very clear recommendation indications. This is not very surprising, since we had just a small dataset and comparatively many missing values. The rank-2 approximation is already a bit too well adapting to the missing values neutral rating.</p>
<div class="amsmath math notranslate nohighlight" id="equation-9c7bd336-5ffc-4aa2-9cb9-e2f7c2291309">
<span class="eqno">(58)<a class="headerlink" href="#equation-9c7bd336-5ffc-4aa2-9cb9-e2f7c2291309" title="Permalink to this equation">#</a></span>\[\begin{align}
  \begin{pmatrix}
    5 &amp; \mu &amp; 1 &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; \mu  \\\
    2 &amp; 1 &amp; 5 &amp; 3 \\
    4 &amp; \mu &amp; 4 &amp; 2\\
    5 &amp; 5 &amp; \mu &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; 3 \\
  \end{pmatrix}
  &amp;\approx
  \begin{pmatrix}
    4.3 &amp; 3.7 &amp; 1.4 &amp; 0.6\\
    2.8 &amp; 1.2 &amp; 5.1 &amp; 3.0\\
    2.2 &amp; 0.7 &amp; 5.0 &amp; 2.9\\
    4.2 &amp; 2.8 &amp; 3.9 &amp; 2.1\\
    5.5 &amp; 4.5 &amp; 2.7 &amp; 1.3\\
    2.8 &amp; 1.2 &amp; 5.1 &amp; 3.0
  \end{pmatrix}
\end{align}\]</div>
<p>If we plot the original data and the approximation next to another, then we also see that there are no big differences.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/af679276630fcca081ab9cde6bf74784f56ff6f7092d15553214ec3ddc28e1ae.png" src="_images/af679276630fcca081ab9cde6bf74784f56ff6f7092d15553214ec3ddc28e1ae.png" />
</div>
</div>
<section id="interpretation-of-the-factorization">
<h3>Interpretation of the Factorization<a class="headerlink" href="#interpretation-of-the-factorization" title="Link to this heading">#</a></h3>
<p>The rank-two matrix product is composed by the sum of two outer products. Every outer product indicates the interaction of a user-pattern with a movie pattern. Hence, looking at the outer-product decomposition is useful for interpretation purposes. The equation below shows again the rank-two approximation.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  \begin{pmatrix}
    5 &amp; \mu &amp; 1 &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; \mu  \\\
    2 &amp; 1 &amp; 5 &amp; 3 \\
    4 &amp; \mu &amp; 4 &amp; 2\\
    5 &amp; 5 &amp; \mu &amp; 1 \\
    \mu &amp; 1 &amp; 5 &amp; 3 \\
  \end{pmatrix}
  &amp;\approx
  \begin{pmatrix}
    -0.3 &amp; 0.5\\
    -0.4 &amp; -0.4\\
    -0.4 &amp; -0.4\\
    -0.4 &amp; 0.1\\
    -0.5 &amp; 0.5\\
    -0.4 &amp; -0.4\\
  \end{pmatrix}
  \begin{pmatrix}
    -9.0 &amp; -5.8 &amp; -9.5 &amp; -5.3\\
    2.6 &amp; 3.3 &amp; -3.3 &amp; -2.2\\
  \end{pmatrix}
\end{align*}\]</div>
<p>Every user’s preferences are approximated by a linear combination of the rows in the second matrix. The rows in the second matrix have an interpretation as movie patterns. For example, the first user adheres to the first movie pattern with a factor of <span class="math notranslate nohighlight">\(-0.3\)</span> and to the second movie pattern with a factor of <span class="math notranslate nohighlight">\(0.5\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \begin{pmatrix}
        5 &amp; \mu &amp; 1 &amp; 1 
    \end{pmatrix}
    \approx&amp;
    -0.3\cdot 
    \begin{pmatrix}
        -9.0 &amp; -5.8 &amp; -9.5 &amp; -5.3
    \end{pmatrix}
    +0.5\cdot
    \begin{pmatrix}
        2.6 &amp; 3.3 &amp; -3.3 &amp; -2.2
    \end{pmatrix}    
\end{align*}\]</div>
<p>We visualize the sum of the two outer products with colored matrices. The more saturated the color, the  higher is the absolute value of the corresponding element in the matrix. Positive values are blue and negative values are pink.<br />
The visualization makes the grid structure apparent which is induced by the outer product. The first movie pattern is <span class="math notranslate nohighlight">\(\begin{pmatrix}-9.0 &amp; -5.8 &amp; -9.5 &amp; -5.3\end{pmatrix}\)</span>, but because all values in the first user pattern are also negative, it would be more intuitive to consider the first movie pattern as <span class="math notranslate nohighlight">\(\begin{pmatrix}9 &amp; 5.8 &amp; 9.5 &amp; 5.3\end{pmatrix}\)</span>. We see this pattern in the first outer product <span class="math notranslate nohighlight">\(Y_{\cdot 1}X_{\cdot 1}^\top\)</span>: the first and third column have a higher intensity than the other columns, corresponding to the high values in the pattern matrix <span class="math notranslate nohighlight">\(9\)</span> and <span class="math notranslate nohighlight">\(9.5\)</span>. We can roughly say that the first outer product indicates how much the user likes movie 1 and movie 3 but not so much the other movies.<br />
However, we cannot make general statements from the first outer product (e.g., user 2 likes movie 1 and movie 3 because their (sign-corrected) coefficient for the first movie pattern is 0.4, which is comparatively high). That is because the second outer product may correct what the first outer product indicates by subtracting and adding values. For evample, user 2 doesn’t particularly like movie 1 (in fact, it’s a missing value) and although the first outer product indicates a score of <span class="math notranslate nohighlight">\(0.4\cdot 9=3.6\)</span>. The second outer product corrects this by subtracting <span class="math notranslate nohighlight">\(-0.4\cdot2.6=-1.04\)</span>.</p>
<p>Hence, we can’t make general statements based on one outer product alone. However, the significance of the outer products in the sense of how they influence the approximation, drops with the index of the outer product. That is because the singular values are decreasing in magnitude with the index. Visually, we can observe this with the fading colors making up the second outer product.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:13: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:20: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:13: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:20: SyntaxWarning: invalid escape sequence &#39;\c&#39;
/var/folders/__/yt6jwp9d3gz1hjldtp6lhvxh0000gn/T/ipykernel_74000/1546977387.py:13: SyntaxWarning: invalid escape sequence &#39;\c&#39;
  plt.title(&quot;$Y_{\cdot 1}X_{\cdot 1}^T$&quot;)
/var/folders/__/yt6jwp9d3gz1hjldtp6lhvxh0000gn/T/ipykernel_74000/1546977387.py:20: SyntaxWarning: invalid escape sequence &#39;\c&#39;
  plt.title(&quot;$Y_{\cdot 2}X_{\cdot 2}^T$&quot;)
</pre></div>
</div>
<img alt="_images/068536bd4093a629506e6b2f7eeef7db92841a384d9bbc4bd8aecef6dedb7f39.png" src="_images/068536bd4093a629506e6b2f7eeef7db92841a384d9bbc4bd8aecef6dedb7f39.png" />
</div>
</div>
</section>
</section>
<section id="what-happens-when-we-increase-the-rank">
<h2>What Happens When We Increase the Rank?<a class="headerlink" href="#what-happens-when-we-increase-the-rank" title="Link to this heading">#</a></h2>
<p>The rank of the matrix factorization is a hyperparameter. If we choose the rank too low, then the approximation might underfit and some of the patterns in the data remain undiscovered. If the rank is too high, then the model also approximates noise-effects from the data and overfits. Noise effects could be in the recommender setting fluctuations in the mood of the user at the time of the rating. In the setting of the matrix completion task, a too high rank also results in an approximation of the neutral rankings that are imputed for the missing values.</p>
<p>We visualize the factorization with a rank of three below. We see that the third outer product does make some minor corrections for user 1 and 5. At this point, the information in the third outer product becomes neglectable and should not be included in the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">Vᵀ</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:12: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:19: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:26: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:12: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:19: SyntaxWarning: invalid escape sequence &#39;\c&#39;
&lt;&gt;:26: SyntaxWarning: invalid escape sequence &#39;\c&#39;
/var/folders/__/yt6jwp9d3gz1hjldtp6lhvxh0000gn/T/ipykernel_74000/190330829.py:12: SyntaxWarning: invalid escape sequence &#39;\c&#39;
  plt.title(&quot;$Y_{\cdot 1}X_{\cdot 1}^T$&quot;)
/var/folders/__/yt6jwp9d3gz1hjldtp6lhvxh0000gn/T/ipykernel_74000/190330829.py:19: SyntaxWarning: invalid escape sequence &#39;\c&#39;
  plt.title(&quot;$Y_{\cdot 2}X_{\cdot 2}^T$&quot;)
/var/folders/__/yt6jwp9d3gz1hjldtp6lhvxh0000gn/T/ipykernel_74000/190330829.py:26: SyntaxWarning: invalid escape sequence &#39;\c&#39;
  plt.title(&quot;$Y_{\cdot 3}X_{\cdot 3}^T$&quot;)
</pre></div>
</div>
<img alt="_images/2d9276ca32fedc7df23d3c28df13387d0d2638ff4d203f5c5050c8e7795ba876.png" src="_images/2d9276ca32fedc7df23d3c28df13387d0d2638ff4d203f5c5050c8e7795ba876.png" />
</div>
</div>
</section>
<section id="low-rank-mf-on-observed-entries">
<h2>Low-Rank MF on Observed Entries<a class="headerlink" href="#low-rank-mf-on-observed-entries" title="Link to this heading">#</a></h2>
<p>The SVD of the rating matrix with neutral rating imputations gave us some indications of possible recommendations, but an issue with this simple approach is that the imputed missing values are also approximated by the factorization. Especially when observations are sparse, which is usually the case for movie recommendations for example, then this method won’t work because the actual ratings will be perceived as outliers.</p>
<p>An idea to solve the matrix completion task more accurately is to compute a factorization that approximates only the values of observed entries.
This approach has been used in the Netflix Price 2009 competition and made it to the top-3.</p>
<div class="tip admonition">
<p class="admonition-title">Task (Low-Rank Matrix Factorization with Missing Values)</p>
<p><strong>Given</strong> a data matrix <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}\)</span> having observed entries <span class="math notranslate nohighlight">\(D_{ik}\)</span> for indices <span class="math notranslate nohighlight">\((i,k)\)</span> where the binary indicator matrix <span class="math notranslate nohighlight">\(O\in\{0,1\}^{n\times d}\)</span> has an entry of one <span class="math notranslate nohighlight">\(O_{ik}=1\)</span>, and a rank <span class="math notranslate nohighlight">\(r&lt;\min\{n,d\}\)</span>.</p>
<p><strong>Find</strong> matrices <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{d\times r}\)</span> and <span class="math notranslate nohighlight">\(Y\in\mathbb{R}^{n\times r}\)</span> whose product approximates the data matrix only on observed entries:</p>
<div class="amsmath math notranslate nohighlight" id="equation-845af069-f0f8-40ae-84ca-80d2050e44f2">
<span class="eqno">(59)<a class="headerlink" href="#equation-845af069-f0f8-40ae-84ca-80d2050e44f2" title="Permalink to this equation">#</a></span>\[\begin{align}
    \min_{X,Y}&amp;\lVert O\circ(D- YX^\top)\rVert^2 +\lambda\lVert X\rVert^2+\lambda\lVert Y\rVert^2\\ 
    &amp;=\sum_{(i,k):O_{ik}=1}(D_{ik}-Y_{i\cdot}X_{k\cdot}^\top)^2+\lambda\lVert X\rVert^2+\lambda\lVert Y\rVert^2\\ 
    \text{s.t. }&amp; X\in \mathbb{R}^{d\times r}, Y\in\mathbb{R}^{n\times r}
\end{align}\]</div>
<p><strong>Return</strong> the low-dimensional approximation of the data <span class="math notranslate nohighlight">\((X,Y)\)</span>.</p>
</div>
<section id="id1">
<h3>Optimization<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>The low-rank MF on observed entries can not be computed directly by SVD. However, we can derive the minimizers of one column of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> when fixing the other matrix.</p>
<div class="proof theorem admonition" id="theorem-3">
<p class="admonition-title"><span class="caption-number">Theorem 30 </span></p>
<section class="theorem-content" id="proof-content">
<p>The minimizers of the objective to minimize <span class="math notranslate nohighlight">\(\lVert O\circ(D- YX^\top)\rVert^2\)</span> subject to a row of <span class="math notranslate nohighlight">\(X\)</span> or <span class="math notranslate nohighlight">\(Y\)</span> is given as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;D_{\cdot k}^\top \diag(O_{\cdot k})Y(Y^\top \diag(O_{\cdot k}) Y+\lambda I)^{-1}\\ 
&amp;\quad = \argmin_{X_{k\cdot}}
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert X\rVert^2\\
&amp;D_{i\cdot} \diag(O_{i\cdot})X(X^\top \diag(O_{i\cdot}) X+\lambda I)^{-1}\\ 
&amp;\quad = \argmin_{Y_{i\cdot}}
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert Y\rVert^2
\end{align*}\]</div>
</section>
</div><div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. We show the result for minimizing over <span class="math notranslate nohighlight">\(X_{k\cdot}\)</span>, the result for <span class="math notranslate nohighlight">\(Y_{i\cdot}\)</span> follows by transposing the factorization. First, we observe that the minimization subject to <span class="math notranslate nohighlight">\(X_{k\cdot}\)</span> reduces to the minimization over the <span class="math notranslate nohighlight">\(k\)</span>-th column:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\argmin_{X_{k\cdot}}&amp;
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert X\rVert^2\\
&amp;= \argmin_{X_{k\cdot}}
\lVert O_{\cdot k}\circ(D_{\cdot k}- YX_{k\cdot }^\top)\rVert^2+ \lambda\lVert X_{k\cdot}\rVert^2
\end{align*}\]</div>
<p>The element-wise multiplication with the binary vector <span class="math notranslate nohighlight">\(O_{\cdot k}\)</span> selects the rows for which we have observed entries in column <span class="math notranslate nohighlight">\(k\)</span>. This selection of rows can also be performed with a multiplication of <span class="math notranslate nohighlight">\(\diag(O_{\cdot k})\)</span> from the left. This way, we can write the objective to optimize subject to <span class="math notranslate nohighlight">\(X_{k\cdot}\)</span> as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\argmin_{X_{k\cdot}}&amp;
\lVert O\circ(D- YX^\top)\rVert^2 + \lambda\lVert X\rVert^2\\ 
&amp;= \argmin_{X_{k\cdot}}
\lVert \underbrace{\diag(O_{\cdot k})D_{\cdot k}}_{=\tilde{\vvec{y}}}- \underbrace{\diag(O_{\cdot k})Y}_{=\tilde{X}}\underbrace{X_{k\cdot }^\top}_{=\tilde{\beta}})\rVert^2 + \lambda\lVert X\rVert^2 
\end{align*}\]</div>
<p>The objective above is equivalent to a ridge regression objective (using the notation of ridge regression, the target vector <span class="math notranslate nohighlight">\(\tilde{\vvec{y}}\)</span>, the design matrix <span class="math notranslate nohighlight">\(\tilde{X}\)</span> and the parameter vector <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span> are annotated above). We know the minimizer of this objective, it is given by the vector</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(\tilde{X}^\top \tilde{X}+\lambda I)^{-1}\tilde{X}^\top\tilde{\vvec{y}}
&amp;= (Y^\top \diag(O_{\cdot k})^2 Y +\lambda I)^{-1} Y^\top\diag(O_{\cdot k})^2D_{\cdot k}\\
&amp;=(Y^\top \diag(O_{\cdot k}) Y +\lambda I)^{-1} Y^\top\diag(O_{\cdot k})D_{\cdot k}
\end{align*}\]</div>
<p>where the last equation follows from the fact that binary values do not change when they are squared.</p>
</div>
</div>
<p>The theorem above motivates a block-coordinate descent approach, where we go in every iteration through each column of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> and update it. This procedure is described in the algorithm below.</p>
<div class="proof algorithm admonition" id="algorithm-4">
<p class="admonition-title"><span class="caption-number">Algorithm 13 </span> (MatrixCompletion)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: the dataset <span class="math notranslate nohighlight">\(D\)</span>, rank <span class="math notranslate nohighlight">\(r\)</span>, maximum number of iterations <span class="math notranslate nohighlight">\(t_{max} = 100\)</span>, and regularization weight  <span class="math notranslate nohighlight">\(\lambda = 0.1\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\((X, Y) \gets\)</span> <code class="docutils literal notranslate"><span class="pre">InitRandom</span></code><span class="math notranslate nohighlight">\((n, d, r)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(O \gets\)</span> <code class="docutils literal notranslate"><span class="pre">IndicatorObserved</span></code><span class="math notranslate nohighlight">\((D)\)</span></p></li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(t\in\{1,\ldots,t_{max}\}\)</span></p>
<ol class="arabic simple">
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(k \in \{1, \ldots, d\}\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(X_{k\cdot} \leftarrow D_{\cdot k}^{\top} \diag(O_{\cdot k})Y (Y^{\top} \diag(O_{\cdot k}) Y + \lambda I)^{-1}\)</span></p></li>
</ol>
</li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(i \in \{1, \ldots, n\}\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(Y_{i\cdot} \leftarrow D_{i\cdot}\diag(O_{i\cdot}) X (X^{\top} \diag(O_{i\cdot}) X + \lambda I)^{-1}\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\((X,Y)\)</span></p></li>
</ol>
</section>
</div></section>
<section id="from-unsupervised-matrix-completion-to-supervised-behavioral-modeling">
<h3>From Unsupervised Matrix Completion to Supervised Behavioral Modeling<a class="headerlink" href="#from-unsupervised-matrix-completion-to-supervised-behavioral-modeling" title="Link to this heading">#</a></h3>
<p>Early recommender systems, such as those based on matrix factorization, are traditionally framed as unsupervised learning problems. The central task is to complete a sparse user-item rating matrix by learning latent factors that explain the observed ratings. This process resembled dimensionality reduction (e.g., via SVD), and the goal was to estimate the missing entries as accurately as possible.</p>
<p>However, this perspective has shifted in modern systems.
Today, most recommendation systems rely not on explicit ratings, but on <strong>implicit feedback</strong>:</p>
<ul class="simple">
<li><p>Clicks, taps, swipes</p></li>
<li><p>Watch time or scroll depth</p></li>
<li><p>Song plays, skips, replays</p></li>
<li><p>Add-to-cart or wishlist events</p></li>
</ul>
<p>These behaviors are logged at scale, and treated as training signals for supervised learning. The recommendation problem becomes:</p>
<blockquote>
<div><p><em>Given past behavior, predict whether a user will interact with an item.</em></p>
</div></blockquote>
<p>This makes modern recommendation fundamentally a prediction task, rather than a pure matrix completion task. Reformulating the unsupervised recommender system task into a supervised one has the advantages that we can test the performance. In addition, it provides guidance to the model in terms of what makes a user hooked to the screen, which is generally easier to monetize than solving the more general task to provide good recommendations.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="dim_reduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Dimensionality Reduction Techniques</p>
      </div>
    </a>
    <a class="right-next"
       href="dim_reduction_pca.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Principal Component Analysis</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formal-problem-definition">Formal Problem Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-matrix-completion-recommender-system">A Simple Matrix Completion Recommender System</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-factorization">Interpretation of the Factorization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-when-we-increase-the-rank">What Happens When We Increase the Rank?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-mf-on-observed-entries">Low-Rank MF on Observed Entries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-unsupervised-matrix-completion-to-supervised-behavioral-modeling">From Unsupervised Matrix Completion to Supervised Behavioral Modeling</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sibylle Hess
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022. Eindhoven University of Technology.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>